---
title: "Dell PowerEdge R760 Installation and Service Manual"
author: "Dell Inc."
source: "doc_to_md\sample_data\dell-poweredge-r760-installation-and-service-manual.pdf"
page_count: 392
chunk_count: 411
---



### Dell PowerEdge R760





#### Installation and Service Manual

**Regulatory Model: E82S**
**Regulatory Type: E82S001**
**June 2025**
**Rev. A09**

Notes, cautions, and warnings

**NOTE:** A NOTE indicates important information that helps you make better use of your product.

**CAUTION: A CAUTION indicates either potential damage to hardware or loss of data and tells you how to avoid**
**the problem.**

**WARNING: A WARNING indicates a potential for property damage, personal injury, or death.**

Copyright © 2025 Dell Inc. All Rights Reserved. Dell Technologies, Dell, and other trademarks are trademarks of Dell Inc. or its subsidiaries.
Other trademarks may be trademarks of their respective owners.



# Contents

**Chapter 1: About this document.................................................................................................... 9**

**Chapter 2: System overview........................................................................................................ 10**

Front view of the system.................................................................................................................................................10

Left control panel view............................................................................................................................................... 12
Right control panel view.............................................................................................................................................13
Rear view of the system.................................................................................................................................................. 14
Inside the system............................................................................................................................................................... 21
Locating the Express Service Code and Service Tag...............................................................................................23
System information labels............................................................................................................................................... 24
Rail sizing and rack compatibility matrix.......................................................................................................................31

**Chapter 3: Technical specifications.............................................................................................33**

Chassis dimensions ..........................................................................................................................................................34
System weight................................................................................................................................................................... 34
Processor specifications..................................................................................................................................................35
PSU specifications............................................................................................................................................................ 36
Supported operating systems.........................................................................................................................................37
Cooling fan specifications............................................................................................................................................... 38
System battery specifications........................................................................................................................................39
Expansion card riser specifications...............................................................................................................................39
Memory specifications.....................................................................................................................................................40
Storage controller specifications....................................................................................................................................41
Drives.................................................................................................................................................................................... 41
Ports and connectors specifications............................................................................................................................ 42

USB ports specifications........................................................................................................................................... 42
NIC port specifications...............................................................................................................................................42
Serial connector specifications................................................................................................................................ 42
VGA ports specifications........................................................................................................................................... 43
Video specifications..........................................................................................................................................................43
Environmental specifications..........................................................................................................................................43

Particulate and gaseous contamination specifications.......................................................................................45
Thermal restriction matrix.........................................................................................................................................46
Thermal air restrictions.............................................................................................................................................. 62

**Chapter 4: Initial system setup and configuration........................................................................64**

Setting up the system......................................................................................................................................................64
iDRAC configuration......................................................................................................................................................... 64

Options to set up iDRAC IP address.......................................................................................................................64
Options to log in to iDRAC........................................................................................................................................65
Resources to install operating system......................................................................................................................... 66

Options to download drivers and firmware ..........................................................................................................66
Options to download and install OS drivers .........................................................................................................67
Downloading drivers and firmware..........................................................................................................................67

**Chapter 5: Pre-operating system management applications........................................................ 68**

System Setup.....................................................................................................................................................................68

System BIOS.................................................................................................................................................................69
iDRAC Settings............................................................................................................................................................ 90
Device Settings............................................................................................................................................................90
Service Tag Settings.................................................................................................................................................. 90
Dell Lifecycle Controller...................................................................................................................................................90

Embedded system management..............................................................................................................................90
Boot Manager.................................................................................................................................................................... 90
PXE boot..............................................................................................................................................................................91

**Chapter 6: Minimum to POST and system management configuration validation......................... 92**

Minimum configuration to POST ..................................................................................................................................92
Configuration validation...................................................................................................................................................92

Error messages............................................................................................................................................................ 93

**Chapter 7: Disassembly and reassembly...................................................................................... 94**

Safety instructions............................................................................................................................................................94
Before working inside your system ..............................................................................................................................95
After working inside your system..................................................................................................................................95
Recommended tools.........................................................................................................................................................96
Optional front bezel..........................................................................................................................................................96

Removing the front bezel..........................................................................................................................................96
Installing the front bezel............................................................................................................................................ 97
System cover..................................................................................................................................................................... 98

Removing the system cover..................................................................................................................................... 98
Installing the system cover....................................................................................................................................... 99
Drive backplane cover....................................................................................................................................................100

Removing the drive backplane cover....................................................................................................................100
Installing the drive backplane cover.......................................................................................................................101
Air shrouds........................................................................................................................................................................ 103

Removing the air shroud..........................................................................................................................................103
Installing the air shroud............................................................................................................................................ 103
Removing the GPU air shroud top cover.............................................................................................................104
Installing the GPU air shroud top cover...............................................................................................................105
Removing the GPU air shroud filler.......................................................................................................................106
Installing the GPU air shroud filler......................................................................................................................... 107
Removing the GPU air shroud................................................................................................................................108
Installing the GPU air shroud.................................................................................................................................. 109
Removing the 2 x 2.5-inch rear drive module air shroud..................................................................................110
Installing the 2 x 2.5-inch rear drive module air shroud.....................................................................................111
Removing the 4 x 2.5-inch rear drive module air shroud..................................................................................112
Installing the 4 x 2.5-inch rear drive module air shroud....................................................................................113
Removing the EDSFF E3.S rear drive module air shroud..................................................................................114
Installing the EDSFF E3.S rear drive module air shroud....................................................................................115
Cooling fans....................................................................................................................................................................... 116

Removing the cooling fan cage assembly ........................................................................................................... 116
Installing the cooling fan cage assembly............................................................................................................... 117

Removing a cooling fan.............................................................................................................................................118
Installing a cooling fan............................................................................................................................................... 119
Removing a 2 x 2.5-inch rear drive module cooling fan................................................................................... 120
Installing a 2 x 2.5-inch rear drive module cooling fan...................................................................................... 121
Removing a 4 x 2.5-inch rear drive module cooling fan................................................................................... 122
Installing a 4 x 2.5-inch rear drive module cooling fan..................................................................................... 123
Removing the EDSFF E3.S rear drive module cooling fan...............................................................................124
Installing the EDSFF E3.S rear drive module cooling fan.................................................................................125
Drives..................................................................................................................................................................................126

Removing a drive blank............................................................................................................................................ 126
Installing a drive blank...............................................................................................................................................127
Removing a drive carrier.......................................................................................................................................... 127
Installing the drive carrier........................................................................................................................................ 128
Removing the drive from the drive carrier.......................................................................................................... 129
Installing the drive into the drive carrier.............................................................................................................. 130
Removing an EDSFF E3.S drive blank................................................................................................................... 131
Installing an EDSFF E3.S drive blank.....................................................................................................................132
Removing an EDSFF E3.S drive carrier................................................................................................................ 133
Installing an EDSFF E3.S drive carrier.................................................................................................................. 134
Removing an EDSFF E3.S drive from the drive carrier.................................................................................... 135
Installing an EDSFF E3.S drive into the drive carrier........................................................................................ 136
Rear drive module............................................................................................................................................................137

Removing the 2 x 2.5-inch rear drive module.....................................................................................................137
Installing the 2 x 2.5-inch rear drive module.......................................................................................................138
Removing the 4 x 2.5-inch rear drive module.....................................................................................................139
Installing the 4 x 2.5-inch rear drive module........................................................................................................141
Removing the EDSFF E3.S rear drive module.................................................................................................... 142
Installing the EDSFF E3.S rear drive module.......................................................................................................143
Drive backplane................................................................................................................................................................144

Drive backplane.......................................................................................................................................................... 144
Removing the drive backplane .............................................................................................................................. 148
Installing the drive backplane..................................................................................................................................150
Side wall brackets.............................................................................................................................................................151

Removing the side wall bracket.............................................................................................................................. 151
Installing the side wall bracket................................................................................................................................152
Cable routings...................................................................................................................................................................154
PERC module...................................................................................................................................................................208

Removing the rear mounting front PERC module............................................................................................ 208
Installing the rear mounting front PERC module................................................................................................ 211
Removing the adapter PERC module....................................................................................................................213
Installing the adapter PERC module......................................................................................................................215
Removing the EDSFF E3.S PERC module........................................................................................................... 216
Installing the EDSFF E3.S PERC module............................................................................................................. 219
EDSFF E3.S backplane module....................................................................................................................................222

Removing the EDSFF E3.S backplane module...................................................................................................222
Installing the EDSFF E3.S backplane module.....................................................................................................223
System memory...............................................................................................................................................................224

System memory guidelines..................................................................................................................................... 224
General memory module installation guidelines................................................................................................. 226
Removing a memory module...................................................................................................................................227

Installing a memory module.....................................................................................................................................228
Processor and heat sink module..................................................................................................................................229

Removing the processor and heat sink module.................................................................................................229
Removing the processor.......................................................................................................................................... 231
Installing the processor............................................................................................................................................233
Installing the processor and heat sink module................................................................................................... 238
Removing the Direct Liquid Cooling module.......................................................................................................240
Removing the processor.......................................................................................................................................... 241
Installing the processor............................................................................................................................................243
Installing the Direct Liquid Cooling module.........................................................................................................246
Expansion cards and expansion card risers.............................................................................................................. 248

Expansion card installation guidelines.................................................................................................................. 248
Removing the expansion card risers.....................................................................................................................294
Installing the expansion card risers.......................................................................................................................299
Removing expansion card from the expansion card riser............................................................................... 304
Installing an expansion card into the expansion card riser..............................................................................305
Removing the full length expansion card risers.................................................................................................307
Installing the full length expansion card risers...................................................................................................309
Removing a GPU........................................................................................................................................................312
Installing a GPU.......................................................................................................................................................... 314
Removing R1 and R4 paddle cards.........................................................................................................................317
Installing R1 and R4 paddle cards...........................................................................................................................318
Data processing unit (DPU)..........................................................................................................................................319

Removing a DPU card from a full length riser.................................................................................................... 319
Installing a DPU into a full length riser.................................................................................................................322
Optional serial COM port.............................................................................................................................................. 324

Removing the serial COM port.............................................................................................................................. 324
Installing the serial COM port................................................................................................................................ 326
Optional VGA port for Direct Liquid Cooling module..............................................................................................327

Removing the VGA port...........................................................................................................................................327
Installing the VGA port.............................................................................................................................................329
M.2 SSD module..............................................................................................................................................................330

Removing the M.2 NVMe SSD module................................................................................................................330
Installing the M.2 NVMe SSD module...................................................................................................................331
Optional BOSS-N1 module............................................................................................................................................333

Removing the BOSS-N1 module blank.................................................................................................................333
Installing the BOSS-N1 module blank................................................................................................................... 333
Removing the BOSS-N1 card carrier blank.........................................................................................................334
Installing the BOSS-N1 card carrier blank...........................................................................................................335
Removing the BOSS-N1 module............................................................................................................................336
Installing the BOSS-N1 module..............................................................................................................................338
System battery ...............................................................................................................................................................339

Replacing the system battery................................................................................................................................ 339
Optional internal USB card............................................................................................................................................341

Removing the internal USB card............................................................................................................................341
Installing the internal USB card............................................................................................................................. 342
Intrusion switch............................................................................................................................................................... 343

Removing the intrusion switch module................................................................................................................343
Installing the intrusion switch module..................................................................................................................343
Optional OCP NIC card..................................................................................................................................................344

Removing the OCP NIC card................................................................................................................................. 344
Installing the OCP NIC card....................................................................................................................................346
Power supply unit............................................................................................................................................................347

Hot spare feature......................................................................................................................................................347
Removing a power supply unit blank....................................................................................................................348
Installing a power supply unit blank...................................................................................................................... 348
Removing a power supply unit adapter............................................................................................................... 349
Installing a power supply unit adapter................................................................................................................. 350
Removing a power supply unit...............................................................................................................................350
Installing a power supply unit..................................................................................................................................351
Trusted Platform Module..............................................................................................................................................352

Upgrading the Trusted Platform Module............................................................................................................ 352
Initializing TPM for users.........................................................................................................................................353
Initializing the TPM 2.0 for users..........................................................................................................................353
System board...................................................................................................................................................................354

Removing the system board...................................................................................................................................354
Installing the system board.....................................................................................................................................355
Restoring the system using Easy Restore...........................................................................................................357
Manually update the Service Tag..........................................................................................................................357
LOM card, MIC card and rear I/O board...................................................................................................................357

Removing the LOM card, MIC card and rear I/O board..................................................................................357
Installing the LOM card, MIC card and rear I/O board....................................................................................359
Control panel.....................................................................................................................................................................361

Removing the right control panel.......................................................................................................................... 361
Installing the right control panel............................................................................................................................362
Removing the left control panel............................................................................................................................ 363
Installing the left control panel.............................................................................................................................. 364

**Chapter 8: Upgrade Kits............................................................................................................ 366**

BOSS-N1 module kit.......................................................................................................................................................367
GPU kit..............................................................................................................................................................................369
Internal USB card kit...................................................................................................................................................... 373
Serial COM port kit.........................................................................................................................................................373

**Chapter 9: Jumpers and connectors.......................................................................................... 374**

System board jumpers and connectors..................................................................................................................... 374
System board jumper settings..................................................................................................................................... 376
Disabling a forgotten password................................................................................................................................... 377

**Chapter 10: System diagnostics and indicator codes..................................................................378**

Status LED indicators.....................................................................................................................................................378
System health and system ID indicator codes.........................................................................................................380
iDRAC Quick Sync 2 indicator codes......................................................................................................................... 380
iDRAC Direct LED indicator codes.............................................................................................................................. 381
LCD panel.......................................................................................................................................................................... 381

Viewing Home screen...............................................................................................................................................382
Setup menu.................................................................................................................................................................382
View menu.................................................................................................................................................................. 383
NIC indicator codes........................................................................................................................................................383

Power supply unit indicator codes..............................................................................................................................384
Drive indicator codes..................................................................................................................................................... 385
EDSFF E3.S drive led codes.........................................................................................................................................386
Using system diagnostics..............................................................................................................................................387

Dell Embedded System Diagnostics......................................................................................................................387

**Chapter 11: Getting help............................................................................................................ 389**

Recycling or End-of-Life service information.......................................................................................................... 389
Contacting Dell Technologies...................................................................................................................................... 389
Accessing system information by using QR code................................................................................................... 389

QR code for PowerEdge R760 system resources............................................................................................ 390
Receiving automated support with Secure Connect Gateway (SCG).............................................................. 390

**Chapter 12: Documentation resources........................................................................................391**



# 1 About this document

This document provides an overview about the system, information about installing and replacing components, diagnostic tools,
and guidelines to be followed while installing certain components.



# 2 System overview

The PowerEdge R760 system is a 2U server that supports:

- Two 4 [th] Gen Intel® Xeon® Scalable or Intel® Xeon® Max Processors with up to 56 cores
- Two 5 [th] Gen Intel® Xeon® Scalable Processors with up to 64 cores
- Optional Direct Liquid Cooling
- 32 DIMM slots
- Two redundant AC or DC power supply units
- Up to 12 x 3.5-inch SAS, SATA or 24 x 2.5-inch, 16 x 2.5-inch, 8 x 2.5-inch, or 2 x 2.5-inch (rear), 4 x 2.5-inch(rear), 4 x
EDSFF E3.S (rear) SAS, SATA, or NVMe (HDD/SSD) drives, or 16 x EDSFF E3.S NVMe (SSD) drives.

**NOTE:** For more information about how to hot swap NVMe PCIe SSD U.2 device, see the _Dell Express Flash NVMe PCIe_
_SSD User's Guide_ [at Dell Support](https://www.dell.com/support/home/en-us) **> Browse all products**   - **Infrastructure**   - **Data Center Infrastructure**   - **Storage**
**Adapters & Controllers**   - **Dell PowerEdge Express Flash NVMe PCIe SSD**   - **Select This Product**   - **Documentation**

  - **Manuals and Documents** .
**NOTE:** All instances of SAS, SATA, and NVMe drives are referred to as drives in this document, unless specified otherwise.

**CAUTION: Do not install GPUs, network cards, or other PCIe devices on your system that are not validated**
**and tested by Dell. Damage caused by unauthorized and invalidated hardware installation will null and void the**
**system warranty.**

**Topics:**

- Front view of the system
- Rear view of the system
- Inside the system
- Locating the Express Service Code and Service Tag
- System information labels
- Rail sizing and rack compatibility matrix



## Front view of the system

**Figure 1. Front view of 24 x 2.5-inch drive system**

**Figure 2. Front view of 16 x 2.5-inch drive system**

**Figure 3. Front view of 8 x 2.5-inch drive system**

**Figure 4. Front view of 12 x 3.5-inch drive system**

**Figure 5. Front view of 16 x EDSFF E3.S drive system**

**Figure 6. Front view of 16 x EDSFF E3.S drive system**

**Table 1. Features available on the front of the system**

|Item|Ports, panels, and<br>slots|Icon|Description|
|---|---|---|---|
|1|Left control panel|N/A|Contains the system health, system ID, and the status LED indicator.|
|2|Drive|N/A|Enables you to install drives that are supported on your system.<br>**NOTE:** For drive slot numbers, see theSystem information labels<br>section.|
|3|Right control panel|N/A|Contains the power button, VGA port, USB port, iDRAC Direct<br>(Micro-AB USB) port, and the iDRAC Direct status LED.|
|4|Express Service Tag|N/A|The Express Service Tag is a slide-out label panel that contains<br>system information such as Service Tag, NIC, MAC address, and so<br>on. If you have opted for the secure default access to iDRAC, the<br>Information tag will also contain the iDRAC secure default password.|

**NOTE:** For more information about ports, panels, and slots, see the Technical specification section.



### Left control panel view

**Figure 7. Left control panel**

**Table 2. Left control panel**

|Item|Indicator, button, or<br>connector|Icon|Description|
|---|---|---|---|
|1|Status LED indicators|NA|Indicates the status of the system. For more information, see the<br>Status LED indicators section.|
|2|System health and system<br>ID||Indicates the system health. For more information, see theSystem<br>health and system ID indicator codes section.|
|1|Status LED indicators|N/A|Indicates the status of the system. For more information, see the<br>Status LED indicators section.|
|2|System health and system<br>ID indicator||Indicates the system health. For more information, see the<br>System health and system ID indicator codes section.|
|3|iDRAC Quick Sync<br>2 wireless indicator<br>(optional)||Indicates if the iDRAC Quick Sync 2 wireless option is<br>activated. The Quick Sync 2 feature allows management of the<br>system using mobile devices. This feature aggregates hardware/<br>firmware inventory and various system level diagnostic/error<br>information that can be used in troubleshooting the system. You<br>can access system inventory, Dell Lifecycle Controller logs or<br>system logs, system health status, and also configure iDRAC,<br>BIOS, and networking parameters. You can also launch the virtual<br>Keyboard, Video, and Mouse (KVM) viewer and virtual Kernel-<br>based Virtual Machine (KVM), on a supported mobile device.<br>For more information, see the Integrated Dell Remote Access<br>Controller User's Guide atPowerEdge manuals.|

**NOTE:** For more information about the indicator codes, see the System diagnostics and indicator codes section.



### Right control panel view

**Figure 9. Right control panel**

**Table 4. Right control panel**

|Item|Indicator or button|Icon|Description|
|---|---|---|---|
|1|Power button||Indicates if the system is powered on or off. Press the power button to<br>manually power on or off the system.<br>**NOTE:** Press the power button to gracefully shut down an ACPI-compliant<br>operating system.|
|2|USB 2.0 port||The USB port is 4-pin, 2.0-compliant. This port enables you to connect USB<br>devices to the system.|
|3|iDRAC Direct<br>(Micro-AB USB)<br>port||The iDRAC Direct (Micro-AB USB) port enables you to access the iDRAC<br>direct Micro-AB USB features. For more information, see the_Integrated Dell_<br>_Remote Access Controller User's Guide_ atPowerEdge Manuals. <br>**NOTE:** You can configure iDRAC Direct by using a USB to micro USB (type<br>AB) cable, which you can connect to your laptop or tablet. Cable length<br>should not exceed 3 feet (0.91 meters). Performance could be affected by<br>cable quality.|
|4|VGA port||Enables you to connect a display device to the system.|

**NOTE:** For more information about ports, panels, and slots, see the Technical specification section.



## Rear view of the system





## Figure 10. Rear view of the system

**Table 5. Rear view of the system**

|Item|Ports, panels, or slots|Icon|Description|
|---|---|---|---|
|1|PCIe expansion card riser 1<br>(slot 1 and slot 2)|N/A|The expansion card riser enables you to connect PCI Express<br>expansion cards. For more information, see theExpansion card<br>installation guidelines section.|
|2|BOSS module|N/A|BOSS module for internal system boot.|
|3|PCIe expansion card riser 2<br>(slot 3 and slot 6)|N/A|The expansion card riser enables you to connect PCI Express<br>expansion cards. For more information, see theExpansion card<br>installation guidelines section.|
|4|PCIe expansion card riser 3<br>(slot 4 and slot 5)|N/A|The expansion card riser enables you to connect PCI Express<br>expansion cards. For more information, see theExpansion card<br>installation guidelines section.|
|5|VGA port||Enables you to connect a display device to the system.|
|6|PCIe expansion card riser 4<br>(slot 7 and slot 8)|N/A|The expansion card riser enables you to connect PCI Express<br>expansion cards. For more information, see theExpansion card<br>installation guidelines section.|
|7|Power supply unit (PSU2)||PSU2 is the secondary PSU of the system.|
|8|USB 2.0 port||The USB port is 4-pin, 2.0-compliant. This port enables you to<br>connect USB devices to the system.|
|9|USB 3.0 port||The USB port is 9-pin and 3.0-compliant. This port enables you to<br>connect USB devices to the system.|
|10|Dedicated iDRAC9 Ethernet<br>port||Enables you to remotely access iDRAC. For more information, see the<br>Integrated_Dell Remote Access Controller User's Guide_ atPowerEdge<br>Manuals.|
|11|System Identification (ID)<br>button||The System Identification (ID) button is available on the front and<br>back of the system. Press the button to identify a system in a rack<br>by turning on the system ID button. You can also use the system ID<br>button to reset iDRAC and to access BIOS using the step through<br>mode. When pressed, the system ID LED in the back panel blinks until<br>either the front or rear button is pressed again. Press the button to<br>toggle between on or off mode.<br>**NOTE:** If the server stops responding during POST, press and<br>hold the**System ID** button for more than five seconds to enter<br>the BIOS progress mode.<br>**NOTE:** To reset the iDRAC (if not disabled on the iDRAC setup<br>page by pressing F2 during system boot), press and hold the<br>**System ID** button for more than 15 seconds.|
|12|OCP NIC card (optional)|N/A|The OCP NIC card supports OCP 3.0. The NIC ports are integrated<br>on the OCP card which is connected to the system board.|
|13|NIC ports (optional)||The NIC ports that are integrated on the LOM card provide network<br>connectivity which is connected to the system board or Management<br>Interface Card (MIC) to support Dell Data Processing Unit (DPU)<br>card to be installed in the riser.<br>**NOTE:** The system allows either LOM card or MIC card to be<br>installed in the system.|
|14|Power supply unit (PSU1)||PSU1 is the primary PSU of the system.|

**Figure 11. Rear view of the system with optional liquid cooling**

**Table 6. Rear view of the system with optional liquid cooling**

|Item|Ports, panels, or slots|Icon|Description|
|---|---|---|---|
|1|PCIe expansion card riser 1<br>(slot 1 and slot 2)|N/A|The expansion card riser enables you to connect PCI Express<br>expansion cards. For more information, see theExpansion card<br>installation guidelines section.|
|2|BOSS blank|N/A|Insert BOSS blank when BOSS module is not used.|
|3|PCIe expansion card riser 2<br>(slot 3 and slot 6)|N/A|The expansion card riser enables you to connect PCI Express<br>expansion cards. For more information, see theExpansion card<br>installation guidelines section.|
|4|PCIe expansion card riser 3<br>(slot 5)|N/A|The expansion card riser enables you to connect PCI Express<br>expansion cards. For more information, see theExpansion card<br>installation guidelines section.|
|5|Coolant tubes|N/A|Cold coolant flows into the system from one tube and hot coolant<br>leaves the system from another tube.|
|6|PCIe expansion card riser 4<br>(slot 7)|N/A|The expansion card riser enables you to connect PCI Express<br>expansion cards. For more information, see theExpansion card<br>installation guidelines section.|
|7|Power supply unit (PSU2)||PSU2 is the secondary PSU of the system.|
|8|USB 2.0 port||The USB port is 4-pin, 2.0-compliant. This port enables you to<br>connect USB devices to the system.|
|9|USB 3.0 port||The USB port is 9-pin and 3.0-compliant. This port enables you to<br>connect USB devices to the system.|
|10|Dedicated iDRAC9 Ethernet<br>port||Enables you to remotely access iDRAC. For more information, see the<br>Integrated_Dell Remote Access Controller User's Guide_ atPowerEdge<br>Manuals.|
|11|System Identification (ID)<br>button||The System Identification (ID) button is available on the front and<br>back of the system. Press the button to identify a system in a rack<br>by turning on the system ID button. You can also use the system ID<br>button to reset iDRAC and to access BIOS using the step through<br>mode. When pressed, the system ID LED in the back panel blinks until<br>either the front or rear button is pressed again. Press the button to<br>toggle between on or off mode.<br>**NOTE:** If the server stops responding during POST, press and<br>hold the**System ID** button for more than five seconds to enter<br>the BIOS progress mode.<br>**NOTE:** To reset the iDRAC (if not disabled on the iDRAC setup<br>page by pressing F2 during system boot), press and hold the<br>**System ID** button for more than 15 seconds.|
|12|OCP NIC card (optional)|N/A|The OCP NIC card supports OCP 3.0. The NIC ports are integrated<br>on the OCP card which is connected to the system board.|
|13|NIC ports (optional)||The NIC ports that are integrated on the LOM card provide network<br>connectivity which is connected to the system board or MIC to<br>support Dell DPU card to be installed in the riser.<br>**NOTE:** The system allows either LOM card or MIC card to be<br>installed in the system.|
|14|Power supply unit (PSU1)||PSU1 is the primary PSU of the system.|
|1|PCIe expansion card riser 1<br>(slot 1 and slot 2)|N/A|The expansion card riser enables you to connect PCI Express<br>expansion cards. For more information, see theExpansion card<br>installation guidelines section.|
|2|BOSS module|N/A|BOSS module for internal system boot.|
|3|PCIe expansion card riser 2<br>(slot 3 and slot 6)|N/A|The expansion card riser enables you to connect PCI Express<br>expansion cards. For more information, see theExpansion card<br>installation guidelines section.|
|4|Rear drive module|N/A|Enables you to install rear drives that are supported on your system.<br>**NOTE:** For drive slot numbers, see theRear drive module<br>section.|
|5|VGA port||Enables you to connect a display device to the system.|
|6|PCIe expansion card riser 4<br>(slot 7 and slot 8)|N/A|The expansion card riser enables you to connect PCI Express<br>expansion cards. For more information, see theExpansion card<br>installation guidelines section.|
|7|Power supply unit (PSU2)||PSU2 is the secondary PSU of the system.|
|8|USB 2.0 port||The USB port is 4-pin, 2.0-compliant. This port enables you to<br>connect USB devices to the system.|
|9|USB 3.0 port||The USB port is 9-pin and 3.0-compliant. This port enables you to<br>connect USB devices to the system.|
|10|Dedicated iDRAC9 Ethernet<br>port||Enables you to remotely access iDRAC. For more information, see the<br>Integrated_Dell Remote Access Controller User's Guide_ atPowerEdge<br>Manuals.|
|11|System Identification (ID)<br>button||The System Identification (ID) button is available on the front and<br>back of the system. Press the button to identify a system in a rack<br>by turning on the system ID button. You can also use the system ID<br>button to reset iDRAC and to access BIOS using the step through<br>mode. When pressed, the system ID LED in the back panel blinks until<br>either the front or rear button is pressed again. Press the button to<br>toggle between on or off mode.<br>**NOTE:** If the server stops responding during POST, press and<br>hold the**System ID** button for more than five seconds to enter<br>the BIOS progress mode.<br>**NOTE:** To reset the iDRAC (if not disabled on the iDRAC setup<br>page by pressing F2 during system boot), press and hold the<br>**System ID** button for more than 15 seconds.|

**Table 7. Rear view of the system with 2 x 2.5-inch rear drive module (continued)**

|Item|Ports, panels, or slots|Icon|Description|
|---|---|---|---|
|12|OCP NIC card (optional)|N/A|The OCP NIC card supports OCP 3.0. The NIC ports are integrated<br>on the OCP card which is connected to the system board.|
|13|NIC ports (optional)||The NIC ports that are integrated on the LOM card provide network<br>connectivity which is connected to the system board or MIC to<br>support Dell DPU card to be installed in the riser.<br>**NOTE:** The system allows either LOM card or MIC card to be<br>installed in the system.|
|14|Power supply unit (PSU1)||PSU1 is the primary PSU of the system.|
|1|Rear drive module|N/A|Enables you to install rear drives that are supported on your system.<br>**NOTE:** For drive slot numbers, see theRear drive module<br>section.|
|2|PCIe expansion card riser 2<br>(slot 3 and slot 6)|N/A|The expansion card riser enables you to connect PCI Express<br>expansion cards. For more information, see theExpansion card<br>installation guidelines section.|
|3|BOSS module|N/A|BOSS module for internal system boot.|
|4|VGA port||Enables you to connect a display device to the system.|
|5|PCIe expansion card riser 4<br>(slot 7 and slot 8)|N/A|The expansion card riser enables you to connect PCI Express<br>expansion cards. For more information, see theExpansion card<br>installation guidelines section.|
|6|Power supply unit (PSU2)||PSU2 is the secondary PSU of the system.|
|7|USB 2.0 port||The USB port is 4-pin, 2.0-compliant. This port enables you to<br>connect USB devices to the system.|
|8|USB 3.0 port||The USB port is 9-pin and 3.0-compliant. This port enables you to<br>connect USB devices to the system.|
|9|Dedicated iDRAC9 Ethernet<br>port||Enables you to remotely access iDRAC. For more information, see the<br>Integrated_Dell Remote Access Controller User's Guide_ atPowerEdge<br>Manuals.|
|10|System Identification (ID)<br>button||The System Identification (ID) button is available on the front and<br>back of the system. Press the button to identify a system in a rack<br>by turning on the system ID button. You can also use the system ID<br>button to reset iDRAC and to access BIOS using the step through<br>mode. When pressed, the system ID LED in the back panel blinks until|

**Table 8. Rear view of the system with 4 x 2.5-inch rear drive module (continued)**

|Item|Ports, panels, or slots|Icon|Description|
|---|---|---|---|
||||either the front or rear button is pressed again. Press the button to<br>toggle between on or off mode.<br>**NOTE:** If the server stops responding during POST, press and<br>hold the**System ID** button for more than five seconds to enter<br>the BIOS progress mode.<br>**NOTE:** To reset the iDRAC (if not disabled on the iDRAC setup<br>page by pressing F2 during system boot), press and hold the<br>**System ID** button for more than 15 seconds.|
|11|OCP NIC card (optional)|N/A|The OCP NIC card supports OCP 3.0. The NIC ports are integrated<br>on the OCP card which is connected to the system board.|
|12|NIC ports (optional)||The NIC ports that are integrated on the LOM card provide network<br>connectivity which is connected to the system board or MIC to<br>support Dell DPU card to be installed in the riser.<br>**NOTE:** The system allows either LOM card or MIC card to be<br>installed in the system.|
|13|Power supply unit (PSU1)||PSU1 is the primary PSU of the system.|
|1|PCIe expansion card riser 1<br>(slot 1 and slot 2)|N/A|The expansion card riser enables you to connect PCI Express<br>expansion cards. For more information, see theExpansion card<br>installation guidelines section.|
|2|BOSS module|N/A|BOSS module for internal system boot.|
|3|PCIe expansion card riser 2<br>(slot 3 and slot 6)|N/A|The expansion card riser enables you to connect PCI Express<br>expansion cards. For more information, see theExpansion card<br>installation guidelines section.|
|4|Rear drive module|N/A|Enables you to install rear drives that are supported on your system.<br>**NOTE:** For drive slot numbers, see theRear drive module<br>section.|
|5|VGA port||Enables you to connect a display device to the system.|
|6|PCIe expansion card riser 4<br>(slot 7 and slot 8)|N/A|The expansion card riser enables you to connect PCI Express<br>expansion cards. For more information, see theExpansion card<br>installation guidelines section.|

**Table 9. Rear view of the system with 4 x EDSFF E3.S rear drive module (continued)**

|Item|Ports, panels, or slots|Icon|Description|
|---|---|---|---|
|7|Power supply unit (PSU2)||PSU2 is the secondary PSU of the system.|
|8|USB 2.0 port||The USB port is 4-pin, 2.0-compliant. This port enables you to<br>connect USB devices to the system.|
|9|USB 3.0 port||The USB port is 9-pin and 3.0-compliant. This port enables you to<br>connect USB devices to the system.|
|10|Dedicated iDRAC9 Ethernet<br>port||Enables you to remotely access iDRAC. For more information, see the<br>Integrated_Dell Remote Access Controller User's Guide_ atPowerEdge<br>Manuals.|
|11|System Identification (ID)<br>button||The System Identification (ID) button is available on the front and<br>back of the system. Press the button to identify a system in a rack<br>by turning on the system ID button. You can also use the system ID<br>button to reset iDRAC and to access BIOS using the step through<br>mode. When pressed, the system ID LED in the back panel blinks until<br>either the front or rear button is pressed again. Press the button to<br>toggle between on or off mode.<br>**NOTE:** If the server stops responding during POST, press and<br>hold the**System ID** button for more than five seconds to enter<br>the BIOS progress mode.<br>**NOTE:** To reset the iDRAC (if not disabled on the iDRAC setup<br>page by pressing F2 during system boot), press and hold the<br>**System ID** button for more than 15 seconds.|
|12|OCP NIC card (optional)|N/A|The OCP NIC card supports OCP 3.0. The NIC ports are integrated<br>on the OCP card which is connected to the system board.|
|13|NIC ports (optional)||The NIC ports that are integrated on the LOM card provide network<br>connectivity which is connected to the system board or MIC to<br>support Dell DPU card to be installed in the riser.<br>**NOTE:** The system allows either LOM card or MIC card to be<br>installed in the system.|
|14|Power supply unit (PSU1)||PSU1 is the primary PSU of the system.|



## Inside the system

**Figure 15. Inside the system**

1. Backplane 2. Rear mounting front PERC module
3. Cooling fans 4. Air shroud
5. Memory DIMM sockets 6. Expansion riser 4
7. Expansion riser 3 8. Intrusion switch module
9. Power supply unit (PSU2) 10. Power supply unit (PSU1)
11. Rear handle 12. Expansion riser 1
13. Expansion riser 2 14. System board
15. Cooling fan cage assembly 16. Backplane
17. Express Service Tag
**Figure 16. Inside the system with full length risers and GPU shroud**

1. Backplane 2. Rear mounting front PERC module
3. Cooling fans 4. GPU air shroud
5. Expansion riser 4 6. Expansion riser 3
7. Intrusion switch module 8. Power supply unit (PSU2)
9. Power supply unit (PSU1) 10. Rear handle
11. Expansion riser 1 12. Expansion riser 2
13. System board 14. Cooling fan cage assembly
15. Backplane 16. Express Service Tag
**Figure 17. Inside the system with processor liquid cooling module**

1. Backplane 2. Rear mounting front PERC module
3. Cooling fans 4. Processor liquid cooling modules
5. Memory DIMM sockets 6. Expansion riser 4
7. Expansion riser 3 8. Intrusion switch module
9. Power supply unit (PSU2) 10. Power supply unit (PSU1)
11. Rear handle 12. Expansion riser 1
13. Expansion riser 2 14. System board
15. Cooling fan cage assembly 16. Backplane
17. Express Service Tag
**NOTE:** Air shroud is hidden in the above image to show the processor liquid cooling configuration.



## Locating the Express Service Code and Service Tag

The unique Express Service Code and Service Tag are used to identify the system.

The Express Service Tag is located on the front of the system that includes system information such as the Service Tag,
Express Service Code, Manufacture date, NIC, MAC address, QR code, and so on. If you have opted for the secure default
access to iDRAC, the Information tag also contains the iDRAC secure default password. If you have opted for iDRAC Quick Sync
2, the Information tag also contains the OpenManage Mobile (OMM) label, where administrators can configure, monitor, and
troubleshoot the PowerEdge servers.



## Figure 18. Locating the Express Service Code and Service tag

1. Express Service Tag (front view)
2. Express Service Tag (rear view)
3. OpenManage Mobile (OMM) label
4. iDRAC MAC address and iDRAC secure password label
5. Service Tag, Express Service Code, QR code

The Mini Enterprise Service Tag (MEST) label is on the rear of the system that includes the Service Tag (ST), and Express
Service Code (Exp Svc Code). The Exp Svc Code is used by Dell to route support calls to the appropriate personnel.

**Figure 19. Locating the Mini Express Service Tag**



## System information labels

The system information label is on the back side of the system cover.

**Figure 20. Service information**

**Figure 21. Memory information**

**Figure 22. Electrical overview**

**Figure 23. LED behavior**

**Figure 24. Icon legend**

**Figure 25. Configuration and layout for 2.5-inch drive system**

**Figure 26. Configuration and layout for 3.5-inch drive system**

**Figure 27. Configuration and layout for EDSFF E3.S drive system**

**Figure 28. System tasks**

**Figure 29. Heat sink**

**Figure 30. BOSS-N1**

**Figure 31. EDSFF E3.S drive**

**Figure 32. Caution**

**Figure 33. Express service tag**



## Rail sizing and rack compatibility matrix

For specific information about the rail solutions compatible with your system, see the Dell Enterprise Systems Rail Sizing and
Rack Compatibility Matrix.

The document provides the information that is listed below:

- Specific details about rail types and their functionalities.
- Rail adjustability range for various types of rack mounting flanges.
- Rail depth with and without cable management accessories.
- Types of racks that are supported for various types of rack mounting flanges.



# 3 Technical specifications

The technical and environmental specifications of your system are outlined in this section.
**Topics:**

- Chassis dimensions
- System weight
- Processor specifications
- PSU specifications
- Supported operating systems
- Cooling fan specifications
- System battery specifications
- Expansion card riser specifications
- Memory specifications
- Storage controller specifications
- Drives
- Ports and connectors specifications
- Video specifications
- Environmental specifications



## Chassis dimensions

**Figure 34. Chassis dimensions**

**Table 10. PowerEdge R760 chassis dimensions**

|Xa|Xb|Y|Za|Zb|Zc|
|---|---|---|---|---|---|
|482.0 mm (18.97 inches)|434.0 mm (17.08<br>inches)|86.8 mm (3.41<br>inches)|35.84 mm (1.41<br>inches) With<br>bezel<br>22.0 mm (0.86<br>inches)<br>Without bezel|700.7 mm (27.58<br>inches) Ear to rear<br>wall|736.29 mm<br>(28.98 inches)<br>Ear to PSU<br>handle|

**NOTE:** Zb is the nominal rear wall external surface where the system board I/O connectors reside.



## System weight

**Table 11. PowerEdge R760 system weight**

|System configuration|Maximum weight (with all drives/SSDs)|
|---|---|
|A server with fully populated drives|36.1 kg (79.58 lbs)|
|A server without drives and PSU installed|25.1 kg (55.33 lbs)|



## Processor specifications

**Table 12. PowerEdge R760 processor specifications**

|Supported processor|Number of processors supported|
|---|---|
|4th Gen Intel® Xeon® Scalable or Intel® Xeon® Max<br>Processors or 5th Gen Intel® Xeon® Scalable Processors|Up to two|

**Table 13. Minimum Firmware version requirement for 4th Gen Intel® Xeon® Scalable Processors**

|Processors|iDRAC|BIOS|CPLD|
|---|---|---|---|
|8462Y+|6.10.39.00|1.2.1|1.0.5|
|6458Q|6458Q|6458Q|6458Q|
|6448Y|6448Y|6448Y|6448Y|
|6444Y|6444Y|6444Y|6444Y|
|6442Y|6442Y|6442Y|6442Y|
|6438Y+|6438Y+|6438Y+|6438Y+|
|6438N|6438N|6438N|6438N|
|6438M|6438M|6438M|6438M|
|6434|6434|6434|6434|
|6428N|6428N|6428N|6428N|
|6426Y|6426Y|6426Y|6426Y|
|6421N|6421N|6421N|6421N|
|5420+|5420+|5420+|5420+|
|5418Y|5418Y|5418Y|5418Y|
|5418N|5418N|5418N|5418N|
|5416S|5416S|5416S|5416S|
|5415+|5415+|5415+|5415+|
|5412U|5412U|5412U|5412U|
|5411N|5411N|5411N|5411N|
|4416+|4416+|4416+|4416+|
|4410Y|4410Y|4410Y|4410Y|
|3408U|3408U|3408U|3408U|
|9480|6.10.80.00|1.3.2|1.0.5|
|9470|9470|9470|9470|
|9460|9460|9460|9460|
|9462|9462|9462|9462|
|5th Gen Intel® Xeon® Scalable Processors|7.10.05.00|2.0.0|N/A|



## PSU specifications

The PowerEdge R760 system supports up to two AC or DC power supply units (PSUs).

**Table 15. PSU specifications**

|PSU|Class|Heat<br>dissipation<br>(maximum)<br>(BTU/hr)|Frequen<br>cy (Hz)|AC Voltage|Col6|Col7|DC Voltage|Col9|Col10|Current<br>(A)|
|---|---|---|---|---|---|---|---|---|---|---|
|**PSU**|**Class**|**Heat**<br>**dissipation**<br>**(maximum)**<br>**(BTU/hr)**|**Frequen**<br>**cy (Hz)**|**200—**<br>**240 V**|**100—**<br>**120 V**|**277 V**|**240 V**|**- (48—**<br>**60) V**|**336 V**|**336 V**|
|700 W<br>mixed mode<br>HLAC|Titanium|2625|50/60|700 W|N/A|N/A|N/A|N/A|N/A|4.1|
|700 W<br>mixed mode<br>HLAC|N/A|2625|N/A|N/A|N/A|N/A|700 W|N/A|N/A|3.4|
|800 W<br>mixed mode|Platinum|3000|50/60|800 W|800 W|N/A|N/A|N/A|N/A|9.2—4.7|
|800 W<br>mixed mode|N/A|3000|N/A|N/A|N/A|N/A|800 W|N/A|N/A|3.8|
|1100 W<br>mixed mode|Titanium|4100|50/60|1100 W|1050 W|N/A|N/A|N/A|N/A|12—6.3|
|1100 W<br>mixed mode|N/A|4100|N/A|N/A|N/A|N/A|1100 W|N/A|N/A|5.2|
|1400 W<br>mixed mode|Platinum|5250|50/60|1400 W|1050 W|N/A|N/A|N/A|N/A|12—8|
|1400 W<br>mixed mode|N/A|5250|N/A|N/A|N/A|N/A|1400 W|N/A|N/A|6.6|
|1400 W<br>mixed mode<br>277 Vac and<br>HVDC|Titanium|5250|50/60|N/A|N/A|1400<br>W|N/A|N/A|N/A|5.8|
|1400 W<br>mixed mode<br>277 Vac and<br>HVDC|Titanium|5250|N/A|N/A|N/A|N/A|N/A|N/A|1400<br>W|5.17|
|1800 W<br>mixed mode<br>HLAC|Titanium|6750|50/60|1800|N/A|N/A|N/A|N/A|N/A|10|
|1800 W<br>mixed mode<br>HLAC|N/A|6750|N/A|N/A|N/A|N/A|1800 W|N/A|N/A|8.2|
|2400 W<br>mixed mode|Platinum|9000|50/60|2400 W|1400 W|N/A|N/A|N/A|N/A|16—13.5|
|2400 W<br>mixed mode|N/A|9000|N/A|N/A|N/A|N/A|2400 W|N/A|N/A|11.2|
|2800 W<br>mixed mode<br>HLAC|Titanium|10500|50/60|2800 W|N/A|N/A|N/A|N/A|N/A|15.6|
|2800 W<br>mixed mode<br>HLAC|N/A|10500|N/A|N/A|N/A|N/A|2800 W|N/A|N/A|13.6|
|1100 W -48<br>V DC|N/A|4265|N/A|N/A|N/A|N/A|N/A|1100 W|N/A|27|
|3200 W<br>mixed mode<br>277 Vac and<br>HVDC|Titanium|12000|50/60|N/A|N/A|3200<br>W|N/A|N/A|N/A|13|
|3200 W<br>mixed mode<br>277 Vac and<br>HVDC|Titanium|12000|N/A|N/A|N/A|N/A|N/A|N/A|3200<br>W|11.5|

**NOTE:** Heat dissipation is calculated using the PSU wattage rating.

**NOTE:** When selecting or upgrading the system configuration, to ensure optimum power utilization, verify the system
[power consumption with the Enterprise Infrastructure Planning Tool available at calc.](https://dell-eipt-landingpage.azurewebsites.net/)

**NOTE:** If a system with AC 2400 W PSUs operates at low line 100-120 Vac, then the power rating per PSU is degraded to
1400 W.

**NOTE:** If a system with AC 1400 W or 1100 W PSUs operates at low line 100-120 Vac, then the power rating per PSU is
degraded to 1050 W.

**NOTE:**

  - HLAC stands for High-Line AC, with a range of 200 - 240 V AC.
  - HVDC stands for High-Voltage DC, with 336 V DC.
**Figure 35. PSU power cables**

**Figure 36. 277VAC/HVDC power cable**

**Table 16. PSU power cables**

|Form factor|Output|Power cable|
|---|---|---|
|Redundant 60 mm|700 W mixed mode HLAC|C13|
|Redundant 60 mm|800 W mixed mode|C13|
|Redundant 60 mm|1100 W mixed mode|C13|
|Redundant 60 mm|1400 W mixed mode|C13|
|Redundant 60 mm|1400 W mixed mode 277 Vac<br>and HVDC|277VAC/HVDC|
|Redundant 60 mm|1800 W mixed mode HLAC|C15|
|Redundant 86 mm|2400 W mixed mode|C19|
|Redundant 86 mm|2800 W mixed mode HLAC|C21|
|Redundant 86 mm|3200 W mixed mode 277 Vac<br>and HVDC|277VAC/HVDC|

**NOTE:** C19 power cable combined with C20 to C21 jumper power cable can be used to adapt 2800 W PSU.

**NOTE:** C13 power cable combined with C14 to C15 jumper power cable can be used to adapt 1800 W PSU.



## Supported operating systems

The PowerEdge R760 system supports the following operating systems:

- Canonical Ubuntu Server LTS
- Microsoft Windows Server with Hyper-V
- Red Hat Enterprise Linux
- SUSE Linux Enterprise Server
- VMware ESXi

[For more information, go to Operating System Manuals.](https://www.dell.com/support/home/en-us/products/software_int/software_operating_systems)



## Cooling fan specifications





###### Cooling options

The PowerEdge R760 requires various cooling components that are based on processor TDP, storage modules, rear drives, and
GPU to maintain optimum thermal performance.

The PowerEdge R760 offers two types of cooling options:

- Air cooling
- Direct Liquid Cooling (DLC) (optional)



## Cooling fan specifications

The PowerEdge R760 system system supports up to six standard (STD), High performance Silver (HPR Silver) grade, or High
performance Gold (HPR Gold) grade cooling fans.

**Table 17. Cooling fan specifications**

|Fan type|Abbreviation|Label color|Label image|
|---|---|---|---|
|Standard (STD)<br>fans|STD|No label||
|High<br>performance<br>Silver (HPR<br>Silver) fans|HPR Silver|Silver||
|High<br>performance<br>Gold (HPR Gold)<br>fans|HPR Gold|Gold||

**NOTE:** See the Thermal restriction matrix for required fan support with air cooled and DLC configurations.



## System battery specifications

The PowerEdge R760 system uses one CR 2032 3.0 V lithium coin cell battery.



## Expansion card riser specifications

The PowerEdge R760 system supports up to eight PCI express (PCIe) slots (six full lengths and two low profiles) on the system
board.

**Table 18. Expansion card slots supported on the system board**

|PCIe<br>slot|With<br>Regula<br>r<br>shroud|With<br>GPGPU<br>shroud|R1B|R1P|R1Q|R1R|R2A|R3A|R3B|R4B|R4P|R4Q|R4R|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|Slot 1|Full<br>height -<br>Half<br>length|Full<br>height -<br>Full<br>length|x8|-|x8<br>(Gen5)|x16|-|-|-|-|-|-|-|
|Slot 2|Full<br>height -<br>Half<br>length|Full<br>height -<br>Full<br>length|x8|x16<br>(Gen<br>5)<br>(Dou<br>ble<br>widt<br>h <br>GPU<br>)|x8<br>(Gen5)|x16<br>(Gen<br>5)|-|-|-|-|-|-|-|
|Slot 3|Low<br>profile -<br>Half<br>length|Low<br>profile -<br>Half<br>length|-|-|-|-|x16|-|-|-|-|-|-|
|Slot 4|Full<br>height -<br>Half<br>length|Full<br>height -<br>Half<br>length|-|-|-|-|-|-|x8|-|-|-|-|
|Slot 5|Full<br>height -<br>Half<br>length|Full<br>height -<br>Full<br>length|-|-|-|-|-|x16|x8|-|-|-|-|
|Slot 6|Low<br>profile -<br>Half<br>length|Low<br>profile -<br>Half<br>length|-|-|-|-|x16|-|-|-|-|-|-|
|Slot 7|Full<br>height -<br>Half<br>length|Full<br>height -<br>Full<br>length|-|-|-|-|-|-|-|x8|x16<br>(Gen5)<br>(Double<br>width<br>GPU)|x8<br>(Gen5)|-|
|Slot 7<br>SNAPI|Full<br>height-<br>Half<br>length|Full<br>height -<br>Full<br>length|-|-|-|-|-|-|-|-|-|-|x8+x8<br>(Gen5<br>)|
|Slot 8|Full<br>height -|Full<br>height -|-|-|-|-|-|-|-|x8|-|x8<br>(Gen5)|x8<br>(Gen5<br>)|
||Half<br>length|Half<br>length||||||||||||



## Memory specifications

The PowerEdge R760 system supports the following memory specifications for optimized operation.

**Table 19. Memory specifications for 4** **[th]** **Gen Intel® Xeon® Scalable or Intel® Xeon® Max Processors**

|DIMM type|DIMM rank|DIMM<br>capacity|Single processor|Col5|Dual processors|Col7|
|---|---|---|---|---|---|---|
|**DIMM type**|**DIMM rank**|**DIMM**<br>**capacity**|**Minimum**<br>**system**<br>**capacity**|**Maximum system**<br>**capacity**|**Minimum**<br>**system**<br>**capacity**|**Maximum system**<br>**capacity**|
|DDR5 RDIMM|Single rank|16 GB|16 GB|256 GB|32 GB|512 GB|
|DDR5 RDIMM|Dual rank|32 GB|32 GB|512 GB|64 GB|1 TB|
|DDR5 RDIMM|Dual rank|64 GB|64 GB|1 TB|128 GB|2 TB|
|DDR5 RDIMM|Quad rank|128 GB|128 GB|2 TB|256 GB|4 TB|
|DDR5 RDIMM|Octa rank|256 GB|256 GB|4 TB|512 GB|8 TB|

**Table 20. Memory module sockets**

|Memory module sockets|Rated DIMM speed|
|---|---|
|32 (288-pin)|4800 MT/s|

**NOTE:** The processor may reduce the performance of the rated DIMM speed.

**Table 21. Memory specifications for 5** **[th]** **Gen Intel® Xeon® Scalable Processors**

|DIMM type|DIMM rank|DIMM<br>capacity|Single processor|Col5|Dual processors|Col7|
|---|---|---|---|---|---|---|
|**DIMM type**|**DIMM rank**|**DIMM**<br>**capacity**|**Minimum**<br>**system**<br>**capacity**|**Maximum system**<br>**capacity**|**Minimum**<br>**system**<br>**capacity**|**Maximum system**<br>**capacity**|
|DDR5 RDIMM|Single rank|16 GB|16 GB|256 GB|32 GB|512 GB|
|DDR5 RDIMM|Dual rank|32 GB|32 GB|512 GB|64 GB|1 TB|
|DDR5 RDIMM|Dual rank|64 GB|64 GB|1 TB|128 GB|2 TB|
|DDR5 RDIMM|Dual rank|96 GB|96 GB|1.5 TB|192 GB|3 TB|
|DDR5 RDIMM|Quad rank|128 GB|128 GB|2 TB|256 GB|4 TB|
|DDR5 RDIMM|Octa rank|256 GB|256 GB|4 TB|512 GB|8 TB|

**NOTE:** DDR4 memories are not supported in the R760.

**Table 22. Memory module sockets**

|Memory module sockets|Rated DIMM speed|
|---|---|
|32 (288-pin)|5600 MT/s|

**NOTE:** The processor may reduce the performance of the rated DIMM speed.

**NOTE:** Memory DIMM slots are not hot pluggable.



## Storage controller specifications

The PowerEdge R760 system supports the following controller cards:

**Table 23. Storage controller cards**

**Supported storage controller cards**

Internal controllers

 - PERC H965i
 - PERC H755
 - PERC H755N
 - PERC H355

External controllers

 - PERC H965e

Internal Boot

 - Boot Optimized Storage Subsystem (BOSS-N1): HWRAID 2 x M.2 NVMe SSD
 - USB

Software RAID

 - S160

SAS Host Bus Adapters (HBA) non-RAID

 - HBA355e
 - HBA355i
 - HBA465i



## Drives

The PowerEdge R760 system supports:

- 12 x 3.5-inch hot-swappable SAS or SATA drives
- 8 x 2.5-inch hot-swappable SAS, SATA, or NVMe drives
- 16 x 2.5-inch hot-swappable SAS, SATA, or NVMe drives
- 24 x 2.5-inch hot-swappable SAS or SATA, or NVMe drives
- 16 x EDSFF E3.S hot-swappable NVMe drives
- 2 x 2.5-inch rear hot-swappable SAS, SATA, or NVMe drives
- 4 x 2.5-inch rear hot-swappable SAS, SATA, or NVMe drives
- 4 x EDSFF E3.S rear hot-swappable NVMe drives
**NOTE:** For more information about how to hot swap NVMe PCIe SSD U.2 device, see the _Dell Express Flash NVMe PCIe_
_SSD User's Guide_ [at Dell Support](https://www.dell.com/support/home/en-us) **> Browse all products**   - **Infrastructure**   - **Data Center Infrastructure**   - **Storage**
**Adapters & Controllers**   - **Dell PowerEdge Express Flash NVMe PCIe SSD**   - **Select This Product**   - **Documentation**

  - **Manuals and Documents** .



## Ports and connectors specifications





### USB ports specifications

**Table 24. PowerEdge R760 USB specifications**

|Front|Col2|Rear|Col4|Internal (optional)|Col6|
|---|---|---|---|---|---|
|**USB port type**|**No. of ports**|**USB port type**|**No. of ports**|**USB port type**|**No. of ports**|
|USB 2.0-<br>compliant port|One|USB 2.0-<br>compliant port|One|Internal USB 3.0-<br>compliant port|One|
|iDRAC Direct<br>port (Micro-AB<br>USB 2.0-<br>compliant port)|One|USB 3.0-<br>compliant port|One|One|One|

**NOTE:** The micro USB 2.0 compliant port can only be used as an iDRAC Direct or a management port.



### NIC port specifications

The PowerEdge R760 system supports up to two Network Interface Controller (NIC) ports embedded on the LAN on
Motherboard (LOM) card and up to four ports integrated on the Open Compute Project (OCP) NIC card.

**Table 25. NIC port specification for the system**

|Feature|Specifications|
|---|---|
|LOM card (optional)|1 GbE x 2|
|OCP NIC card (OCP NIC 3.0) (optional)|1GbE x 4, 10 GbE x 2, 10 GbE x 4, 25 GbE x 2, 25 GbE x 4,<br>100GbE x 2|
|Management Interface Card (MIC) to support Dell Data<br>Processing Unit (DPU) card (optional)|25 GbE x 2, 100 GbE x 2 or 200 GbE x 2|

**NOTE:** The system allows either LOM card or an OCP NIC card or both to be installed in the system.

**NOTE:** On the MS system board, the supported OCP NIC PCIe width is x8; when x16 PCIe width is installed, it is
downgraded to x8.

**NOTE:** A 100 GbE OCP NIC card of PCIe width x16 can be used by connecting the OCP NIC cable from SL11_CPU1_PB7 to
SL13_CPU1_PB7 on the MAX system board.

**NOTE:** For storage configurations that already use the SL11_CPU1_PB7 or SL13_CPU1_PB7 connector on the Max system
board, there is a restriction on supporting OCP NIC cable.

**NOTE:** The system allows either LOM card or MIC card to be installed in the system.



### Serial connector specifications

The PowerEdge R760 system supports one serial port on the system board, which is Data Terminal Equipment (DTE), 16550compliant.

The serial connector is installed as default on the system board.



### VGA ports specifications

The PowerEdge R760 system supports DB-15 VGA port on front panel and on rear I/O board (optional for Direct Liquid Cooling
configuration).



## Video specifications

The PowerEdge R760 system supports integrated Matrox G200 graphics controller with 16 MB of video frame buffer.

**Table 26. Supported video resolution options**

|Resolution|Refresh rate (Hz)|Color depth (bits)|
|---|---|---|
|1024 x 768|60|8, 16, 32|
|1280 x 800|60|8, 16, 32|
|1280 x 1024|60|8, 16, 32|
|1360 x 768|60|8, 16, 32|
|1440 x 900|60|8, 16, 32|
|1600 x 900|60|8, 16, 32|
|1600 x 1200|60|8, 16, 32|
|1680 x 1050|60|8, 16, 32|
|1920 x 1080|60|8, 16, 32|
|1920 x 1200|60|8, 16, 32|



## Environmental specifications

**NOTE:** For additional information about environmental certifications, refer to the _Product Environmental Datasheet_ located
with the _Documentation_ [on Dell Support.](https://www.dell.com/support/home/en-us)

**Table 27. Continuous Operation Specifications for ASHRAE A2**

|Temperature|Specifications|
|---|---|
|Allowable continuous operations|Allowable continuous operations|
|Temperature range for<br>altitudes <= 900 m (<=<br>2953 ft)|10–35°C (50–95°F) with no direct sunlight on the equipment|
|Humidity percent range<br>(non-condensing at all<br>times)|8% RH with -12°C (10.4°F) minimum dew point to 80% RH with 21°C (69.8°F) maximum dew<br>point|
|Operational altitude de-<br>rating|Maximum temperature is reduced by 1°C/300 m (1.8°F/984 Ft) above 900 m (2953 Ft)|

**Table 28. Continuous Operation Specifications for ASHRAE A3**

|Temperature|Specifications|Col3|
|---|---|---|
|Allowable continuous operations|Allowable continuous operations|Allowable continuous operations|
|Temperature range for<br>altitudes <= 900 m (<=<br>2953 ft)|5–40°C (41–104°F) with no direct sunlight on the equipment|5–40°C (41–104°F) with no direct sunlight on the equipment|
|Temperature range for<br>altitudes <= 900 m (<=<br>2953 ft)|Excursion Limited Operation|5-35°C (41-95°F) Continuous Operation|
|Temperature range for<br>altitudes <= 900 m (<=<br>2953 ft)|Excursion Limited Operation|35-40°C (95-104°F) 10% Annual Runtime|

**Table 28. Continuous Operation Specifications for ASHRAE A3 (continued)**

|Temperature|Specifications|
|---|---|
|Humidity percent range<br>(non-condensing at all<br>times)|8% RH with -12°C (10.4°F) minimum dew point to 85% RH with 24°C (75.2°F) maximum dew<br>point|
|Operational altitude de-<br>rating|Maximum temperature is reduced by 1°C/175 m (1.8°F/574 Ft) above 900 m (2953 Ft)|

**Table 29. Continuous Operation Specifications for ASHRAE A4**

|Temperature|Specifications|Col3|
|---|---|---|
|Allowable continuous operations|Allowable continuous operations|Allowable continuous operations|
|Temperature range for<br>altitudes <= 900 m (<=<br>2953 ft)|5–45°C (41–113°F) with no direct sunlight on the equipment|5–45°C (41–113°F) with no direct sunlight on the equipment|
|Temperature range for<br>altitudes <= 900 m (<=<br>2953 ft)|Excursion Limited Operation|5-35°C (41-95°F) Continuous Operation|
|Temperature range for<br>altitudes <= 900 m (<=<br>2953 ft)|Excursion Limited Operation|35-40°C (95-104°F) 10% Annual Runtime|
|Temperature range for<br>altitudes <= 900 m (<=<br>2953 ft)|Excursion Limited Operation|40-45°C (104-113°F) 1% Annual Runtime|
|Humidity percent range<br>(non-condensing at all<br>times)|8% RH with -12°C (10.4°F) minimum dew point to 90% RH with 24°C (75.2°F) maximum dew<br>point|8% RH with -12°C (10.4°F) minimum dew point to 90% RH with 24°C (75.2°F) maximum dew<br>point|
|Operational altitude de-<br>rating|Maximum temperature is reduced by 1°C/125 m (1.8°F/410 Ft) above 900 m (2953 Ft)|Maximum temperature is reduced by 1°C/125 m (1.8°F/410 Ft) above 900 m (2953 Ft)|

**Table 30. Common Environmental Specifications for ASHRAE A2, A3 and A4**

|Temperature|Specifications|
|---|---|
|Allowable continuous operations|Allowable continuous operations|
|Maximum temperature gradient (applies to both<br>operation and non-operation)|20°C in an hour* (36°F in an hour) and 5°C in 15 minutes (9°F in 15<br>minutes), 5°C in an hour* (9°F in an hour) for tape hardware<br>**NOTE:** * - Per ASHRAE thermal guidelines for tape hardware, these are<br>not instantaneous rates of temperature change.|
|Non-operational temperature limits|-40 to 65°C (-40 to 149°F)|
|Non-operational humidity limits|5% to 95% RH with 27°C (80.6°F) maximum dew point|
|Maximum non-operational altitude|12,000 meters (39,370 feet)|
|Maximum operational altitude|3,050 meters (10,006 feet)|

**Table 31. Maximum vibration specifications**

|Maximum vibration|Specifications|
|---|---|
|Operating|0.21 Grms at 5 Hz to 500 Hz for 10 minutes (all operation orientations)|
|Storage|1.88 Grms at 10 Hz to 500 Hz for 15 minutes (all six sides tested)|

**Table 32. Maximum shock pulse specifications**

|Maximum shock pulse|Specifications|
|---|---|
|Operating|Six consecutively executed shock pulses in the positive and negative x, y,<br>and z axis of 6 G for up to 11 ms|
|Storage|Six consecutively executed shock pulses in the positive and negative x, y,<br>and z axis (one pulse on each side of the system) of 71 G for up to 2 ms|



### Particulate and gaseous contamination specifications

The following table defines the limitations that help avoid any equipment damage or failure from particulates and gaseous
contamination. If the levels of particulates or gaseous pollution exceed the specified limitations and result in equipment damage
or failure, you may need to rectify the environmental conditions. Remediation of environmental conditions is the responsibility of
the customer.

**Table 33. Particulate contamination specifications**

|Particulate contamination|Specifications|
|---|---|
|Air filtration|Data center air filtration as defined by ISO Class 8 per ISO 14644-1<br>with a 95% upper confidence limit<br>**NOTE:** This condition applies to data center environments only. Air<br>filtration requirements do not apply to IT equipment designed to be<br>used outside a data center, in environments such as an office or<br>factory floor.<br>**NOTE:** Air entering the data center must have MERV11 or MERV13<br>filtration.|
|Conductive dust|Air must be free of conductive dust, zinc whiskers, or other<br>conductive particles<br>**NOTE:** This condition applies to data center and non-data center<br>environments.|
|Corrosive dust|- Air must be free of corrosive dust<br>- Residual dust present in the air must have a deliquescent point less<br>than 60% relative humidity<br>**NOTE:** This condition applies to data center and non-data center<br>environments.|
|Walk-Up Edge Data Center or Cabinet (sealed, closed<br>loop environment)|Filtration is not required for cabinets that are anticipated to be opened<br>6 times or less per year. Class 8 per ISO 1466-1 filtration as defined<br>above is required otherwise<br>**NOTE:** In environments commonly above ISA-71 Class G1 or that<br>may have known challenges, special filters may be required.|

**Table 34. Gaseous contamination specifications**

|Gaseous contamination|Specifications|
|---|---|
|Copper coupon corrosion rate|<300 Å/month per Class G1 as defined by ANSI/ISA71.04-2013|
|Silver coupon corrosion rate|<200 Å/month as defined by ANSI/ISA71.04-2013|

### Particulate and gaseous contamination specifications

The following table defines the limitations that help avoid any equipment damage or failure from particulates and gaseous
contamination. If the levels of particulates or gaseous pollution exceed the specified limitations and result in equipment damage
or failure, you must rectify the environmental conditions. Remediation of environmental conditions is the responsibility of the
customer.

**Table 35. Particulate contamination specifications**

|Particulate contamination|Specifications|
|---|---|
|Air filtration: Conventional Data Center only|Data center air filtration as defined by ISO Class 8 per ISO 14644-1<br>with a 95% upper confidence limit<br>**NOTE:** Filtering room air with a MERV8 filter, as specified<br>in ANSI/ASHRAE Standard 127, is a recommended method for<br>achieving the necessary environmental conditions.|
||**NOTE:** Air entering the data center must have MERV11 or MERV13<br>filtration.<br>**NOTE:** This condition applies to data center environments only. Air<br>filtration requirements do not apply to IT equipment designed to be<br>used outside a data center, in environments such as an office or<br>factory floor.|
|Walk-Up Edge Data Center or Cabinet (sealed, closed<br>loop environment)|Filtration is not required for cabinets that are anticipated to be opened<br>six times or less per year. Class 8 per ISO 1466-1 filtration as defined<br>above is required otherwise.<br>**NOTE:** In environments commonly above ISA-71 Class G1 or that<br>may have known challenges, special filters may be required.|
|Conductive dust: data center and non-data center<br>environments|Air must be free of conductive dust, zinc whiskers, or other<br>conductive particles.<br>**NOTE:** Conductive dust, which can interfere with equipment<br>operation, can originate from various sources, including<br>manufacturing processes and zinc whiskers that may develop on<br>the plating of raised floor tiles.<br>**NOTE:** This condition applies to data center and non-data center<br>environments.|
|Corrosive dust: data center and non-data center<br>environments|- Air must be free of corrosive dust.<br>- Residual dust present in the air must have a deliquescent point less<br>than 60% relative humidity.<br>**NOTE:** This condition applies to data center and non-data center<br>environments.|

**Table 36. Gaseous contamination specifications**

|Gaseous contamination|Specifications|Notes|
|---|---|---|
|Copper coupon corrosion rate|ISA-71 Class G1: <300 Å/month|Per ANSI/ISA71.04|
|Silver coupon corrosion rate|ISA-71 Class G1: <200 Å/month|Per ANSI/ISA71.04|



### Thermal restriction matrix

**Table 37. Processor and heat sink matrix**

|Heat sink|Processor TDP|
|---|---|
|STD HSK|≤ 165 W (supports only 2.5-inch drives and non-GPU<br>configuration)|
|2U HPR HSK|125 W–250 W (supports 3.5-inch drives and non-GPU<br>configuration)|
|2U HPR HSK|165 W–350 W (supports 2.5-inch drives and non-GPU<br>configuration)|
|L-type HSK|Supports all GPU/FPGA configurations|

**NOTE:** All GPU/FGPA cards require 1U L-type HSK and GPU shroud.

**Table 38. Label reference**

|Label|Description|
|---|---|
|STD|Standard|
|HPR (Silver)|High performance Silver (HPR Silver) fan|
|HPR (Gold)|High performance Gold (HPR Gold) fan|
|HSK|Heat sink|
|LP|Low profile|
|FH|Full height|
|DLC|Direct Liquid Cooling|

**NOTE:** The ambient temperature of the configuration is determined by the critical component in that configuration. For
example, if the processor's supported ambient temperature is 35°C (95°F), the DIMM is 35°C (95°F), and the GPU is
30°C (86°F), the combined configuration can only support 30°C (86°F).

Thermal restriction matrix for 4 [th] Gen Intel® Xeon® Scalable or Intel® Xeon® Max
Processors

**Table 39. Thermal restriction matrix for air cooled configuration**

|Configuration|Col2|Col3|Col4|No<br>back<br>plan<br>e|8 x<br>2.5-<br>inc<br>h<br>NV<br>Me|16 x<br>2.5-<br>inch<br>SAS<br>and<br>Split<br>NVMe-<br>SAS|16 x<br>2.5-<br>inch<br>or 16 x<br>EDSFF<br>E3.S<br>NVMe|24 x 2.5-inch<br>SAS|Col10|16 x<br>2.5-<br>inch<br>SAS +<br>8 x<br>2.5-<br>inch<br>NVMe|24 x<br>2.5-<br>inch<br>NVM<br>e|12 x 3.5-<br>inch|Col14|Ambien<br>t<br>temper<br>ature|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|**Rear storage**|**Rear storage**|**Rear storage**|**Rear storage**|**No**<br>**rear**<br>**drive**<br>**s**|**No**<br>**rear**<br>**driv**<br>**es**|**No**<br>**rear**<br>**drives**|**No**<br>**rear**<br>**drives**|**No**<br>**re**<br>**ar**<br>**dri**<br>**ve**<br>**s**|**2.5-inch**<br>**or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear fan**|**No**<br>**rear**<br>**drives**|**No**<br>**rear**<br>**driv**<br>**es**|**No**<br>**rear**<br>**driv**<br>**es**|**2.5-**<br>**inch or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear**<br>**fan**|**2.5-**<br>**inch or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear**<br>**fan**|
|**CPU TDP/cTDP**|**CPU TDP/cTDP**|**Cores**|**T-**<br>**Case**<br>**max**<br>**cent**<br>**er**<br>**(°C)**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**HPR GOLD**<br>**fan 70%^**|**HPR GOLD**<br>**fan 70%^**|**HPR GOLD**<br>**fan 70%^**|
|3408U|125 W1|8|79|STD|STD|STD|STD|ST<br>D|HPR<br>SLVR|STD|HPR<br>GOL<br>D|HPR<br>SLV<br>R|HPR<br>GOLD|35°C<br>(95°F)|
|5415+|150 W1|8|78|STD|STD|STD|STD|ST<br>D|HPR<br>SLVR|STD|HPR<br>GOL<br>D|HPR<br>SLV<br>R|HPR<br>GOLD|35°C<br>(95°F)|
|4410Y|4410Y|12|78|78|78|78|78|78|78|78|78|78|78|78|
|5416S|5416S|16|78|78|78|78|78|78|78|78|78|78|78|78|
|5418N|165 W1|24|84|STD|STD|STD|STD|ST<br>D|HPR<br>SLVR|STD|HPR<br>GOL<br>D|HPR<br>SLV<br>R|HPR<br>GOLD|35°C<br>(95°F)|
|5411N|5411N|24|84|84|84|84|84|84|84|84|84|84|84|84|
|4416+|4416+|20|82|82|82|82|82|82|82|82|82|82|82|82|
|6426Y|185 W1|16|72|STD|STD|STD|STD|ST<br>D|HPR<br>SLVR|HPR<br>SLVR|HPR<br>GOL<br>D|HPR<br>GOL<br>D|HPR<br>GOLD|35°C<br>(95°F)|
|5418Y|5418Y|24|80|80|80|80|80|80|80|80|80|80|80|80|
|**Rear storage**|**Rear storage**|**Rear storage**|**Rear storage**|**No**<br>**rear**<br>**drive**<br>**s**|**No**<br>**rear**<br>**driv**<br>**es**|**No**<br>**rear**<br>**drives**|**No**<br>**rear**<br>**drives**|**No**<br>**re**<br>**ar**<br>**dri**<br>**ve**<br>**s**|**2.5-inch**<br>**or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear fan**|**No**<br>**rear**<br>**drives**|**No**<br>**rear**<br>**driv**<br>**es**|**No**<br>**rear**<br>**driv**<br>**es**|**2.5-**<br>**inch or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear**<br>**fan**|**2.5-**<br>**inch or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear**<br>**fan**|
|**CPU TDP/cTDP**|**CPU TDP/cTDP**|**Cores**|**T-**<br>**Case**<br>**max**<br>**cent**<br>**er**<br>**(°C)**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**HPR GOLD**<br>**fan 70%^**|**HPR GOLD**<br>**fan 70%^**|**HPR GOLD**<br>**fan 70%^**|
|5412U||24|80||||||||||||
|6428N|6428N|32|85|85|85|85|85|85|85|85|85|85|85|85|
|6421N|6421N|32|85|85|85|85|85|85|85|85|85|85|85|85|
|6434|205 W1|8|96|STD|STD|STD|STD|ST<br>D|HPR<br>SLVR|HPR<br>SLVR|HPR<br>GOL<br>D|HPR<br>GOL<br>D|HPR<br>GOLD|35°C<br>(95°F)|
|5420+|5420+|28|84|84|84|84|84|84|84|84|84|84|84|84|
|6438Y+|6438Y+|32|76|76|76|76|76|76|76|76|76|76|76|76|
|6438M|6438M|32|84|84|84|84|84|84|84|84|84|84|84|84|
|6438N|6438N|32|84|84|84|84|84|84|84|84|84|84|84|84|
|6442Y|225 W1|24|79|STD|STD|STD|STD|ST<br>D|HPR<br>SLVR|HPR<br>SLVR|HPR<br>GOL<br>D|HPR<br>GOL<br>D*|HPR<br>GOLD*|35°C<br>(95°F)|
|6448Y|6448Y|32|79|79|79|79|79|79|79|79|79|79|79|79|
|6444Y|270 W2|32|75|HPR<br>SLVR|HPR<br>SLV<br>R|HPR<br>SLVR|HPR<br>SLVR|HP<br>R <br>SL<br>VR|HPR<br>SLVR|HPR<br>SLVR|HPR<br>GOL<br>D|Req<br>uire<br>d <br>DLC|Require<br>d DLC|35°C<br>(95°F)|
|8462Y+|300 W2|32|81|HPR<br>SLVR|HPR<br>SLV<br>R|HPR<br>SLVR|HPR<br>SLVR|HP<br>R <br>SL<br>VR|HPR<br>SLVR fan|HPR<br>SLVR|HPR<br>GOL<br>D|Req<br>uire<br>d <br>DLC|Require<br>d DLC|35°C<br>(95°F)|
|6458Q|350 W2|32|64|Requi<br>red<br>DLC|Req<br>uire<br>d <br>DLC|Require<br>d DLC|Requir<br>ed DLC|Re<br>qui<br>red<br>DL<br>C|Required<br>DLC|Requir<br>ed<br>DLC|Requ<br>ired<br>DLC|Req<br>uire<br>d <br>DLC|Require<br>d DLC|35°C<br>(95°F)|
|6414U|250 W2|32|76|STD<br>fan|STD<br>fan|STD<br>fan|STD<br>fan|ST<br>D <br>fan|HPR<br>SLVR fan|HPR<br>SLVR|HPR<br>GOL<br>D|HPR<br>GOL<br>D*|HPR<br>GOLD*|35°C<br>(95°F)|
|6454S|270 W2|32|71|HPR<br>SLVR|HPR<br>SLV<br>R|HPR<br>SLVR|HPR<br>SLVR|HP<br>R <br>SL<br>VR|HPR<br>SLVR fan|HPR<br>SLVR|HPR<br>GOL<br>D|Req<br>uire<br>d <br>DLC|Require<br>d DLC|35°C<br>(95°F)|
|6430|6430|32|71|71|71|71|71|71|71|71|71|71|71|71|

**Table 39. Thermal restriction matrix for air cooled configuration (continued)**

|Configuration|Col2|Col3|Col4|No<br>back<br>plan<br>e|8 x<br>2.5-<br>inc<br>h<br>NV<br>Me|16 x<br>2.5-<br>inch<br>SAS<br>and<br>Split<br>NVMe-<br>SAS|16 x<br>2.5-<br>inch<br>or 16 x<br>EDSFF<br>E3.S<br>NVMe|24 x 2.5-inch<br>SAS|Col10|16 x<br>2.5-<br>inch<br>SAS +<br>8 x<br>2.5-<br>inch<br>NVMe|24 x<br>2.5-<br>inch<br>NVM<br>e|12 x 3.5-<br>inch|Col14|Ambien<br>t<br>temper<br>ature|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|**Rear storage**|**Rear storage**|**Rear storage**|**Rear storage**|**No**<br>**rear**<br>**drive**<br>**s**|**No**<br>**rear**<br>**driv**<br>**es**|**No**<br>**rear**<br>**drives**|**No**<br>**rear**<br>**drives**|**No**<br>**re**<br>**ar**<br>**dri**<br>**ve**<br>**s**|**2.5-inch**<br>**or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear fan**|**No**<br>**rear**<br>**drives**|**No**<br>**rear**<br>**driv**<br>**es**|**No**<br>**rear**<br>**driv**<br>**es**|**2.5-**<br>**inch or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear**<br>**fan**|**2.5-**<br>**inch or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear**<br>**fan**|
|**CPU TDP/cTDP**|**CPU TDP/cTDP**|**Cores**|**T-**<br>**Case**<br>**max**<br>**cent**<br>**er**<br>**(°C)**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**HPR GOLD**<br>**fan 70%^**|**HPR GOLD**<br>**fan 70%^**|**HPR GOLD**<br>**fan 70%^**|
|8471N|300 W2|52|76|HPR<br>SLVR|HPR<br>SLV<br>R|HPR<br>SLVR|HPR<br>SLVR|HP<br>R <br>SL<br>VR|HPR<br>SLVR|HPR<br>SLVR|HPR<br>GOL<br>D|Req<br>uire<br>d <br>DLC|Require<br>d DLC|35°C<br>(95°F)|
|8470N|8470N|52|76|76|76|76|76|76|76|76|76|76|76|76|
|8460Y+|8460Y+|40|75|75|75|75|75|75|75|75|75|75|75|75|
|8452Y|8452Y|36|75|75|75|75|75|75|75|75|75|75|75|75|
|8480+|350 W2|56|79|HPR<br>SLVR|HPR<br>SLV<br>R|HPR<br>SLVR|HPR<br>SLVR|HP<br>R <br>SL<br>VR|HPR<br>SLVR|HPR<br>SLVR|HPR<br>GOL<br>D*|Req<br>uire<br>d <br>DLC|Require<br>d DLC|35°C<br>(95°F)|
|8470|8470|52|79|79|79|79|79|79|79|79|79|79|79|79|
|8468|8468|48|79|79|79|79|79|79|79|79|79|79|79|79|
|8470Q|350 W2|52|57|Requi<br>red<br>DLC|Req<br>uire<br>d <br>DLC|Require<br>d DLC|Requir<br>ed DLC|Re<br>qui<br>red<br>DL<br>C|Required<br>DLC|Requir<br>ed<br>DLC|Requ<br>ired<br>DLC|Req<br>uire<br>d <br>DLC|Require<br>d DLC|35°C<br>(95°F)|
|9480|350 W2|56|64|Requi<br>red<br>DLC|Req<br>uire<br>d <br>DLC|Require<br>d DLC|Requir<br>ed DLC|Re<br>qui<br>red<br>DL<br>C|Required<br>DLC|Requir<br>ed<br>DLC|Requ<br>ired<br>DLC|Req<br>uire<br>d <br>DLC|Require<br>d DLC|35°C<br>(95°F)|
|9470|9470|52|64|64|64|64|64|64|64|64|64|64|64|64|
|9460|350 W2|40|77|HPR<br>SLVR|HPR<br>SLV<br>R|HPR<br>SLVR|HPR<br>SLVR|HP<br>R <br>SL<br>VR|HPR<br>SLVR|HPR<br>SLVR|HPR<br>GOL<br>D*|Req<br>uire<br>d <br>DLC|Require<br>d DLC|35°C<br>(95°F)|
|9462|9462|32|77|77|77|77|77|77|77|77|77|77|77|77|

**NOTE:** The platform supports Maximum (MAX) and Mainstream (MS) system boards.

- 1 supports MS system board (CPU TDP < 250 W)
- 2 supports MAX system board (CPU TDP => 250 W)
**NOTE:** ^The fan speed in the 3.5-inch chassis is limited to 70% due to the drive dynamic profile.

**NOTE:** *Supported ambient temperature is 30°C (86°F).

**Table 40. Thermal restriction matrix for memory with air cooled configuration (non-GPU)**

|Configuration|Col2|No<br>backpl<br>ane|8 x 2.5-<br>inch<br>NVMe|16 x<br>2.5-<br>inch<br>SAS<br>and<br>Split<br>NVMe-<br>SAS|16 x 2.5-<br>inch or 16<br>x EDSFF<br>E3.S<br>NVMe|24 x 2.5-inch<br>SAS|Col8|16 x<br>2.5-<br>inch<br>SAS +<br>8 x<br>2.5-<br>inch<br>NVMe|24 x 2.5-<br>inch<br>NVMe|12 x 3.5-inch|Col12|
|---|---|---|---|---|---|---|---|---|---|---|---|
|**Rear storage**|**Rear storage**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No**<br>**rear**<br>**drive**<br>**s**|**2.5-inch**<br>**or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear fan**|**No**<br>**rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**2.5-inch**<br>**or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with rear**<br>**fan**|
|**DIMM**<br>**Configur**<br>**ation**|**2DP**<br>**C/**<br>**Pow**<br>**er**|**STD fan (CPU TDP <= 250 W)**|**STD fan (CPU TDP <= 250 W)**|**STD fan (CPU TDP <= 250 W)**|**STD fan (CPU TDP <= 250 W)**|**STD fan (CPU TDP <= 250 W)**|**HPR**<br>**SLVR**<br>**fan**<br>**(CPU**<br>**TDP up**<br>**to 350**<br>**W)**|**STD**<br>**fan**<br>**(CPU**<br>**TDP <=**<br>**165 W)**|**HPR**<br>**GOLD**<br>**fan**<br>**(CPU**<br>**TDP up**<br>**to 350**<br>**W)**|**HPR SLVR fan 70%**<br>**(CPU TDP up to**<br>**165 W)^**|**HPR SLVR fan 70%**<br>**(CPU TDP up to**<br>**165 W)^**|
|256 GB<br>RDIMM|12.7<br>W|30°C<br>(86°F)|30°C<br>(86°F)|30°C<br>(86°F)|30°C<br>(86°F)|30°C<br>(86°F<br>)|35°C<br>(95°F)|Require<br>d DLC|35°C<br>(95°F)|Required<br>DLC|Required<br>DLC|
|128 GB<br>RDIMM|8.9<br>W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|30°C<br>(86°F<br>)|35°C<br>(95°F)|30°C<br>(86°F)|35°C<br>(95°F)|30°C<br>(86°F)|30°C<br>(86°F)|
|64 GB<br>RDIMM|6.9<br>W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|30°C<br>(86°F)|30°C<br>(86°F)|
|32 GB<br>RDIMM|4.1<br>W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|
|16 GB<br>RDIMM|3 W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|
|**DIMM**<br>**Configur**<br>**ation**|**2DP**<br>**C/**<br>**Pow**<br>**er**|**HPR SLVR fan (CPU TDP up to 350 W)**|**HPR SLVR fan (CPU TDP up to 350 W)**|**HPR SLVR fan (CPU TDP up to 350 W)**|**HPR SLVR fan (CPU TDP up to 350 W)**|**HPR SLVR fan (CPU TDP up to 350 W)**|**HPR SLVR fan (CPU TDP up to 350 W)**|**HPR SLVR fan (CPU TDP up to 350 W)**|**HPR**<br>**GOLD**<br>**fan**<br>**(CPU**<br>**TDP up**<br>**to 350**<br>**W)**|**HPR GOLD fan**<br>**70% (CPU TDP up**<br>**to 250 W**)|**HPR GOLD fan**<br>**70% (CPU TDP up**<br>**to 250 W**)|
|256 GB<br>RDIMM|12.7<br>W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|Required<br>DLC|Required<br>DLC|
|128 GB<br>RDIMM|8.9<br>W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|30°C<br>(86°F)|30°C<br>(86°F)|
|64 GB<br>RDIMM|6.9<br>W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|30°C<br>(86°F)|30°C<br>(86°F)|
|32 GB<br>RDIMM|4.1<br>W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|
|**Rear storage**|**Rear storage**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No**<br>**rear**<br>**drive**<br>**s**|**2.5-inch**<br>**or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear fan**|**No**<br>**rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**2.5-inch**<br>**or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with rear**<br>**fan**|
|**DIMM**<br>**Configur**<br>**ation**|**2DP**<br>**C/**<br>**Pow**<br>**er**|**STD fan (CPU TDP <= 250 W)**|**STD fan (CPU TDP <= 250 W)**|**STD fan (CPU TDP <= 250 W)**|**STD fan (CPU TDP <= 250 W)**|**STD fan (CPU TDP <= 250 W)**|**HPR**<br>**SLVR**<br>**fan**<br>**(CPU**<br>**TDP up**<br>**to 350**<br>**W)**|**STD**<br>**fan**<br>**(CPU**<br>**TDP <=**<br>**165 W)**|**HPR**<br>**GOLD**<br>**fan**<br>**(CPU**<br>**TDP up**<br>**to 350**<br>**W)**|**HPR SLVR fan 70%**<br>**(CPU TDP up to**<br>**165 W)^**|**HPR SLVR fan 70%**<br>**(CPU TDP up to**<br>**165 W)^**|
|16 GB<br>RDIMM|3 W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|

**NOTE:** In 12 x 3.5-inch with rear module configuration, for CPU TDP greater than 270 W and specific Low Temperaturecase CPUs are not supported.

**NOTE:** ^The fan speed in the 3.5-inch chassis is limited to 70% due to the drive dynamic profile.

**Table 41. Thermal restriction matrix for rear NVMe drives with air cooled configuration (non-GPU)**

|Configuration|Col2|Col3|24 x 2.5-inch SAS|Col5|12 x 3.5-inch|Col7|
|---|---|---|---|---|---|---|
|**Rear storage**|**Rear storage**|**Rear storage**|**2 x 2.5-inch**<br>**with rear fan**|**4 x 2.5-inch**<br>**with rear fan**|**2 x 2.5-inch**<br>**with rear fan**|**4 x 2.5-inch**<br>**with rear fan**|
|**Drive type**|**Drives capacity**|**Power**|**HPR SLVR fan**|**HPR SLVR fan**|**HPR GOLD fan 70%**|**HPR GOLD fan 70%**|
|Kioxia CD7|15.36 TB|19 W|35°C (95°F)|35°C (95°F)|30°C (86°F)|30°C (86°F)|
|Samsung PM9A3|7.68 TB|14 W|35°C (95°F)|35°C (95°F)|30°C (86°F)|30°C (86°F)|
|Samsung PM1733|15.36 TB|22 W|30°C (86°F)|30°C (86°F)|N/A|N/A|
|Samsung<br>PM1733a|15.36 TB|19.7 W|35°C (95°F)|30°C (86°F)|30°C (86°F)|N/A|
|Samsung<br>PM1735a|12.8 TB|19.8 W|35°C (95°F)|30°C (86°F)|30°C (86°F)|N/A|
|Redtail|7.68 TB|24.5 W|30°C (86°F)|30°C (86°F)|N/A|N/A|
|Hynix PE8010|7.68/3.84/1.92<br>TB|17 W|35°C (95°F)|30°C (86°F)|30°C (86°F)|N/A|
|Intel P5520|15.36 TB|20 W|35°C (95°F)|35°C (95°F)|35°C (95°F)|35°C (95°F)|
|Kioxia CM7|30.72 TB|25 W|35°C (95°F)|35°C (95°F)|35°C (95°F)|35°C (95°F)|
|Kioxia CD8|15.36 TB|19 W|35°C (95°F)|35°C (95°F)|35°C (95°F)|35°C (95°F)|
|PE8110|7.68 TB|20 W|30°C (86°F)|N/A|N/A|N/A|
|PE8110|3.84/1.92 TB|20 W|35°C (95°F)|30°C (86°F)|30°C (86°F)|N/A|
|PS1010|15.36 TB|20 W|35°C (95°F)|35°C (95°F)|30°C (86°F)|30°C (86°F)|
|GPU) (continued)|Col2|Col3|Col4|Col5|Col6|Col7|
|**Configuration**|**Configuration**|**Configuration**|**24 x 2.5-inch SAS**|**24 x 2.5-inch SAS**|**12 x 3.5-inch**|**12 x 3.5-inch**|
|**Rear storage**|**Rear storage**|**Rear storage**|**2 x 2.5-inch**<br>**with rear fan**|**4 x 2.5-inch**<br>**with rear fan**|**2 x 2.5-inch**<br>**with rear fan**|**4 x 2.5-inch**<br>**with rear fan**|
|**Drive type**|**Drives capacity**|**Power**|**HPR SLVR fan**|**HPR SLVR fan**|**HPR GOLD fan 70%**|**HPR GOLD fan 70%**|
|PS1030|12.8 TB|20 W|35°C (95°F)|35°C (95°F)|30°C (86°F)|30°C (86°F)|

**Table 42. Thermal restriction matrix for GPU configurations**

|Configuration|Col2|Col3|Col4|No<br>back<br>plane|8 x<br>2.5-<br>inch<br>NVMe|8 x 2.5-<br>inch<br>NVMe +<br>8 x 2.5-<br>inch<br>SAS|16 x<br>2.5-<br>inch<br>SAS|16 x 2.5-<br>inch or<br>16 x<br>EDSFF<br>E3.S<br>NVMe|24 x<br>2.5-<br>inch<br>SAS|16 x<br>2.5-<br>inch<br>SAS +<br>8 x<br>2.5-<br>inch<br>NVMe|24 x<br>2.5-<br>inch<br>NVMe|
|---|---|---|---|---|---|---|---|---|---|---|---|
|**Rear storage**|**Rear storage**|**Rear storage**|**Rear storage**|**No**<br>**rear**<br>**drive**<br>**s**|**No**<br>**rear**<br>**drives**|**No rear**<br>**drives**|**No**<br>**rear**<br>**drives**|**No rear**<br>**drives**|**No**<br>**rear**<br>**drives**|**No rear**<br>**drives**|**No**<br>**rear**<br>**drives**|
|**CPU TDP/cTDP**|**CPU TDP/cTDP**|**Cores**|**T-Case**<br>**max**<br>**center**<br>**(°C)**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|
|3408U|125 W1|8|79|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|
|5415+|150 W1|8|78|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|
|4410Y|4410Y|12|78|78|78|78|78|78|78|78|78|
|5416S|5416S|16|78|78|78|78|78|78|78|78|78|
|5418N|165 W1|24|84|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|
|5411N|5411N|24|84|84|84|84|84|84|84|84|84|
|4416+|4416+|20|82|82|82|82|82|82|82|82|82|
|6426Y|185 W1|16|72|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|
|5418Y|5418Y|24|80|80|80|80|80|80|80|80|80|
|5412U|5412U|24|80|80|80|80|80|80|80|80|80|
|6428N|6428N|32|85|85|85|85|85|85|85|85|85|
|6421N|6421N|32|85|85|85|85|85|85|85|85|85|
|6434|205 W1|8|96|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(86°F)|30°C<br>(86°F)|
|5420+|5420+|28|84|84|84|84|84|84|84|84|84|
|6438Y+|6438Y+|32|76|76|76|76|76|76|76|76|76|
|6438M|6438M|32|84|84|84|84|84|84|84|84|84|
|6438N|6438N|32|84|84|84|84|84|84|84|84|84|
|6442Y|225 W1|24|79|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|
|6448Y|6448Y|32|79|79|79|79|79|79|79|79|79|
|6444Y|270 W2|32|75|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|
|**Rear storage**|**Rear storage**|**Rear storage**|**Rear storage**|**No**<br>**rear**<br>**drive**<br>**s**|**No**<br>**rear**<br>**drives**|**No rear**<br>**drives**|**No**<br>**rear**<br>**drives**|**No rear**<br>**drives**|**No**<br>**rear**<br>**drives**|**No rear**<br>**drives**|**No**<br>**rear**<br>**drives**|
|**CPU TDP/cTDP**|**CPU TDP/cTDP**|**Cores**|**T-Case**<br>**max**<br>**center**<br>**(°C)**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|
|8462Y+|300 W2|32|81|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|30°C<br>(86°F)|30°C<br>(86°F)|30°C<br>(86°F)|
|6458Q|350 W2|32|64|Requi<br>red<br>DLC|Requir<br>ed<br>DLC|Required<br>DLC|Requir<br>ed<br>DLC|Required<br>DLC|Requir<br>ed DLC|Require<br>d DLC|Requir<br>ed<br>DLC|
|6414U|250 W2|32|76|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|
|6454S|270 W2|32|71|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|30°C<br>(86°F)|30°C<br>(86°F)|30°C<br>(86°F)|
|6430|6430|32|71|71|71|71|71|71|71|71|71|
|8471N|300 W2|52|76|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|30°C<br>(86°F)|30°C<br>(86°F)|30°C<br>(86°F)|
|8470N|8470N|52|76|76|76|76|76|76|76|76|76|
|8460Y+|8460Y+|40|75|75|75|75|75|75|75|75|75|
|8452Y|8452Y|36|75|75|75|75|75|75|75|75|75|
|8480+|350 W2|56|79|30°C<br>(86°F<br>)|30°C<br>(86°F)|30°C<br>(86°F)|30°C<br>(86°F<br>)|30°C<br>(86°F)|Requir<br>ed DLC|Require<br>d DLC|Requir<br>ed<br>DLC|
|8470|8470|52|79|79|79|79|79|79|79|79|79|
|8468|8468|48|79|79|79|79|79|79|79|79|79|
|8470Q|350 W2|52|57|Requi<br>red<br>DLC|Requir<br>ed<br>DLC|Required<br>DLC|Requir<br>ed<br>DLC|Required<br>DLC|Requir<br>ed DLC|Require<br>d DLC|Requir<br>ed<br>DLC|
|9480|350 W2|56|64|Requi<br>red<br>DLC|Requir<br>ed<br>DLC|Required<br>DLC|Requir<br>ed<br>DLC|Required<br>DLC|Requir<br>ed DLC|Require<br>d DLC|Requir<br>ed<br>DLC|
|9470|9470|52|64|64|64|64|64|64|64|64|64|
|9460|350 W2|40|77|30°C<br>(86°F<br>)|30°C<br>(86°F)|30°C<br>(86°F)|30°C<br>(86°F<br>)|30°C<br>(86°F)|Requir<br>ed DLC|Require<br>d DLC|Requir<br>ed<br>DLC|
|9462|9462|32|77|77|77|77|77|77|77|77|77|

**NOTE:** The platform supports Maximum (MAX) and Mainstream (MS) system boards.

 - 1 supports MS system board (CPU TDP < 250 W)
 - 2 supports MAX system board (CPU TDP => 250 W)
**NOTE:** ^The fan speed in the 3.5-inch chassis is limited to 70% due to the drive dynamic profile.

**NOTE:** *Supported ambient temperature is 30°C (86°F).

**NOTE:** GPU configuration supports only High performance Gold (HPR Gold) fan.

**Table 43. Thermal restriction matrix for memory with air cooled configuration (GPU)**

|Configuration|Col2|No<br>backpla<br>ne|8 x 2.5-<br>inch NVMe|16 x 2.5-<br>inch SAS<br>and Split<br>NVMe-<br>SAS *|16 x 2.5-inch or<br>16 x EDSFF<br>E3.S NVMe **|24 x 2.5-<br>inch<br>SAS*|16 x 2.5-inch<br>SAS + 8 x 2.5-<br>inch NVMe***|24 x 2.5-<br>inch<br>NVMe***|
|---|---|---|---|---|---|---|---|---|
|**DIMM**<br>**Configura**<br>**tion**|**2DPC/**<br>**Power**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|
|256 GB<br>RDIMM|12.7 W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|Required<br>DLC|Required DLC|Required<br>DLC|
|128 GB<br>RDIMM|8.9 W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|35°C (95°F)|
|64 GB<br>RDIMM|6.9 W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|35°C (95°F)|
|32 GB<br>RDIMM|4.1 W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|35°C (95°F)|
|16 GB<br>RDIMM|3 W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|35°C (95°F)|

**NOTE:** *In 16 x 2.5-inch SAS and 8 x 2.5-inch NVMe configurations, for CPU TDP 350 W supported ambient temperature is
30°C (86°F).

**NOTE:** **In 16 x 2.5-inch NVMe configuration, for CPU TDP greater than 300 W supported ambient temperature is 30°C
(86°F).

**NOTE:** ***In 24 x 2.5-inch SAS/NVMe configuration and 16 x 2.5-inch SAS + 8 x 2.5-inch NVMe, for CPU TDP 270 W 300 W and specific Low Temperature-case CPUs supported ambient temperature is 30°C (86°F).

**Table 44. Optimized Ecological upgrade thermal restriction matrix for air cooled configuration**

|Configuration|Col2|Col3|Col4|No<br>bac<br>kpla<br>ne|8 x<br>2.5-<br>inch<br>NV<br>Me|16 x<br>2.5-<br>inch<br>SAS<br>and<br>split<br>NV<br>Me-<br>SAS|16 x<br>2.5-<br>inch<br>or 16 x<br>EDSF<br>F E3.S<br>NVMe|24 x 2.5-inch<br>SAS|Col10|16 x<br>2.5-<br>inch<br>SAS<br>+ 8 x<br>2.5-<br>inch<br>NVM<br>e|24 x<br>2.5-<br>inch<br>NV<br>Me|12 x 3.5-inch|Col14|Col15|Amb<br>ient<br>tem<br>pera<br>ture|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|**Rear storage**|**Rear storage**|**Rear storage**|**Rear storage**|**No**<br>**rear**<br>**driv**<br>**es**|**No**<br>**rear**<br>**driv**<br>**es**|**No**<br>**rear**<br>**driv**<br>**es**|**No**<br>**rear**<br>**drives**|**No**<br>**rear**<br>**driv**<br>**es**|**2.5-**<br>**inch or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear**<br>**fan**|**No**<br>**rear**<br>**drive**<br>**s**|**No**<br>**rear**<br>**driv**<br>**es**|**No**<br>**rear**<br>**drives**|**2.5-**<br>**inch**<br>**rear**<br>**drive**<br>**s **<br>**with**<br>**rear**<br>**fan**|**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear**<br>**fan**|**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear**<br>**fan**|
|**CPU TDP/cTDP**|**CPU TDP/cTDP**|**Cores**|**T-**<br>**Cas**<br>**e **<br>**ma**<br>**x **<br>**cen**<br>**ter**<br>**(°C**<br>**)**|**Fan/HSK**|**Fan/HSK**|**Fan/HSK**|**Fan/HSK**|**Fan/HSK**|**Fan/HSK**|**Fan/HSK**|**Fan/HSK**|**HPR GOLD fan 70%^**|**HPR GOLD fan 70%^**|**HPR GOLD fan 70%^**|**HPR GOLD fan 70%^**|
|5415+|150 W|8|78|STD<br>/2U<br>HPR|STD<br>/2U<br>HPR|STD<br>/2U<br>HPR|STD /<br>2U<br>HPR|STD<br>/2U<br>HPR|HPR<br>SLVR /<br>2U HPR|STD /<br>2U<br>HPR|HPR<br>GOL|HPR<br>SLVR|HPR<br>GOL<br>D /|HPR<br>SLVR /<br>2U HPR|35°<br>C|
|4410Y|4410Y|12|78|78|78|78|78|78|78|78|78|78|78|78|78|
|configuration (continued)|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|
|**Configuration**|**Configuration**|**Configuration**|**Configuration**|**No**<br>**bac**<br>**kpla**<br>**ne**|**8 x**<br>**2.5-**<br>**inch**<br>**NV**<br>**Me**|**16 x**<br>**2.5-**<br>**inch**<br>**SAS**<br>**and**<br>**split**<br>**NV**<br>**Me-**<br>**SAS**|**16 x**<br>**2.5-**<br>**inch**<br>**or 16 x**<br>**EDSF**<br>**F E3.S**<br>**NVMe**|**24 x 2.5-inch**<br>**SAS**|**24 x 2.5-inch**<br>**SAS**|**16 x**<br>**2.5-**<br>**inch**<br>**SAS**<br>**+ 8 x**<br>**2.5-**<br>**inch**<br>**NVM**<br>**e**|**24 x**<br>**2.5-**<br>**inch**<br>**NV**<br>**Me**|**12 x 3.5-inch**|**12 x 3.5-inch**|**12 x 3.5-inch**|**Amb**<br>**ient**<br>**tem**<br>**pera**<br>**ture**|
|**Rear storage**|**Rear storage**|**Rear storage**|**Rear storage**|**No**<br>**rear**<br>**driv**<br>**es**|**No**<br>**rear**<br>**driv**<br>**es**|**No**<br>**rear**<br>**driv**<br>**es**|**No**<br>**rear**<br>**drives**|**No**<br>**rear**<br>**driv**<br>**es**|**2.5-**<br>**inch or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear**<br>**fan**|**No**<br>**rear**<br>**drive**<br>**s**|**No**<br>**rear**<br>**driv**<br>**es**|**No**<br>**rear**<br>**drives**|**2.5-**<br>**inch**<br>**rear**<br>**drive**<br>**s **<br>**with**<br>**rear**<br>**fan**|**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear**<br>**fan**|**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear**<br>**fan**|
|**CPU TDP/cTDP**|**CPU TDP/cTDP**|**Cores**|**T-**<br>**Cas**<br>**e **<br>**ma**<br>**x **<br>**cen**<br>**ter**<br>**(°C**<br>**)**|**Fan/HSK**|**Fan/HSK**|**Fan/HSK**|**Fan/HSK**|**Fan/HSK**|**Fan/HSK**|**Fan/HSK**|**Fan/HSK**|**HPR GOLD fan 70%^**|**HPR GOLD fan 70%^**|**HPR GOLD fan 70%^**|**HPR GOLD fan 70%^**|
|5416S||16|78||||||||D /<br>STD|/2U<br>HPR|2U<br>HPR||(95°<br>F)|
|5418N/<br>5411N|165 W|24|84|STD<br>/2U<br>HPR|STD<br>/2U<br>HPR|STD<br>/2U<br>HPR|STD /<br>2U<br>HPR|STD<br>/2U<br>HPR|HPR<br>SLVR /<br>2U HPR|STD /<br>2U<br>HPR|HPR<br>GOL<br>D /<br>STD|HPR<br>SLVR<br>/2U<br>HPR|HPR<br>GOL<br>D /<br>2U<br>HPR|HPR<br>SLVR /<br>2U HPR|35°<br>C <br>(95°<br>F)|
|4416+|4416+|20|82|82|82|82|82|82|82|82|82|82|82|82|82|

**NOTE:** ^The fan speed in the 3.5-inch chassis is limited to 70% due to the drive dynamic profile.

Thermal restriction matrix for 5 [th] Gen Intel® Xeon® Scalable Processors

**Table 45. Thermal restriction matrix for air cooled configuration**

|Configuration|Col2|Col3|Col4|No<br>back<br>plane|8 x<br>2.5-<br>inc<br>h<br>NV<br>Me|16 x<br>2.5-<br>inch<br>SAS<br>and<br>Split<br>NVM<br>e-<br>SAS|16 x<br>2.5-<br>inch or<br>16 x<br>EDSFF<br>E3.S<br>NVMe|24 x 2.5-inch<br>SAS|Col10|16 x<br>2.5-<br>inch<br>SAS<br>+ 8 x<br>2.5-<br>inch<br>NVM<br>e|24 x<br>2.5-<br>inch<br>NV<br>Me|12 x 3.5-inch^|Col14|Ambien<br>t<br>temper<br>ature|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|**Rear storage**|**Rear storage**|**Rear storage**|**Rear storage**|**No**<br>**rear**<br>**drive**<br>**s**|**No**<br>**rear**<br>**driv**<br>**es**|**No**<br>**rear**<br>**drive**<br>**s**|**No rear**<br>**drives**|**No**<br>**rea**<br>**r **<br>**dri**<br>**ves**|**2.5-**<br>**inch or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear fan**|**No**<br>**rear**<br>**drive**<br>**s**|**No**<br>**rear**<br>**driv**<br>**es**|**No**<br>**rear**<br>**drive**<br>**s**|**2.5-**<br>**inch or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear**<br>**fan**|**2.5-**<br>**inch or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear**<br>**fan**|
|**CPU TDP/cTDP**|**CPU TDP/cTDP**|**Core**<br>**s**|**T-**<br>**Case**<br>**max**<br>**cent**<br>**er**<br>**(°C)**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|
|4509Y|125 W1|8|84|STD|STD|STD|STD|ST<br>D|HPR<br>SLVR|STD|HPR<br>GOL<br>D|HPR<br>SLVR|HPR<br>GOLD|35°C<br>(95°F)|
|4510|150 W1|12|84|STD|STD|STD|STD|ST<br>D|HPR<br>SLVR|STD|HPR<br>GOL<br>D|HPR<br>SLVR|HPR<br>GOLD|35°C<br>(95°F)|
|4514Y|4514Y|16|79|79|79|79|79|79|79|79|79|79|79|79|
|5512U|185 W1|28|89|STD|STD|STD|STD|ST<br>D|HPR<br>SLVR|HPR<br>SLVR|HPR<br>GOL<br>D|HPR<br>GOL<br>D|HPR<br>GOLD|35°C<br>(95°F)|
|6534|195 W1|8|64|STD|STD|STD|STD|ST<br>D|HPR<br>SLVR|HPR<br>SLVR|HPR<br>GOL<br>D|HPR<br>GOL<br>D|HPR<br>GOLD|35°C<br>(95°F)|
|6526Y|6526Y|16|82|82|82|82|82|82|82|82|82|82|82|82|
|6542Y|250 W1|24|83|STD|STD|STD|STD|ST<br>D|HPR<br>SLVR|HPR<br>SLVR|HPR<br>GOL<br>D|HPR<br>GOL<br>D*|HPR<br>GOLD*|35°C<br>(95°F)|
|6548Y+|6548Y+|32|83|83|83|83|83|83|83|83|83|83|83|83|
|6548N|6548N|32|83|83|83|83|83|83|83|83|83|83|83|83|
|8562Y+|300 W2|32|81|HPR<br>SLVR|HPR<br>SLV<br>R|HPR<br>SLVR|HPR<br>SLVR|HP<br>R <br>SLV<br>R|HPR<br>SLVR|HPR<br>SLVR|HPR<br>GOL<br>D|Requi<br>red<br>DLC|Require<br>d DLC|35°C<br>(95°F)|
|8558U|300 W2|48|78|HPR<br>SLVR|HPR<br>SLV<br>R|HPR<br>SLVR|HPR<br>SLVR|HP<br>R <br>SLV<br>R|HPR<br>SLVR|HPR<br>SLVR|HPR<br>GOL<br>D|Requi<br>red<br>DLC|Require<br>d DLC|35°C<br>(95°F)|
|8568Y+|350 W2|48|81|HPR<br>SLVR|HPR<br>SLV<br>R|HPR<br>SLVR|HPR<br>SLVR|HP<br>R <br>SLV<br>R|HPR<br>SLVR<br>fan|HPR<br>SLVR<br>fan|HPR<br>GOL<br>D*|Requi<br>red<br>DLC|Require<br>d DLC|35°C<br>(95°F)|
|8580|8580|60|81|81|81|81|81|81|81|81|81|81|81|81|
|8592+|8592+|64|81|81|81|81|81|81|81|81|81|81|81|81|

**NOTE:** The platform supports Maximum (MAX) and Mainstream (MS) system boards.

  - 1 supports MS system board (CPU TDP < 250 W)
  - 2 supports MAX system board (CPU TDP ≥ 250 W)
**NOTE:** *Supported ambient temperature is 30°C (86°F).

**NOTE:** ^The fan speed in the 3.5-inch chassis is limited to 70% due to the drive dynamic profile.

**Table 46. Thermal restriction matrix for memory with air cooled configuration (non-GPU)**

|Configuration|Col2|No<br>backpl<br>ane|8 x 2.5-<br>inch<br>NVMe|16 x<br>2.5-<br>inch<br>SAS<br>and<br>Split<br>NVMe-<br>SAS|16 x 2.5-<br>inch or 16<br>x EDSFF<br>E3.S<br>NVMe|24 x 2.5-inch<br>SAS|Col8|16 x<br>2.5-<br>inch<br>SAS +<br>8 x<br>2.5-<br>inch<br>NVMe|24 x 2.5-<br>inch<br>NVMe|12 x 3.5-inch|Col12|
|---|---|---|---|---|---|---|---|---|---|---|---|
|**Rear storage**|**Rear storage**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No**<br>**rear**<br>**drive**<br>**s**|**2.5-inch**<br>**or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear fan**|**No**<br>**rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**2.5-inch**<br>**or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with rear**<br>**fan**|
|**DIMM**<br>**Configur**<br>**ation**|**2DP**<br>**C/**<br>**Pow**<br>**er**|**STD fan (CPU TDP <= 250 W)**|**STD fan (CPU TDP <= 250 W)**|**STD fan (CPU TDP <= 250 W)**|**STD fan (CPU TDP <= 250 W)**|**STD fan (CPU TDP <= 250 W)**|**HPR**<br>**SLVR**<br>**fan**<br>**(CPU**<br>**TDP up**<br>**to 350**<br>**W)**|**STD**<br>**fan**<br>**(CPU**<br>**TDP <=**<br>**165 W)**|**HPR**<br>**GOLD**<br>**fan**<br>**(CPU**<br>**TDP up**<br>**to 350**<br>**W)**|**HPR SLVR fan 70%**<br>**(CPU TDP up to**<br>**165 W)^**|**HPR SLVR fan 70%**<br>**(CPU TDP up to**<br>**165 W)^**|
|256 GB<br>RDIMM|12.7<br>W|30°C<br>(86°F)|30°C<br>(86°F)|30°C<br>(86°F)|30°C<br>(86°F)|30°C<br>(86°F<br>)|35°C<br>(95°F)|Require<br>d DLC|35°C<br>(95°F)|Required<br>DLC|Required<br>DLC|
|128 GB<br>RDIMM|8.9<br>W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|30°C<br>(86°F<br>)|35°C<br>(95°F)|30°C<br>(86°F)|35°C<br>(95°F)|30°C<br>(86°F)|30°C<br>(86°F)|
|96 GB<br>RDIMM|8.3<br>W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|30°C<br>(86°F<br>)|35°C<br>(95°F)|30°C<br>(86°F)|35°C<br>(95°F)|30°C<br>(86°F)|30°C<br>(86°F)|
|64 GB<br>RDIMM|6.9<br>W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|30°C<br>(86°F)|30°C<br>(86°F)|
|32 GB<br>RDIMM|4.1<br>W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|
|16 GB<br>RDIMM|3 W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|
|**DIMM**<br>**Configur**<br>**ation**|**2DP**<br>**C/**<br>**Pow**<br>**er**|**HPR SLVR fan (CPU TDP up to 350 W)**|**HPR SLVR fan (CPU TDP up to 350 W)**|**HPR SLVR fan (CPU TDP up to 350 W)**|**HPR SLVR fan (CPU TDP up to 350 W)**|**HPR SLVR fan (CPU TDP up to 350 W)**|**HPR SLVR fan (CPU TDP up to 350 W)**|**HPR SLVR fan (CPU TDP up to 350 W)**|**HPR**<br>**GOLD**<br>**fan**<br>**(CPU**<br>**TDP up**<br>**to 350**<br>**W)**|**HPR GOLD fan**<br>**70% (CPU TDP up**<br>**to 250 W)^**|**HPR GOLD fan**<br>**70% (CPU TDP up**<br>**to 250 W)^**|
|256 GB<br>RDIMM|12.7<br>W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|Required<br>DLC|Required<br>DLC|
|128 GB<br>RDIMM|8.9<br>W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|30°C<br>(86°F)|30°C<br>(86°F)|
|**Rear storage**|**Rear storage**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No**<br>**rear**<br>**drive**<br>**s**|**2.5-inch**<br>**or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear fan**|**No**<br>**rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**2.5-inch**<br>**or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with rear**<br>**fan**|
|**DIMM**<br>**Configur**<br>**ation**|**2DP**<br>**C/**<br>**Pow**<br>**er**|**STD fan (CPU TDP <= 250 W)**|**STD fan (CPU TDP <= 250 W)**|**STD fan (CPU TDP <= 250 W)**|**STD fan (CPU TDP <= 250 W)**|**STD fan (CPU TDP <= 250 W)**|**HPR**<br>**SLVR**<br>**fan**<br>**(CPU**<br>**TDP up**<br>**to 350**<br>**W)**|**STD**<br>**fan**<br>**(CPU**<br>**TDP <=**<br>**165 W)**|**HPR**<br>**GOLD**<br>**fan**<br>**(CPU**<br>**TDP up**<br>**to 350**<br>**W)**|**HPR SLVR fan 70%**<br>**(CPU TDP up to**<br>**165 W)^**|**HPR SLVR fan 70%**<br>**(CPU TDP up to**<br>**165 W)^**|
|96 GB<br>RDIMM|8.3<br>W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|30°C<br>(86°F)|30°C<br>(86°F)|
|64 GB<br>RDIMM|6.9<br>W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|30°C<br>(86°F)|30°C<br>(86°F)|
|32 GB<br>RDIMM|4.1<br>W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|
|16 GB<br>RDIMM|3 W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F<br>)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|

**NOTE:** ^The fan speed in the 3.5-inch chassis is limited to 70% due to the drive dynamic profile.

**Table 47. Supported ambient temperature for processors with GPU**

|Configuration|Col2|Col3|Col4|No<br>backpl<br>ane|8 x<br>2.5-<br>inch<br>NVMe|16 x 2.5-<br>inch SAS<br>and Split<br>NVMe-<br>SAS|16 x 2.5-<br>inch or<br>16 x<br>EDSFF<br>E3.S<br>NVMe|24 x 2.5-<br>inch SAS|16 x 2.5-inch<br>SAS + 8 x<br>2.5-inch<br>NVMe|24 x<br>2.5-<br>inch<br>NVMe|
|---|---|---|---|---|---|---|---|---|---|---|
|**Rear storage**|**Rear storage**|**Rear storage**|**Rear storage**|**No**<br>**rear**<br>**drives**|**No**<br>**rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No**<br>**rear**<br>**drive**<br>**s**|
|**CPU TDP/cTDP**|**CPU TDP/cTDP**|**Cores**|**T-Case**<br>**max**<br>**center**<br>**(°C)**|**Support HPR GOLD fan with 1U HPR L-Type HSK**|**Support HPR GOLD fan with 1U HPR L-Type HSK**|**Support HPR GOLD fan with 1U HPR L-Type HSK**|**Support HPR GOLD fan with 1U HPR L-Type HSK**|**Support HPR GOLD fan with 1U HPR L-Type HSK**|**Support HPR GOLD fan with 1U HPR L-Type HSK**|**Support HPR GOLD fan with 1U HPR L-Type HSK**|
|4509Y|125 W1|8|84|35°C|35°C|35°C|35°C|35°C|35°C|35°C|
|4510|150 W1|12|84|35°C|35°C|35°C|35°C|35°C|35°C|35°C|
|4514Y|4514Y|16|79|79|79|79|79|79|79|79|
|5512U|185 W1|28|89|35°C|35°C|35°C|35°C|35°C|35°C|35°C|
|6534|195 W1|8|64|35°C|35°C|35°C|35°C|35°C|35°C|35°C|
|**Rear storage**|**Rear storage**|**Rear storage**|**Rear storage**|**No**<br>**rear**<br>**drives**|**No**<br>**rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No**<br>**rear**<br>**drive**<br>**s**|
|**CPU TDP/cTDP**|**CPU TDP/cTDP**|**Cores**|**T-Case**<br>**max**<br>**center**<br>**(°C)**|**Support HPR GOLD fan with 1U HPR L-Type HSK**|**Support HPR GOLD fan with 1U HPR L-Type HSK**|**Support HPR GOLD fan with 1U HPR L-Type HSK**|**Support HPR GOLD fan with 1U HPR L-Type HSK**|**Support HPR GOLD fan with 1U HPR L-Type HSK**|**Support HPR GOLD fan with 1U HPR L-Type HSK**|**Support HPR GOLD fan with 1U HPR L-Type HSK**|
|6526Y||16|82||||||||
|6542Y|250 W1|24|83|35°C|35°C|35°C|35°C|35°C|35°C|35°C|
|6548Y+|6548Y+|32|83|83|83|83|83|83|83|83|
|6548N|6548N|32|83|83|83|83|83|83|83|83|
|8562Y+|300 W2|32|81|35°C|35°C|35°C|35°C|30°C|30°C|30°C|
|8558U|300 W2|48|78|35°C|35°C|35°C|35°C|30°C|30°C|30°C|
|8568Y+|350 W2|48|81|30°C|30°C|30°C|30°C|Required<br>DLC|Required DLC|Requir<br>ed<br>DLC|
|8580|8580|60|81|81|81|81|81|81|81|81|
|8592+|8592+|64|81|81|81|81|81|81|81|81|

**NOTE:** The platform supports Maximum (MAX) and Mainstream (MS) system boards.

  - 1 supports MS system board (CPU TDP < 250 W)
  - 2 supports MAX system board (CPU TDP ≥ 250 W)
**NOTE:** *Supported ambient temperature is 30°C (86°F).

**Table 48. Thermal restriction matrix for memory with air cooled configuration (GPU)**

|Configuration|Col2|No<br>backpla<br>ne|8 x 2.5-<br>inch NVMe|16 x 2.5-<br>inch SAS<br>and Split<br>NVMe-<br>SAS|16 x 2.5-inch<br>NVMe|24 x 2.5-<br>inch SAS|16 x 2.5-inch<br>SAS + 8 x 2.5-<br>inch NVMe|24 x 2.5-<br>inch NVMe|
|---|---|---|---|---|---|---|---|---|
|**DIMM**<br>**Configura**<br>**tion**|**2DPC/**<br>**Power**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|
|256 GB<br>RDIMM|12.7 W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|Required<br>DLC|Required DLC|Required<br>DLC|
|128 GB<br>RDIMM|8.9 W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|35°C (95°F)|
|96 GB<br>RDIMM|8.3 W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|35°C (95°F)|
|64 GB<br>RDIMM|6.9 W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|35°C (95°F)|
|32 GB<br>RDIMM|4.1 W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|35°C (95°F)|
|16 GB<br>RDIMM|3 W|35°C<br>(95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|35°C (95°F)|

**NOTE:** *256 GB RDIMM with 5 [th] Gen Intel® Xeon® Scalable Processors will be supported in the future release.

Common thermal restrictions for both 4 [th] and 5 [th] Gen Intel processors

**Table 49. GPU types support thermal restriction for both air cooling and liquid cooling configuration**

|Configuration|No backplane|8 x 2.5-<br>inch<br>NVMe|16 x 2.5-inch<br>SAS and<br>split NVMe-<br>SAS|16 x 2.5-inch<br>NVMe or 16<br>x EDSFF<br>E3.S NVMe|24 x 2.5-<br>inch SAS|16 x 2.5-<br>inch SAS<br>+ 8 x 2.5-<br>inch<br>NVMe|24 x 2.5-<br>inch NVMe|
|---|---|---|---|---|---|---|---|
|**Rear storage**|**No rear drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|
|**GPU**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|**HPR GOLD fan with 1U HPR L-Type HSK**|
|A40 (Max 2)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|30°C (86°F)|30°C<br>(86°F)|30°C<br>(86°F)|30°C (86°F)|
|Intel PVC (Max<br>2)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|35°C (95°F)|30°C<br>(86°F)|30°C<br>(86°F)|30°C (86°F)|
|A100 80 GB (Max<br>2)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|35°C (95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|
|A16 (Max 2)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|35°C (95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|
|A30 (Max 2)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|35°C (95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|
|A2 (Max 6)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|35°C (95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|
|H100 (Max 2)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|35°C (95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|
|A800 (Max 2)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|35°C (95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|
|L4 (Max 6)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|35°C (95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|
|L40 (Max 2)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|35°C (95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|
|Intel ASM (Max<br>6)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|35°C (95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|
|L40S (Max 2)|35°C (95°F)|35°C<br>(95°F)|35°C (95°F)|35°C (95°F)|35°C<br>(95°F)|35°C<br>(95°F)|35°C (95°F)|

**Table 50. Thermal restriction for memory with liquid cooled configuration(non-GPU)**

|Configuration|Col2|No<br>backp<br>lane|8 x<br>2.5-<br>inch<br>NVMe|16 x<br>2.5-<br>inch<br>SAS<br>and<br>Split<br>NVMe<br>-SAS|16 x<br>2.5-<br>inch<br>or 16 x<br>EDSF<br>F E3.S<br>NVMe|24 x 2.5-inch<br>SAS|Col8|16 x 2.5-<br>inch<br>SAS + 8<br>x 2.5-<br>inch<br>NVMe|24 x<br>2.5-<br>inch<br>NVMe|12 x 3.5-inch^|Col12|Ambient<br>temperat<br>ure|
|---|---|---|---|---|---|---|---|---|---|---|---|---|
|**Rear storage**|**Rear storage**|**No**<br>**rear**<br>**drives**|**No**<br>**rear**<br>**drives**|**No**<br>**rear**<br>**drives**|**No**<br>**rear**<br>**drives**|**No**<br>**rear**<br>**drives**|**2.5-inch**<br>**or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear fan**|**No rear**<br>**drives**|**No**<br>**rear**<br>**drives**|**No**<br>**rear**<br>**drives**|**2.5-**<br>**inch or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear fan**|**2.5-**<br>**inch or**<br>**EDSFF**<br>**E3.S**<br>**rear**<br>**drives**<br>**with**<br>**rear fan**|
|**DIMM**<br>**Configur**<br>**ation**|**Pow**<br>**er**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|
|256 GB<br>RDIMM|12.7<br>W|HPR<br>SLVR|HPR<br>SLVR|HPR<br>SLVR|HPR<br>SLVR|HPR<br>SLVR|HPR<br>SLVR|HPR<br>SLVR|HPR<br>SLVR|HPR<br>GOLD|HPR<br>GOLD<br>fan|35°C<br>(95°F)|
|128 GB<br>RDIMM|8.9<br>W|STD|STD|STD|STD|STD|STD|STD|STD|HPR<br>GOLD|HPR<br>GOLD|35°C<br>(95°F)|
|96 GB<br>RDIMM|8.3<br>W|STD|STD|STD|STD|STD|STD|STD|STD|HPR<br>GOLD|HPR<br>GOLD|35°C<br>(95°F)|
|64 GB<br>RDIMM|6.9<br>W|STD|STD|STD|STD|STD|STD|STD|STD|HPR<br>GOLD|HPR<br>GOLD|35°C<br>(95°F)|
|32 GB<br>RDIMM|4.1<br>W|STD|STD|STD|STD|STD|STD|STD|STD|HPR<br>GOLD|HPR<br>GOLD|35°C<br>(95°F)|
|16 GB<br>RDIMM|3 W|STD|STD|STD|STD|STD|STD|STD|STD|HPR<br>GOLD|HPR<br>GOLD|35°C<br>(95°F)|

**NOTE:** ^The fan speed in the 3.5-inch chassis is limited to 70% due to the drive dynamic profile.

**Table 51. Thermal restriction for memory with liquid cooled configuration(GPU)**

|Configuration|Col2|No<br>backp<br>lane|8 x 2.5-<br>inch<br>NVMe|16 x 2.5-<br>inch SAS<br>and Split<br>NVMe-<br>SAS|16 x 2.5-inch<br>or 16 x<br>EDSFF E3.S<br>NVMe|24 x 2.5-inch<br>SAS|16 x 2.5-<br>inch SAS +<br>8 x 2.5-<br>inch NVMe|24 x 2.5-<br>inch<br>NVMe|Ambient<br>temperat<br>ure|
|---|---|---|---|---|---|---|---|---|---|
|**Rear storage**|**Rear storage**|**No**<br>**rear**<br>**drive**<br>**s**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|
|**DIMM**<br>**Config**<br>**uration**|**Power**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|
|256 GB<br>RDIMM|12.7 W|HPR GOLD fan|HPR GOLD fan|HPR GOLD fan|HPR GOLD fan|HPR GOLD fan|HPR GOLD fan|HPR GOLD fan|35°C<br>(95°F)|
|128 GB<br>RDIMM|8.9 W|8.9 W|8.9 W|8.9 W|8.9 W|8.9 W|8.9 W|8.9 W|8.9 W|
|96 GB<br>RDIMM|8.3 W|8.3 W|8.3 W|8.3 W|8.3 W|8.3 W|8.3 W|8.3 W|8.3 W|
|64 GB<br>RDIMM|6.9 W|6.9 W|6.9 W|6.9 W|6.9 W|6.9 W|6.9 W|6.9 W|6.9 W|
|**Rear storage**|**Rear storage**|**No**<br>**rear**<br>**drive**<br>**s**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|**No rear**<br>**drives**|
|**DIMM**<br>**Config**<br>**uration**|**Power**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|**Fan**|
|32 GB<br>RDIMM|4.1 W|||||||||
|16 GB<br>RDIMM|3 W|3 W|3 W|3 W|3 W|3 W|3 W|3 W|3 W|

**NOTE:** *256 GB RDIMM with 5 [th] Gen Intel® Xeon® Scalable Processors will be supported in the future release.



### Thermal air restrictions

**Table 52. Air cooling configurations thermal restriction for AHSRAE A3 and A4**

|ASHRAE|A3/40°C (104°F)|A4/45°C (113°F)|
|---|---|---|
|PSU|Two PSUs are required in redundant mode. If there is PSU failure, system performance may be reduced.|Two PSUs are required in redundant mode. If there is PSU failure, system performance may be reduced.|
|PCIe card|Non-Dell qualified peripheral cards and peripheral cards greater than 25 W are not supported.|Non-Dell qualified peripheral cards and peripheral cards greater than 25 W are not supported.|
|GPU/FPGA|Not supported|Not supported|
|DIMM|128 GB, or greater capacity DIMMs are not supported.|128 GB, or greater capacity DIMMs are not supported.|
|PCIe SSD|Not supported|Not supported|
|Front storage|Not supported in 12 x 3.5-inch SAS configuration.|Not supported in 12 x 3.5-inch SAS configuration.|
|Rear storage|Not supported|Not supported|
|Fan|HPR SLVR fans are required.|HPR SLVR fans are required.|
|Processor|≤ 165 W|≤ 125 W|
|OCP|Supported with 85°C (185°F) active optic<br>cable.|Supported with 85°C (185°F) active optic cable and<br>cards tier ≤4.|
|BOSS|BOSS-N1 is supported.|BOS-N1 is not supported.|
|PSU|Two PSUs are required in redundant mode. If there is PSU failure, system performance may be reduced.|Two PSUs are required in redundant mode. If there is PSU failure, system performance may be reduced.|
|PCIe card|Non-Dell qualified peripheral cards and peripheral cards greater than 25 W are not supported.|Non-Dell qualified peripheral cards and peripheral cards greater than 25 W are not supported.|
|GPU/FPGA|Not supported|Not supported|
|DIMM|128 GB, or greater capacity DIMMs are not supported.|128 GB, or greater capacity DIMMs are not supported.|
|PCIe SSD|Not supported|Not supported|
|Front storage|Not supported in 12 x 3.5-inch SAS configuration.|Not supported in 12 x 3.5-inch SAS configuration.|
|Rear storage|Not supported|Not supported|
|Fan|HPR SLVR fans are required in 2.5-inch configurations systems.|HPR SLVR fans are required in 2.5-inch configurations systems.|

**Table 53. Liquid cooling configurations thermal restriction for AHSRAE A3 and A4 (continued)**

|ASHRAE|A3/40°C (104°F)|A4/45°C (113°F)|
|---|---|---|
|OCP|Supported with 85°C (185°F) active optic<br>cable.|Supported with 85°C (185°F) active optic cable and<br>cards tier ≤4.|
|BOSS|BOSS-N1 is supported.|BOSS-N1 is not supported.|



# 4 Initial system setup and configuration

This section describes the tasks for initial setup and configuration of the Dell system. The section also provides general steps to
set up the system and the reference guides for detailed information.

**Topics:**

- Setting up the system
- iDRAC configuration
- Resources to install operating system



## Setting up the system

Perform the following steps to set up the system:

**Steps**

1. Unpack the system.
2. Install the system into the rack. For more information, see the rail installation and cable management accessory guides
[relevant to your rail and cable management solution at PowerEdge manuals.](https://www.dell.com/poweredgemanuals)

3. Connect the peripherals to the system and the system to the electrical outlet.
4. Power on the system.

For more information about setting up the system, see the _Getting Started Guide_ that is shipped with your system.

**NOTE:** For information about managing the basic settings and features of the system, see the Pre-operating system
management applications chapter.



## iDRAC configuration

The Integrated Dell Remote Access Controller (iDRAC) is designed to make you more productive as a system administrator and
improve the overall availability of Dell servers. iDRAC alerts you to system issues, helps you to perform remote management, and
reduces the need for physical access to the system.



### Options to set up iDRAC IP address

To enable communication between your system and iDRAC, you must first configure the network settings based on your
network infrastructure. The network settings option is set to **DHCP**, by default.

**NOTE:** For static IP configuration, you must request for the settings at the time of purchase.

You can set up the iDRAC IP address using one of the interfaces in the table below. For information about setting up iDRAC IP
address, see the documentation links provided in the table below.

|Table 54. Interfaces to set up iDRAC IP address|Col2|
|---|---|
|**Interface**|**Documentation links**|
|iDRAC Settings utility|Integrated Dell Remote Access Controller User's Guide at<br>iDRAC Manuals or for system-specific Integrated Dell Remote<br>Access Controller User's Guide, go toPowerEdge Manuals ><br>**Product Support**page of your system >**Documentation.**|

**Table 54. Interfaces to set up iDRAC IP address (continued)**

|Interface|Documentation links|
|---|---|
||**NOTE:** To determine the most recent iDRAC release for<br>your platform and for the latest documentation version,<br>see KB articleKB78115.|
|OpenManage Deployment Toolkit|Dell OpenManage Deployment Toolkit User's Guide available at<br>OpenManage Manuals > OpenManage Deployment Toolkit.|
|iDRAC Direct|Integrated Dell Remote Access Controller User's Guide at<br>iDRAC Manuals or for system-specific Integrated Dell Remote<br>Access Controller User's Guide, go toPowerEdge Manuals ><br>**Product Support** page of your system >**Documentation**.<br>**NOTE:** To determine the most recent iDRAC release for<br>your platform and for the latest documentation version,<br>see KB articleKB78115 .|
|Lifecycle Controller|Dell Lifecycle Controller User's Guide atiDRAC Manuals or for<br>system-specific Dell Lifecycle Controller User's Guide, go to<br>PowerEdge Manuals >**Product Support** page of your system<br>> **Documentation**.<br>**NOTE:** To determine the most recent iDRAC release for<br>your platform and for the latest documentation version,<br>see KB articleKB78115.|
|Server LCD panel|LCD panel section.|
|iDRAC Direct and Quick Sync 2 (optional)|Integrated Dell Remote Access Controller User's Guide at<br>iDRAC Manuals or for system-specific Integrated Dell Remote<br>Access Controller User's Guide, go toPowerEdge Manuals ><br>**Product Support** page of your system >**Documentation**.<br>**NOTE:** To determine the most recent iDRAC release for<br>your platform and for the latest documentation version,<br>see KB articleKB78115.|

**NOTE:** To access iDRAC, ensure that you connect the ethernet cable to the iDRAC dedicated network port or use the
iDRAC Direct port by using the micro USB (type AB) cable. You can also access iDRAC through the shared LOM mode, if
you have opted for a system that has the shared LOM mode enabled.



### Options to log in to iDRAC

To log in to the iDRAC Web User Interface, open a browser and enter the IP address.

You can log in to iDRAC as:

- iDRAC user
- Microsoft Active Directory user
- Lightweight Directory Access Protocol (LDAP) user

In the login screen displayed, if you have opted for secure default access to iDRAC, the default username is `root` and enter
the iDRAC secure default password available on back of the Information Tag. If you opted for legacy password, use the iDRAC
legacy username and password - `root` and `calvin`, the iDRAC default password will be blank on the information tag. Then you
will be prompted and required to create a password of your choice before proceeding. You can also log in by using your Single
Sign-On or Smart Card.

**NOTE:** Ensure that you change the default username and password after setting up the iDRAC IP address.

For more information about logging in to the iDRAC and iDRAC licenses, see the latest Integrated Dell Remote Access Controller
User's Guide

**NOTE:** To determine the most recent iDRAC release for your platform and for latest documentation version, see KB article
[KB78115.](https://www.dell.com/support/article/sln000178115)

You can also access iDRAC using command-line protocol - RACADM. For more information, see the Integrated Dell Remote
Access Controller RACADM CLI Guide.

You can also access iDRAC using automation tool - Redfish API. For more information, see the Integrated Dell Remote Access
Controller User's Guide Redfish API Guide.



## Resources to install operating system

If the system is shipped without an operating system, you can install a supported operating system by using one of the
resources provided in the table below. For information about how to install the operating system, see the documentation links
provided in the table below.

**Table 55. Resources to install the operating system**

|Resource|Documentation links|
|---|---|
|iDRAC|Integrated Dell Remote Access Controller User's Guideor for system specific Integrated Dell Remote<br>Access Controller User's Guide, go toPowerEdge Manuals >**Product Support** page of your system ><br>**Documentation**.<br>**NOTE:** To determine the most recent iDRAC release for your platform and for latest<br>documentation version, see KB article atKB78115.|
|Lifecycle Controller|Dell Lifecycle Controller User's Guide atiDRAC Manualsor for system specific Dell Lifecycle Controller<br>User's Guide, go toPowerEdge Manuals >**Product Support** page of your system >**Documentation**. <br>Dell recommends using Lifecycle Controller to install the OS, since all required drivers are installed on<br>the system.<br>**NOTE:** To determine the most recent iDRAC release for your platform and for latest<br>documentation version, see KB article atKB78115.|
|OpenManage<br>Deployment Toolkit|OpenManage Manuals > OpenManage Deployment Toolkit|
|Dell certified VMware<br>ESXi|Virtualization solutions|

**NOTE:** For more information about installation and how-to videos for operating systems supported on PowerEdge systems,
[see Supported Operating Systems for Dell PowerEdge systems.](https://www.youtube.com/playlist?list=PLe5xhhyFjDPdZ370QxaUBdENO3EKsPA2z)



### Options to download drivers and firmware

You can download the firmware from the Dell support site. For information about downloading firmware, see the Downloading
drivers and firmware section.

You can also choose any one of the following options to download the firmware. For information about how to download the
firmware, see the documentation links provided in the table below.

**Table 56. Options to download firmware**

|Option|Documentation link|
|---|---|
|Using Integrated Dell Remote Access Controller Lifecycle<br>Controller (iDRAC with LC)|iDRAC Manuals|
|Using Dell Repository Manager (DRM)|OpenManage Manuals|
|Using Dell Server Update Utility (SUU)|OpenManage Manuals|
|Using Dell OpenManage Deployment Toolkit (DTK)|OpenManage Manuals|
|Using iDRAC virtual media|iDRAC Manuals|



### Options to download and install OS drivers

You can choose any one of the following options to download and install OS drivers. For information about how to download or
install OS drivers, see the documentation links provided in the table below.

**Table 57. Options to download and install OS drivers**

|Option|Documentation|
|---|---|
|Dell support site|Downloading drivers and firmware section.|
|iDRAC virtual media|Integrated Dell Remote Access Controller User's Guide or<br>for system specific, go toIntegrated Dell Remote Access<br>Controller User's Guide >**Product Support** page of your<br>system >**Documentation**.<br>**NOTE:** To determine the most recent iDRAC release for<br>your platform and for latest documentation version, see<br>Integrated Dell Remote Access Controller Release Notes.|



### Downloading drivers and firmware

It is recommended that you download and install the latest BIOS, drivers, and systems management firmware on the system.

**Prerequisites**

Ensure that you clear the web browser cache before downloading the drivers and firmware.

**Steps**

1. [Go to Drivers.](https://www.dell.com/support/drivers)
2. Enter the Service Tag of the system in the **Enter a Dell Service Tag, Dell Product ID or Model** field, and then press
Enter.

**NOTE:** If you do not have the Service Tag, click **Browse all products**, and navigate to your product.

3. On the displayed product page, click **Drivers & Downloads** .

On the **Drivers & Downloads** page, all drivers that are applicable to the system are displayed.

4. Download the drivers to a USB drive, CD, or DVD.



# 5 Pre-operating system management applications

You can manage basic settings and features of a system without booting to the operating system by using the system firmware.



##### Options to manage the pre-operating system applications

You can use any one of the following options to manage the pre-operating system applications:

- System Setup
- Dell Lifecycle Controller
- Boot Manager
- Preboot Execution Environment (PXE)
**Topics:**

- System Setup
- Dell Lifecycle Controller
- Boot Manager
- PXE boot



## System Setup

Using the
**System Setup** option, you can configure the BIOS settings, iDRAC settings, and device settings of the
system.

You can access system setup by using any one of the following interfaces:

- Graphical User interface — To access go to iDRAC Dashboard, click **Configurations** - **BIOS Settings** .
- Text browser — To enable the text browser, use the Console Redirection.

To view
**System Setup**, power on the system, press F2, and click
**System Setup Main Menu** .

**NOTE:** If the operating system begins to load before you press F2, wait for the system to finish booting, and then restart
the system and try again.

The options on the
**System Setup Main Menu** screen are described in the following table:

|Table 58. System Setup Main Menu|Col2|
|---|---|
|**Option**|**Description**|
|**System BIOS**|Enables you to configure the BIOS settings.|
|**iDRAC Settings**|Enables you to configure the iDRAC settings. The iDRAC<br>settings utility is an interface to set up and configure<br>the iDRAC parameters by using UEFI (Unified Extensible<br>Firmware Interface). You can enable or disable various iDRAC<br>parameters by using the iDRAC settings utility. For more<br>information about this utility, seeIntegrated Dell Remote<br>Access Controller User's Guide|
|Table 58. System Setup Main Menu (continued)|Col2|
|**Option**|**Description**|
|**Device Settings**|Enables you to configure device settings for devices such as<br>storage controllers or network cards.|
|**Service Tag Settings**|Enables you to configure the System Service Tag.|



### System BIOS

To view the **System BIOS** screen, power on the system, press F2, and click **System Setup Main Menu** - **System BIOS** .

|Table 59. System BIOS details|Col2|
|---|---|
|**Option**|**Description**|
|**System Information**|Provides information about the system such as the system model name, BIOS<br>version, and Service Tag.|
|**Memory Settings**|Specifies information and options related to the installed memory.|
|**Processor Settings**|Specifies information and options related to the processor such as speed and<br>cache size.|
|**SATA Settings**|Specifies options to enable or disable the embedded SATA controller and ports.|
|**NVMe Settings**|Specifies options to change the NVMe settings. If the system contains the<br>NVMe drives that you want to configure in a RAID array, you must set both<br>this field and the**Embedded SATA** field on the**SATA Settings** menu to<br>**RAID** mode. You might also need to change the**Boot Mode** setting to**UEFI**. <br>Otherwise, you should set this field to**Non-RAID** mode.|
|**Boot Settings**|Specifies options to specify the Boot mode (BIOS or UEFI). Enables you to<br>modify UEFI and BIOS boot settings.|
|**Network Settings**|Specifies options to manage the UEFI network settings and boot protocols.<br>Legacy network settings are managed from the**Device Settings** menu.<br>**NOTE:** Network Settings are not supported in BIOS boot mode.|
|**Integrated Devices**|Specifies options to manage integrated device controllers and ports, specifies<br>related features, and options.|
|**Serial Communication**|Specifies options to manage the serial ports, its related features, and options.|
|**System Profile Settings**|Specifies options to change the processor power management settings, memory<br>frequency.|
|**System Security**|Specifies options to configure the system security settings, such as system<br>password, setup password, Trusted Platform Module (TPM) security, and UEFI<br>secure boot. It also manages the power button on the system.|
|**Redundant OS Control**|Sets the redundant OS information for redundant OS control.|
|**Miscellaneous Settings**|Specifies options to change the system date and time.|



#### System Information

To view the **System Information** screen, power on the system, press F2, and click **System Setup Main Menu** - **System**
**BIOS** - **System Information** .

|Table 60. System Information details|Col2|
|---|---|
|**Option**|**Description**|
|**System Model Name**|Specifies the system model name.|

**Table 60. System Information details (continued)**

|Option|Description|
|---|---|
|**System BIOS Version**|Specifies the BIOS version installed on the system.|
|**System Management Engine Version**|Specifies the current version of the Management Engine firmware.|
|**System Service Tag**|Specifies the system Service Tag.|
|**System Manufacturer**|Specifies the name of the system manufacturer.|
|**System Manufacturer Contact Information**|Specifies the contact information of the system manufacturer.|
|**System CPLD Version**|Specifies the current version of the system Complex Programmable<br>Logic Device (CPLD) firmware.|
|**UEFI Compliance Version**|Specifies the UEFI compliance level of the system firmware.|



#### Memory Settings

To view the **Memory Settings** screen, power on the system, press F2, and click **System Setup Main Menu** - **System BIOS**

- **Memory Settings** .

|Table 61. Memory Settings details|Col2|
|---|---|
|**Option**|**Description**|
|**System Memory Size**|Specifies the size of the system memory.|
|**System Memory Type**|Specifies the type of memory installed in the system.|
|**System Memory Speed**|Specifies the speed of the system memory.|
|**Video Memory**|Specifies the size video memory.|
|**System Memory Testing**|Specifies whether the system memory tests are run during system boot.<br>The two options available are**Enabled** and**Disabled**. This option is set to<br>**Disabled** by default.|
|**Memory Operating Mode**|This field selects the memory operating mode. This feature is active<br>only if a valid memory configuration is detected. When**Optimizer Mode**<br>is enabled, the DRAM controllers operate independently in 64-bit mode<br>and provide optimized memory performance. When**Dell Fault Resilient**<br>**Mode (FRM)** is enabled, a percentage of the total installed memory is<br>configured to create a fault resilient zone starting from lowest system<br>memory address for use by select hypervisors for host virtualization<br>resilience. Specify the FRM percentage by using the Fault Resilient<br>Mode Memory Size[%] feature. When**Dell NUMA Fault Resilient Mode**<br>**(FRM)** is enabled, a percentage of the installed memory in every NUMA<br>node is configured to create a fault resilient zone for use by select<br>hypervisors for host virtualization resilience. Specify the FRM percentage<br>by using the Fault Resilient Mode Memory Size[%] feature.|
|**Current State of Memory Operating Mode**|Specifies the current state of the memory operating mode.|
|**Fault Resilient Mode Memory Size[%]**|Select to define the percent of total memory size that must be used by<br>the fault resilient mode, when selected in the Memory Operating mode.<br>When Fault Resilient Mode is not selected, this option is grayed out and<br>not used by Fault Resilient Mode.|
|**Node Interleaving**|Enables or disables the Node interleaving option. Specifies if the Non-<br>Uniform Memory Architecture (NUMA) is supported. If this field is set<br>to**Enabled**, memory interleaving is supported if a symmetric memory<br>configuration is installed. If the field is set to**Disabled**, the system<br>supports NUMA (asymmetric) memory configurations. This option is set<br>to**Disabled** by default.|
|**ADDDC Settings**|Enables or disables ADDDC Setting feature. When Adaptive Double DRAM<br>Device Correction (ADDDC) is enabled, failing DRAMs are dynamically|

**Table 61. Memory Settings details (continued)**

|Option|Description|
|---|---|
||mapped out. When set to**Enabled** it can impact the system performance<br>under certain workloads. This feature is applicable for x4 DIMMs only. This<br>option is set to**Enabled** by default.|
|**Memory training**|When option is set to**Fast** and memory configuration is not changed,<br>the system uses previously saved memory training parameters to train<br>the memory subsystems and system boot time is also reduced. If memory<br>configuration is changed, the system automatically enables**Retrain at**<br>**Next boot** to force one-time full memory training steps, and then go back<br>to**Fast** afterward.<br>When option is set to**Retrain at Next boot**, the system performs the<br>force one-time full memory training steps at next power on and boot time<br>is slowed on next boot.<br>When option is set to**Enable**, the system performs the force full<br>memory training steps on every power on and boot time is slowed on<br>every boot.|
|**DIMM Self Healing (Post Package Repair) on**<br>**Uncorrectable Memory Error**|Enables or disables Post Packing Repair (PPR) on uncorrectable memory<br>error. This option is set to**Enabled** by default.|
|**Correctable Error Logging**|Enables or disables correctable error logging. This option is set to<br>**Disabled** by default.|
|**Memory Map Out**|This option controls DIMMs slots on the system. This option is set to<br>**Enabled** by default. It allows to disable system installed DIMMs.|



#### Processor Settings

To view the **Processor Settings** screen, power on the system, press F2, and click **System Setup Main Menu** - **System**
**BIOS** - **Processor Settings** .

|Table 62. Processor Settings details|Col2|
|---|---|
|**Option**|**Description**|
|**Logical Processor**|Each processor core supports up to two logical processors. If<br>this option is set to**Enabled**, the BIOS displays all the logical<br>processors. If this option is set to**Disabled**, the BIOS displays<br>only one logical processor per core. This option is set to**Enabled**<br>by default.|
|**CPU Interconnect Speed**|Enables you to govern the frequency of the communication links<br>among the processors in the system.<br>**NOTE:** The standard and basic bin processors support lower<br>link frequencies.<br>The options available are**Maximum data rate**, **16 GT/s**,<br>**14.4 GT/s**, and**12.8 GT/s**. This option is set to<br>**Maximum data rate** by default.<br>Maximum data rate indicates that the BIOS runs the<br>communication links at the maximum frequency<br>supported by the processors. You can also select specific<br>frequencies that the processors support, which can vary.<br>For best performance, you should select**Maximum data rate**. <br>Any reduction in the<br>communication link frequency affects the performance of non-<br>local memory access and cache coherency traffic.<br>In addition, it can slow access to non-local I/O devices from a<br>particular processor.|

**Table 62. Processor Settings details (continued)**

|Option|Description|
|---|---|
||However, if power saving considerations outweigh performance,<br>reduce the frequency of the processor<br>communication links. Before reducing the frequency, you must<br>localize the memory and I/O access to the nearest<br>NUMA node to minimize the impact to system performance.|
|**Virtualization Technology**|Enables or disables the virtualization technology for the<br>processor. This option is set to**Enabled** by default.|
|**Kernel DMA Protection**|This option is set to**Disabled** by default. When option is set to<br>**Enabled**, BIOS and Operating System will enable direct memory<br>access protection for DMA capable peripheral devices using<br>virtualization technology.|
|**Directory Mode**|Enables or disables the directory mode. This option is set to<br>**Enabled** by default.|
|**Adjacent Cache Line Prefetch**|Optimizes the system for applications that need high utilization<br>of sequential memory access. This option is set to**Enabled** by<br>default. You can disable this option for applications that need<br>high utilization of random memory access.|
|**Hardware Prefetcher**|Enables or disables the hardware prefetcher. This option is set<br>to**Enabled** by default.|
|**DCU Streamer Prefetcher**|Enables or disables the Data Cache Unit (DCU) streamer<br>prefetcher. This option is set to**Enabled** by default.|
|**DCU IP Prefetcher**|Enables or disables the Data Cache Unit (DCU) IP prefetcher.<br>This option is set to**Enabled** by default.|
|**Sub NUMA Cluster**|Enables or disables the Sub NUMA Cluster. This option is set to<br>**Disabled** by default.|
|**MADT Core Enumeration**|Specifies the MADT Core Enumeration. This option is set<br>to default in**Round Robin**. Linear option supports industry<br>core enumeration whereas, Round Robin option supports Dell<br>optimized core enumeration.|
|**UMA Based Clustering**|It is a read-only field and displays as**Quadrant**, when Sub<br>NUMA Cluster is disabled or displays as**Disabled**, when Sub<br>NUMA Cluster is either 2-way or 4-way.|
|**UPI Prefetch**|Enables you to get the memory read started early on DDR<br>bus. The Ultra Path Interconnect (UPI) Rx path spawns the<br>speculative memory that is read to Integrated Memory Controller<br>(iMC) directly. This option is set to**Enabled** by default.|
|**XPT Prefetch**|This option is set to**Enabled** by default.|
|**LLC Prefetch**|Enables or disables the LLC Prefetch on all threads. This option<br>is set to**Enabled** by default.|
|**Dead Line LLC Alloc**|Enables or disables the Dead Line LLC Alloc. This option is set to<br>**Enabled** by default. You can enable this option to enter the dead<br>lines in LLC or disable the option to not enter the dead lines in<br>LLC.|
|**Directory AtoS**|Enables or disables the Directory AtoS. AtoS optimization<br>reduces remote read latencies for repeat read accesses without<br>intervening writes. This option is set to**Disabled** by default.|
|**AVX P1**|Enables you to reconfigure the processor Thermal Design Power<br>(TDP) levels during POST based on the power and thermal<br>delivery capabilities of the system. TDP verifies the maximum|
|Table 62. Processor Settings details (continued)|Col2|
|**Option**|**Description**|
||heat the cooling system is must dissipate. This option is set to<br>**Normal** by default.<br>**NOTE:** This option is only available on certain stock keeping<br>units (SKUs) of the processors.|
|**Intel SST-BF**|Enables Intel SST-BF. This option is displayed if Performance Per<br>Watt (operating system) or Custom (when OSPM is enabled)<br>system profiles are selected. This option is set to**Disabled** by<br>default.|
|**Intel SST-CP**|Enables Intel SST-CP. This option is displayed if Performance<br>Per Watt (operating system) or Custom (when OSPM is<br>enabled) system profiles are selected. This option is displayed<br>and selectable for each system profile mode. This option is set to<br>**Disabled** by default.|
|**x2APIC Mode**|Enables or disables x2APIC mode. This option is set to**Enabled**<br>by default.<br>**NOTE:** For two processors 64 cores configuration, x2APIC<br>mode is not switchable if 256 threads are enabled (BIOS<br>settings: All CCD, cores, and logical processors enabled).|
|**AVX ICCP Pre-Grant License**|Enables or disables AVX ICCP Pre-Grant License. This option is<br>set to**Disabled** by default.|
|**Dell Controlled Turbo**|**Dell Controlled Turbo**|
|**Dell Controlled Turbo Settings**|Controls the turbo engagement. Enable this option only when<br>System Profile is set to**Performance** or**Custom**, and CPU<br>Power Management is set to**Performance**. This item can be<br>selected for each system profile mode. This option is set to<br>**Disabled** by default.<br>**NOTE:** Depending on the number of installed processors,<br>there might be up to two processor listings.|
|**Dell AVX Scaling Technology**|Enables you to configure the Dell AVX scaling technology. This<br>option is set to**0** by default. Enter the value from 0 to 12<br>bins. The value that is entered decreases the Dell AVX Scaling<br>Technology frequency when the Dell-controlled Turbo feature is<br>enabled.|
|**Optimizer Mode**|Enables or disables the CPU performance. When this option<br>is set to**Auto**, set the CPU Power Management to Max<br>Performance. When set to**Enabled**, enables the CPU Power<br>Management settings. When set to**Disabled**, the CPU Power<br>Management option is disabled. This option is set to**Auto** by<br>default.|
|**Number of Cores per Processor**|Controls the number of enabled cores in each processor. This<br>option is set to**All** by default.|
|**CPU Physical Address Limit**|Limit CPU physical address to 46 bits to support older Hyper-V.<br>If enabled, automatically disables TME-MT. This option is set to<br>**Enabled** by default.|
|**AMP Prefetch**|This option enables one of the Mid-Level Cache (MLC) AMP<br>hardware Prefetcher. This option is set to**Disabled** by default.|
|**Homeless Prefetch**|This option allows L1 Data Cache Unit (DCU) to prefetech, when<br>the Fill Buffers (FB) is full. Auto maps to hardware default<br>setting. This option is set to**Auto** by default.|
|**Uncore Frequency RAPL**|This setting controls whether the Running Average Power Limit<br>(RAPL) balancer is enabled or not. If enabled, it activates the|

|Table 62. Processor Settings details (continued)|Col2|
|---|---|
|**Option**|**Description**|
||uncore power budgeting. This option is set to**Enabled** by<br>default.|
|**Processor Core Speed**|Specifies the maximum core frequency of the processor.|
|**Processor Bus Speed**|Specifies the bus speed of the processor.<br>**NOTE:** The processor bus speed option displays only when<br>both processors are installed.|
|**Local Machine Check Exception**|Enables or disables the local machine check exception. This<br>is an extension of the MCA Recovery mechanism providing<br>the capability to deliver Uncorrected Recoverable (UCR)<br>Software Recoverable Action Required (SRAR) errors to one<br>or more specific logical processors threads receiving previously<br>poisoned or corrupted data. When enabled, the UCR SRAR<br>Machine Check Exception is delivered only to the affected<br>thread rather than broadcast to all threads in the system.<br>The feature supports operating system recovery for cases of<br>multiple recoverable faults that are detected close, which would<br>otherwise result in a fatal machine check event. The feature is<br>available only on Advanced RAS processors. This option is set to<br>**Enabled** by default.|
|**CPU Crash Log Support**|This field controls Intel CPU Crash Log feature for collection<br>of previous crash data from shared SRAM of Out-of -Band<br>Management Service Module at post reset. This option is set<br>to**Disabled** by default.|
|**Processor n**|**NOTE:** Depending on the number of processors, there might<br>be up to n processors listed.<br>The following settings are displayed for each processor:|

|Table 63. Processor n details|Col2|
|---|---|
|**Option**|**Description**|
|**Family-Model-Stepping**|Specifies the family, model, and stepping of the processor as<br>defined by Intel.|
|**Brand**|Specifies the brand name.|
|**Level 2 Cache**|Specifies the total L2 cache.|
|**Level 3 Cache**|Specifies the total L3 cache.|
|**Number of Cores**|Specifies the number of cores per processor.|
|**Microcode**|Specifies the processor microcode version.|



#### SATA Settings

To view the **SATA Settings** screen, power on the system, press F2, and click **System Setup Main Menu** - **System BIOS** **SATA Settings** .

|Table 64. SATA Settings details|Col2|
|---|---|
|**Option**|**Description**|
|**Embedded SATA**|Enables the embedded SATA option to be set to**Off**, **AHCI mode**, or**RAID modes**. <br>This option is set to**AHCI Mode** by default.<br>**NOTE:**<br>1.<br>You might also need to change the Boot Mode setting to UEFI. Otherwise, you<br>should set the field to Non-RAID mode.|

**Table 64. SATA Settings details (continued)**

|Option|Description|
|---|---|
||2. No ESXi and Ubuntu OS support under RAID mode.|
|**Security Freeze Lock**|Sends**Security Freeze Lock** command to the embedded SATA drives during POST.<br>This option is applicable only for AHCI Mode. This option is set to**Enabled** by default.|
|**Write Cache**|Enables or disables the command for the embedded SATA drives during POST. This<br>option is applicable only for AHCI Mode. This option is set to**Disabled** by default.|
|**Port n**|Sets the drive type of the selected device.<br>For**AHCI Mode**, BIOS support is always enabled.|

|Table 65. Port n|Col2|
|---|---|
|**Options**|**Descriptions**|
|**Model**|Specifies the drive model of the selected device.|
|**Drive Type**|Specifies the type of drive attached to the SATA port.|
|**Capacity**|Specifies the total capacity of the drive. This field is undefined<br>for removable media devices such as optical drives.|



#### NVMe Settings

This option sets the NVMe drive mode. If the system contains NVMe drives that you want to configure in a RAID array, you
must set both this field and the Embedded SATA field on the SATA settings menu to RAID Mode. You may also need to change
the Boot Mode setting to UEFI.

To view the **NVMe Settings** screen, power on the system, press F2, and click **System Setup Main Menu** - **System BIOS** **NVMe Settings** .

|Table 66. NVMe Settings details|Col2|
|---|---|
|**Option**|**Description**|
|**NVMe mode**|Enables or disables the boot mode. The option is set to**Non-RAID** mode by default.|
|**BIOS NVMe Driver**|Sets the drive type to boot the NVMe driver. The available options are**Dell Qualified**<br>**Drives** and**All Drives**. This option is set to**Dell Qualified Drives** by default.|



#### Boot Settings

You can use the **Boot Settings** screen to set the boot mode to either **BIOS** or **UEFI** . It also enables you to specify the boot
order.

- **UEFI** : The Unified Extensible Firmware Interface (UEFI) is a new interface between operating systems and platform
firmware. The interface consists of data tables with platform related information, boot and runtime service calls that are
available to the operating system and its loader. The following benefits are available when the **Boot Mode** is set to **UEFI** :

  - Support for drive partitions larger than 2 TB.
  - Enhanced security (e.g., UEFI Secure Boot).
  - Faster boot time.
**NOTE:** You must use only the UEFI boot mode in order to boot from NVMe drives.

- **BIOS** : The **BIOS Boot Mode** is the legacy boot mode. It is maintained for backward compatibility.
To view the **Boot Settings** screen, power on the system, press F2, and click **System Setup Main Menu** - **System BIOS** **Boot Settings** .

|Table 67. Boot Settings details|Col2|
|---|---|
|**Option**|**Description**|
|**Boot Mode**|Enables you to set the boot mode of the system. If the operating system supports<br>UEFI, you can set this option to UEFI. Setting this field to BIOS allows compatibility<br>with non-UEFI operating systems. This option is set to**UEFI** by default.<br>**CAUTION: Switching the boot mode may prevent the system from**<br>**booting if the operating system is not installed in the same boot mode.**<br>**NOTE:** Setting this field to UEFI disables the**BIOS Boot Settings** menu.|
|**Boot Sequence Retry**|Enables or disables the Boot sequence retry feature or resets the system. When<br>this option is set to**Enabled** and the system fails to boot, the system re-attempts<br>the boot sequence after 30 seconds. When this option is set to**Reset** and the<br>system fails to boot, the system reboots immediately. This option is set to**Enabled**<br>by default.|
|**Hard-disk Failover**|Enables or disables the Hard-disk failover. This option is set to**Disabled** by default.|
|**Generic USB Boot**|Enables or disables the generic USB boot placeholder. This option is set to<br>**Disabled** by default.|
|**Hard-disk Drive Placeholder**|Enables or disables the Hard-disk drive placeholder. This option is set to**Disabled**<br>by default.|
|**Clean all Sysprep variables and order**|When this option is set to**None**, BIOS will do nothing. When set to**Yes**, BIOS will<br>delete variables of SysPrep #### and SysPrepOrder this option is a onetime option,<br>will reset to none when deleting variables. This setting is only available in**UEFI**<br>**Boot Mode**. This option is set to**None** by default.|
|**UEFI Boot Settings**|Specifies the UEFI boot sequence. Enables or disables UEFI Boot options.<br>**NOTE:** This option controls the UEFI boot order. The first option in the list will<br>be attempted first.|

|Table 68. UEFI Boot Settings|Col2|
|---|---|
|**Option**|**Description**|
|**UEFI Boot Sequence**|Enables you to change the boot device order.|
|**Boot Options Enable/Disable**|Enables you to select the enabled or disabled boot devices|



##### Choosing system boot mode

System Setup enables you to specify one of the following boot modes for installing your operating system:

- UEFI boot mode (the default), is an enhanced 64-bit boot interface. If you have configured your system to boot to UEFI
mode, it replaces the system BIOS.
1. From the **System Setup Main Menu**, click **Boot Settings**, and select **Boot Mode** .
2. Select the UEFI boot mode you want the system to boot into.
**CAUTION: Switching the boot mode may prevent the system from booting if the operating system is not**
**installed in the same boot mode.**

3. After the system boots in the specified boot mode, proceed to install your operating system from that mode.
**NOTE:** Operating systems must be UEFI-compatible to be installed from the UEFI boot mode. DOS and 32-bit operating
systems do not support UEFI and can only be installed from the BIOS boot mode.

**NOTE:** [For the latest information about supported operating systems, go to OS support.](https://www.dell.com/ossupport)



##### Changing boot order

**About this task**

You may have to change the boot order if you want to boot from a USB key or an optical drive. The following instructions may
vary if you have selected **BIOS** for **Boot Mode** .

**NOTE:** Changing the drive boot sequence is only supported in BIOS boot mode.

**Steps**

1. On the **System Setup Main Menu** screen, click **System BIOS** - **Boot Settings** - **UEFI Boot Settings** - **UEFI Boot**
**Sequence** .

2. Use the arrow keys to select a boot device, and use the plus (+) and minus (-) sign keys to move the device down or up in
the order.

3. Click **Exit**, and then click **Yes** to save the settings on exit.
**NOTE:** You can also enable or disable boot order devices as needed.



#### Network Settings

To view the **Network Settings** screen, power on the system, press F2, and click **System Setup Main Menu** - **System BIOS**

- **Network Settings** .
**NOTE:** Network Settings are not supported in BIOS boot mode.

|Table 69. Network Settings details|Col2|
|---|---|
|**Option**|**Description**|
|**UEFI PXE Settings**|Enables you to control the configuration of the UEFI PXE device.|
|**Number of PXE Devices**|This field specifies the number of PXE devices. This option is set to**4** by<br>default.|
|**PXE Device n** (n = 1 to 4)|Enables or disables the device. When enabled, a UEFI PXE boot option is<br>created for the device.|
|**PXE Device n Settings**(n = 1 to 4)|Enables you to control the configuration of the PXE device.|
|**UEFI HTTP Settings**|Enables you to control the configuration of the UEFI HTTP device.|
|**HTTP Device n** (n = 1 to 4)|Enables or disables the device. When enabled, a UEFI HTTP boot option is<br>created for the device.|
|**HTTP Device n Settings** (n = 1 to 4)|Enables you to control the configuration of the HTTP device.|
|**UEFI iSCSI Settings**|Enables you to control the configuration of the iSCSI device.|
|**iSCSI Initiator Name**|Specifies the name of the iSCSI initiator in IQN format.|
|**iSCSI Device1**|Enables or disables the iSCSI device. When disabled, a UEFI boot option<br>is created for the iSCSI device automatically. This is set to**Disabled** by<br>default.|
|**iSCSI Device1 Settings**|Enables you to control the configuration of the iSCSI device.|
|**UEFI NVMe-oF Settings**|Enables you to control the configuration of the NVMe-oF devices.|
|**NVMe-oF**|Enables or disables the NVMe-oF feature. When enabled, it allows to<br>configure the host and target parameters needed for fabric connection.<br>This is set to**Disabled** by default.|
|**NVMe-oF Host NQN**|This field specifies the name of the NVMe-oF host NQN. Allowed input is<br>in the following format: nqn.yyyy-mm.<Reserved Domain Name>:<Unique<br>String>. Leave it empty to use system generated value with following<br>format: nqn.1988-11.com.dell:<Model name>.<Model number>.<Service<br>Tag>.|

**Table 69. Network Settings details (continued)**

|Option|Description|
|---|---|
|**NVMe-oF Host Id**|This field specifies a 16 bytes value of the NVMe-oF host identifier<br>that uniquely identifies this host with the controller in the NVM<br>subsystem. Allowed input is a hexadecimal-encoded string in this format:<br>00112233-4455-6677-8899-aabbccddeeff. Leave it empty to use system<br>generated value. A value of all FF is not allowed.|
|**Host Security Key Path**|This field specifies the Host security key path.|
|**NVMe-oF SubSystem Settings**|This field controls the parameters for the NVMe-oF subsystem n<br>connections.|
|**Interface**|Specifies NIC interface used for the PXE device.|
|**Protocol**|Specifies Protocol used for PXE device. This option is set to**IPv4** or**IPv6**. This option is set to<br>**IPv4** by default.|
|**Vlan**|Enables Vlan for PXE device. This option is set to**Enabled** or**Disabled**. This option is set to<br>**Disabled** by default.|
|**Vlan ID**|Shows the Vlan ID for the PXE device|
|**Vlan Priority**|Shows the Vlan Priority for the PXE device.|

**Table 71. HTTP Device n Settings details**

|Option|Description|
|---|---|
|**Interface**|Specifies NIC interface used for the HTTP device.|
|**Protocol**|Specifies Protocol used for HTTP device. This option is set to**IPv4** or**IPv6**. This option is set<br>to**IPv4** by default.|
|**Vlan**|Enables Vlan for HTTP device. This option is set to**Enable** or**Disable**. This option is set to<br>**Disable** by default.|
|**Vlan ID**|Shows the Vlan ID for the HTTP device|
|**Vlan Priority**|Shows the Vlan Priority for the HTTP device.|
|**DHCP**|Enables or disables DHCP for this HTTP device. This option is set to**Enabled** by default.|
|**IP Address**|Specifies IP address for the HTTP device.|
|**Subnet Mask**|Specifies subnet mask for the HTTP device.|
|**Autoconfiguration**|Enables or disables the**IPv6Autoconfiguration** for the HTTP Device. When set to Enabled,<br>IPv6 Address and Gateway are retrieved from Autoconfiguration mechanism.|
|**Prefix Length**|IPv6 Prefix Length (0~127) for this HTTP Device.|
|**IPv6 Address**|IPv6 Unicast address for this HTTP Device.|
|**Gateway**|Specifies gateway for the HTTP device.|
|**DNS info via DHCP**|Enables or disables DNS Information from DHCP. This option is set to**Enabled** by default.|
|**Primary DNS**|Specifies the primary DNS server IP address for the HTTP Device.|
|**Secondary DNS**|Specifies the secondary DNS server IP address for the HTTP Device.|
|**URI (will obtain from**<br>**DHCP server if not**<br>**specified)**|Obtain URI from the DHCP server if not specified|
|**TLS Authentication**<br>**Configuration**|Specifies the option for TLS authentication configuration.|
|**Connection 1**|Enables or disables the iSCSI connection. This option is set to**Disabled** by default.|
|**Connection 2**|Enables or disables the iSCSI connection. This option is set to**Disabled** by default.|
|**Connection 1 Settings**|Enables you to control the configuration for the iSCSI connection.|
|**Connection 2 Settings**|Enables you to control the configuration for the iSCSI connection.|
|**Connection Order**|Enables you to control the order for which the iSCSI connections will be<br>attempted.|
|**ISCSI F1/F2 Prompt on Error**|This field determines whether the BIOS stops and displays a prompt when iSCSI<br>connection errors occur during POST. The BIOS will display the prompt when this<br>setting is**Enabled**, otherwise, the BIOS will continue through POST and attempt<br>to boot an operating system.<br>**NOTE:** This setting will be grayed out if F1/F2 Prompt on Error in the<br>Miscellaneous Settings menu is**Disabled**.|

**Table 73. TLS Authentication Configuration screen details**

|Option|Description|
|---|---|
|**TLS Authentication Mode**|View or modify the device's boot TLS Authentication Mode. This option is set to**One**<br>**Way** by default.**None** means the HTTP server and the client will not authenticate each<br>other for this boot.|
|**Root Certificate Configuration**|Import, delete, or export the root certificate.|
|**NVMe-oF SubSystem n** (n = 1 to 4)|Enables or disables NVMe-oF SubSystem. This option is set to**Disabled** by<br>default.|
|**NVMe-oF SubSystem n Settings** (n = 1<br>to 4)|Enables you to control the configuration of the NVMe-oF SubSystem, if**Enabled**.|

**Table 75. NVMe-oF SubSystem n Settings**

|Option|Description|
|---|---|
|**Interface**|NIC interface used for NVMe-oF connections. This option is set to**Embedded**<br>**NIC 1 Port 1 Partition 1** by default.|
|**Transport Type**|This field sets the value of transport type for NVMe-oF connection. This option is<br>set to**TCP** by default.|
|**Protocol**|This field sets the value of protocol type for NVMe-oF connection. This option is<br>set to**IPv4** by default. When** IPv6** is selected, IPv6 Autoconfiguration is used to<br>get the IPv6 address.|
|**VLAN**|Enables or disables VLAN for this NVMe-oF connections. This option is set to<br>**Disabled** by default.|
|**VLAN Id**|Specifies the VLAN Id for this NVMe-oF connection. This option is set to**1** by<br>default.|
|**VLAN Priority**|Specifies the VLAN priority for this NVMe-oF connection. This option is set to**0** by<br>default.|
|**Retry Count**|Specifies the retry count for this NVMe-oF connection. This option is set to**3** by<br>default.|
|**Timeout**|Specifies the time out for this NVMe-oF connection. This option is set to**10000** by<br>default.|
|**DHCP**|NVMe-oF connection gets subsystem's information from the DHCP server. This<br>option is set to**Disabled** by default.|
|**Host IP Address**|Specifies the Host IP Address for this NVMe-oF connection.|
|**Host Subnet Mask**|Specifies the Host Subnet Mask for this NVMe-oF connection.|
|**Host Gateway**|Specifies the Host Gateway for this NVMe-oF connection.|
|**NVMe-oF subsystem info via DHCP**|Enables and disables the NVMe-oF subsystem's DHCP for this connection. This<br>option is set to**Disabled** by default.|
|**NVMe-oF subsystem NQN**|Specifies the NVMe-oF subsystem's NQN for this connection.|
|**NVMe-oF subsystem Address**|Specifies the NVMe-oF subsystem's IP address for this connection.|
|**NVMe-oF subsystem Port**|Specifies the NVMe-oF subsystem's port for this connection. If subsystem NQN<br>is empty, this field will be forced to the default Discovery Service port**8009**.This<br>option is set to**4420** by default.|
|**NVMe-oF subsystem NID**|Specifies the NamespaceID (NID) for this NVMe-oF connection.|
|**NVMe-oF subsystem Controller ID**|Specifies the NVMe-oF subsystem's Controller ID for this connection. This option<br>is set to**0** by default.|
|**Security**|Enables or disables the security option for this NVMe-oF connection. This option is<br>set to**Disabled** by default.|
|**Authentication Type**|Specifies the authentication type for this NVMe-oF connection. This option is set<br>to**None** by default.|
|**Securitykeypath**|Specifies the Securitykeypath for this NVMe-oF connection.|



#### Integrated Devices

To view the **Integrated Devices** screen, power on the system, press F2, and click **System Setup Main Menu** - **System BIOS**

- **Integrated Devices** .

|Table 76. Integrated Devices details|Col2|
|---|---|
|**Option**|**Description**|
|**User Accessible USB Ports**|Configures the user accessible USB ports. Selecting**Only Back Ports On**<br>disables the front USB ports; selecting**All Ports Off** disables all front and<br>back USB ports.; selecting**All Ports Off (Dynamic)** disables all front and back<br>USB ports during POST. and front ports can be enabled or disabled dynamically<br>by authorized user without resetting the system. This option is set to**All Ports**<br>**On** by default.|
|**User Accessible USB Ports**|The USB keyboard and mouse still function in certain USB ports during the<br>boot process, depending on the selection. After the boot process is complete,<br>the USB ports will be enabled or disabled as per the setting.|
|**iDRAC Direct USB Port**|The iDRAC Direct USB port is managed by iDRAC exclusively with no host<br>visibility. This option is set to**ON** or**OFF**. When set to**OFF**, iDRAC does not<br>detect any USB devices installed in this managed port. This option is set to**On**<br>by default.|
|**Embedded NIC1 and NIC2**|Enables or disables the OS interface of the Embedded NIC1 and NIC2<br>controller. If set to**Disabled (OS)**, the NIC may still be available for shared<br>network access by the embedded management controller. Configure the<br>**Embedded NIC1 and NIC2** option by using the NIC management utilities of<br>the system. This option is set to**Enabled** by default.|

**Table 76. Integrated Devices details (continued)**

|Option|Description|
|---|---|
|**I/OAT DMA Engine**|Enables or disables the I/O Acceleration Technology (I/OAT) option. I/OAT is<br>a set of DMA features designed to accelerate network traffic and lower CPU<br>utilization. Enable only if the hardware and software support the feature. This<br>option is set to**Disabled** by default.|
|**Embedded Video Controller**|Enables or disables the use of Embedded Video Controller as the primary<br>display. When set to**Enabled**, the Embedded Video Controller will be the<br>primary display even if add-in graphic cards are installed. When set to<br>**Disabled**, an add-in graphics card is used as the primary display. BIOS will<br>output displays to both the primary add-in video and the embedded video<br>during POST and preboot environment. The embedded video will then be<br>disabled right before the operating system boots. This option is set to**Enabled**<br>by default.<br>**NOTE:** When there are multiple add-in graphic cards installed in the<br>system, the first card discovered during PCI enumeration is selected as<br>the primary video. You might have to rearrange the cards in the slots in<br>order to control which card is the primary video.|
|**I/O Snoop HoldOff Response**|Selects the number of cycles PCI I/O can withhold snoop requests, from the<br>CPU, to allow time to complete its own write to LLC. This setting can help<br>improve performance on workloads where throughput and latency are critical.<br>The options available are**256 Cycles**, **512 Cycles**, **1K Cycles**, **2K Cycles**, <br>**4K Cycles**, **8K Cycles**, **16K Cycles**, **32K Cycles**, **64K Cycles** and**128K**<br>**Cycles**.This option is set to**2K Cycles** by default.|
|**Current State of Embedded Video**<br>**Controller**|Displays the current state of the embedded video controller. The**Current**<br>**State of Embedded Video Controller** option is a read-only field. If the<br>Embedded Video Controller is the only display capability in the system (that<br>is, no add-in graphics card is installed), then the Embedded Video Controller<br>is automatically used as the primary display even if the**Embedded Video**<br>**Controller** setting is set to**Disabled**.|
|**SR-IOV Global Enable**|Enables or disables the BIOS configuration of Single Root I/O Virtualization<br>(SR-IOV) devices. This option is set to**Disabled** by default.|
|**OS Watchdog Timer**|If your system stops responding, this watchdog timer aids in the recovery<br>of your operating system. When this option is set to**Enabled**, the operating<br>system initializes the timer. When this option is set to**Disabled** (the default),<br>the timer does not have any effect on the system.|
|**Empty Slot Unhide**|Enables or disables the root ports of all the empty slots that are accessible to<br>the BIOS and operating system. This option is set to**Disabled** by default.|
|**Slot Disablement**|Enables or disables or boot driver disables the available PCIe slots on your<br>system. The slot disablement feature controls the configuration of the PCIe<br>cards installed in the specified slot. Slots must be disabled only when the<br>installed peripheral card prevents booting into the operating system or causes<br>delays in system startup. If the slot is disabled, both the Option ROM and UEFI<br>drivers are disabled. Only slots that are present on the system will be available<br>for control. When this option is set to boot driver disabled, both the Option<br>ROM and UEFI driver from the slot will not run during POST. The system will<br>not boot from the card and its pre-boot services will not be available. However,<br>the card is available to the operating system.|
|**Slot Disablement**|**Slot n**: Enables or disables or only the boot driver is disabled for the PCIe slot<br>n. This option is set to**Enabled** by default.|
|**Slot Bifurcation**|**Auto Discovery Bifurcation Settings** allows**Platform Default Bifurcation**, <br>**Auto Discovery of Bifurcation**, and**Manual bifurcation Control**.|
|**Slot Bifurcation**|This option is set to**Platform Default Bifurcation** by default. The slot<br>bifurcation field is accessible when set to**Manual bifurcation Control** and<br>is grayed out when set to**Platform Default Bifurcation** and**Auto Discovery**<br>**of Bifurcation**.|
||**NOTE:** The slot bifurcation supports on PCIe slot only, does not support<br>slot type from Paddle card to Riser and Slimline connector to Riser.|



#### Serial Communication

To view the **Serial Communication** screen, power on the system, press F2, and click **System Setup Main Menu** - **System**
**BIOS** - **Serial Communication** .

**NOTE:** The serial port is optional for the PowerEdge R760 system. The Serial Communication option is applicable only if the
serial COM port is installed in the system.

|Table 77. Serial Communication details|Col2|
|---|---|
|**Option**|**Description**|
|**Serial Communication**|Enables the serial communication options. Selects serial communication<br>devices (Serial Device 1 and Serial Device 2) in BIOS. BIOS console redirection<br>can also be enabled, and the port address can be specified.<br>The options available for System without serial COM port (DB9) are**On**<br>**without Console Redirection**,<br>**On with Console Redirection**, **Off**, **Auto**. This option is set to<br>**Auto** if the external serial connector is available (connected to the rear I/O<br>board).<br>Else the default will be**Off**.|
|**Serial Port Address**|Enables you to set the port address for serial devices. This<br>option is set to either**COM1** or**COM2** for the serial device<br>**(COM1=0x3F8,COM2=0x2F8)**and set to**COM1** by default.<br>**NOTE:** You can use only Serial Device 2 for the Serial Over LAN (SOL)<br>feature. To use console redirection by SOL, configure the same port<br>address for console redirection and the serial device.<br>**NOTE:** Every time the system boots, the BIOS syncs the serial MUX<br>setting that is saved in iDRAC. The serial MUX setting can independently<br>be changed in iDRAC. Loading the BIOS default settings from within the<br>BIOS setup utility may not always revert the serial MUX setting to the<br>default setting of Serial Device 1.|
|**External Serial Connector**|Enables you to associate the External Serial Connector to**Serial Device 1**, <br>**Serial Device 2**, or the**Remote Access Device** by using this option. This<br>option is set to**Serial Device 1** by default.<br>**NOTE:** Only Serial Device 2 can be used for Serial Over LAN (SOL).<br>To use console redirection by SOL, configure the same port address for<br>console redirection and the serial device.<br>**NOTE:** Every time the system boots, the BIOS syncs the serial MUX<br>setting saved in iDRAC. The serial MUX setting can independently be<br>changed in iDRAC. Loading the BIOS default settings from within the BIOS<br>setup utility may not always revert this setting to the default setting of<br>Serial Device 1.|
|**Failsafe Baud Rate**|Specifies the failsafe baud rate for console redirection. The BIOS attempts to<br>determine the baud rate automatically. This failsafe baud rate is used only if<br>the attempt fails, and the value must not be changed. This option is set to<br>**115200** by default.|
|**Remote Terminal Type**|Sets the remote console terminal type. This option is set to**VT100/VT220** by<br>default.|

**Table 77. Serial Communication details (continued)**

|Option|Description|
|---|---|
|**Redirection After Boot**|Enables or disables the BIOS console redirection when the operating system is<br>loaded. This option is set to**Enabled** by default.|
|**System Profile**|Sets the system profile. If you set the System Profile option to a mode other than<br>**Performance Per Watt (DAPC)**, the BIOS automatically sets the rest of the options. You<br>can only change the rest of the options if the mode is set to**Custom**. This option is set to<br>**Performance Per Watt (DAPC)** by default. Other options include**Custom**, **Performance**, <br>**Performance Per Watt (OS)** and**Workstation Performnce**.<br>**NOTE:** All the parameters on the system profile setting screen are available only when<br>the**System Profile** option is set to**Custom**.|
|**CPU Power Management**|Sets the CPU power management. This option is set to**System DBPM (DAPC)** by default.<br>Other option includes**Maximum Performance**, **OS DBPM**.|
|**Memory Frequency**|Sets the speed of the system memory. You can select**Maximum Performance**, **Maximum**<br>**Reliability** or a specific speed. This option is set to**Maximum Performance** by default.|
|**Turbo Boost**|Enables or disables the processor to operate in the turbo boost mode. This option is set to<br>**Enabled** by default.|
|**Enery Efficient Turbo**|Energy Efficient Turbo (EET) is a mode of operation where a processor's core frequency is<br>adjusted within the turbo range based on workload. This option is set to**Enabled** by default.|
|**C1E**|Enables or disables the processor to switch to a minimum performance state when it is idle.<br>This option is set to**Enabled** by default.|
|**C States**|Enables or disables the processor to operate in all available power states. C States allow the<br>processor to enter lower power states when idle. When set to**Enabled** (OS controlled) or<br>when set to**Autonomous** (if hardware controlled is supported), the processor can operate<br>in all available Power States to save power, but may increase memory latency and frequency<br>jitter. This option is set to**Enabled** by default.|
|**Memory Patrol Scrub**|Sets the memory patrol scrub mode. This option is set to**Standard** by default.|
|**Memory Refresh Rate**|Sets the memory refresh rate to either 1x or 2x. This option is set to**1x** by default.|
|**Uncore Frequency**|Enables you to select the**Uncore Frequency** option.**Dynamic mode** enables the processor<br>to optimize power resources across cores and uncores during runtime. The optimization of<br>the uncore frequency to either save power or optimize performance is influenced by the<br>setting of the**Energy Efficiency Policy** option.|
|**Energy Efficient Policy**|Enables you to select the**Energy Efficient Policy** option. The CPU uses the setting to<br>manipulate the internal behavior of the processor and determines whether to target higher<br>performance or better power savings. This option is set to**Balanced Performance** by<br>default.|
|**Monitor/Mwait**|Enables the Monitor/Mwait instructions in the processor. This option is set to**Enabled** for all<br>system profiles, except**Custom** by default.<br>**NOTE:** This option can be disabled when System Profile is set to**Custom**.<br>**NOTE:** When C States is set to Enabled in the Custom mode, changing the Monitor/<br>Mwait setting does not impact the system power or performance.|

**Table 78. System Profile Settings details (continued)**

|Option|Description|
|---|---|
|**Workload Profile**|This option allows the user to specify the targeted workload of a server. It allows<br>optimization of performance based on the workload type. This option is set to**Not**<br>**Configured** by default.|
|**Dynamic Load Line Switch**|Dynamic Load Line Switch control. Dynamic Load Line (DLL) is a Power Management<br>feature, which dynamically switches to the performance mode during high CPU utilization.<br>This setting is read-only and is set to**Enabled** when Optimized Power Mode is Enabled.<br>Read-only unless System Profile is set to**Custom**.|
|**CPU Interconnect Bus Link**<br>**Power Management**|Enables or disables the CPU Interconnect Bus Link Power Management. This option is set to<br>**Enabled** by default.|
|**PCI ASPM L1 Link Power**<br>**Management**|Enables or disables the PCI**ASPM L1 Link Power Management**. This option is set to<br>**Enabled** by default.|
|**CPU AES-NI**|Improves the speed of applications by performing encryption and decryption by using<br>the Advanced Encryption Standard Instruction Set (AES-NI). This option is set to<br>**Enabled** by default.|
|**System Password**|Sets the system password. This option is read-only if the password jumper is not<br>installed in the system.|
|**Setup Password**|Sets the setup password. This option is read-only if the password jumper is not installed<br>in the system.|
|**Password Status**|Locks the system password. This option is set to**Unlocked** by default.|
|**TPM Information**|Indicates the type of Trusted Platform Module, if present.|
|**EMR CPU TDX/ IFS features**|This option is set to**Disabled**by default.|

|Table 80. TPM 2.0 security information|Col2|
|---|---|
|**Option**|**Description**|
|**TPM Information**|**TPM Information**|
|**TPM Security**|**NOTE:** The TPM menu is available only when the TPM module is<br>installed.<br>Enables you to control the reporting mode of the TPM. When set to Off,<br>the presence of the TPM is not reported to the OS. When set to On, the<br>presence of the TPM is reported to the OS. The**TPM Security** option is<br>set to**Off** by default.|
|**TPM Security**|When TPM 2.0 is installed, the**TPM Security** option is set to**On** or**Off**. <br>This option is set to**Off** by default.|
|**TPM Information**|Indicates the type of Trusted Platform Module, if present.|
|**TPM Firmware**|Indicates the firmware version of the TPM.|
|**TPM Hierarcy**|Enables, disables, or clears the storage and endorsement hierarchies.<br>When set to**Enabled**, the storage and endorsement hierarchies can be<br>used.|

**Table 80. TPM 2.0 security information (continued)**

|Option|Col2|Description|
|---|---|---|
|||When set to**Disabled**, the storage and endorsement hierarchies cannot<br>be used.|
|||When set to**Clear**, the storage and endorsement hierarchies are cleared<br>of any values, and then reset to**Enabled**.|
|**TPM**<br>**Advanced**<br>**Settings**|**TPM PPI Bypass Provision**|When set to**Enabled**, allows the Operating System to bypass<br>Physical Presence Interface (PPI) prompts when issuing PPI Advanced<br>Configuration and Power interface (ACPI) provisioning operations.|
|**TPM**<br>**Advanced**<br>**Settings**|**TPM PPI Bypass Clear**|When set to**Enabled** allows the Operating System to bypass<br>Physical Presence Interface (PPI) prompts when issuing PPI Advanced<br>Configuration and Power Interface (ACPI) clear operations.|
|**TPM**<br>**Advanced**<br>**Settings**|**TPM2 Algorithm Selection**|Allows the user to change the cryptographic algorithms used in the<br>Trusted Platform Module (TPM). The available options are dependent on<br>the TPM firmware.<br>To enable TPM2 Algorithm Selection, Intel(R) TXT technology must be<br>disabled.<br>The TPM2 Algorithm Selection option supports SHA1, SHA128, SHA256,<br>SHA512 and SM3 by detecting the TPM module. This option is set to<br>**SHA1** by default.|

**Table 81. System Security details**

|Option|Description|
|---|---|
|**Intel(R) TXT**|Enables you to set the Intel Trusted Execution Technology (TXT) option. To enable<br>the**Intel TXT** option, virtualization technology and TPM Security must be enabled with<br>Pre-boot measurements. This option is set to**Off** by default. It is set**On** for Secure<br>Launch (Firmware Protection) support on Windows 2022.|
|**Memory Encryption**|Enables or disables the Intel Total Memory Encryption (TME) and Multi-Tenant (Intel® <br>TME-MT). When option is set to**Disabled**, BIOS disables both TME and MK-TME<br>technology. When option is set to**Single Key** BIOS enables the TME technology. When<br>option is set to**Multiple Keys**, BIOS enables the TME-MT technology. This option is set<br>to**Disabled** by default.|
|**TME Encryption Bypass**|Allows the option to bypass the Intel Total Memory Encryption. This option is set to<br>**Disabled** by default.|
|**Intel(R) SGX**|Enables you to set the Intel Software Guard Extension (SGX) option. To enable the**Intel**<br>**SGX** option, processor must be SGX capable, memory population must be compatible<br>(minimum x8 identical DIMM1 to DIMM8 per CPU socket, not support on persistent<br>memory configuration), memory operating mode must be set at optimizer mode, memory<br>encryption must be enabled and node interleaving must be disabled. This option is set to<br>**Off** by default. When this option is to**Off**, BIOS disables the SGX technology. When this<br>option is to**On**, BIOS enables the SGX technology.|
|**Power Button**|Enables or disables the power button on the front of the system. This option is set to<br>**Enabled** by default.|
|**AC Power Recovery**|Sets how the system behaves after AC power is restored to the system. This option is<br>set to**Last** by default.<br>**NOTE:** The host system will not power on until iDRAC Root of Trust (RoT) is<br>completed, host power on will be delayed by minimum 90 seconds after the AC<br>applied.|
|**AC Power Recovery Delay**|Sets the time delay for the system to power up after AC power is restored to<br>the system. This option is set to**Immediate** by default. When this option is set to<br>**Immediate**, there is no delay for power up. When this option is set to**Random**, the|
||system creates a random delay for power up. When this option is set to**User Defined**, <br>the system delay time is manually to power up.|
|**User Defined Delay (120 s to 600**<br>**s)**|Sets the**User Defined Delay** option when the**User Defined** option for**AC Power**<br>**Recovery Delay** is selected. The actual AC recovery time needs to add iDRAC root of<br>trust time (around 50 seconds).|
|**UEFI Variable Access**|Provides varying degrees of securing UEFI variables. When set to**Standard** (the<br>default), UEFI variables are accessible in the operating system per the UEFI<br>specification. When set to**Controlled**, selected UEFI variables are protected in the<br>environment and new UEFI boot entries are forced to be at the end of the current boot<br>order.|
|**In-Band Manageability Interface**|When set to**Disabled**, this setting hides the Management Engine's (ME), HECI devices,<br>and the system's IPMI devices from the operating system. This prevents the operating<br>system from changing the ME power capping settings, and blocks access to all in-<br>band management tools. All management should be managed through out-of-band. This<br>option is set to**Enabled** by default.<br>**NOTE:** BIOS update requires HECI devices to be operational and DUP updates<br>require IPMI interface to be operational. This setting needs to be set to Enabled to<br>avoid updating errors.|
|**SMM Security Mitigation**|Enables or disables the UEFI SMM security mitigation protections. It is set to**Disabled**<br>by default.|
|**Secure Boot**|Enables Secure Boot, where the BIOS authenticates each pre-boot image by using the<br>certificates in the Secure Boot Policy. Secure Boot is set to**Disabled** by default.|
|**Secure Boot Policy**|When Secure Boot policy is set to**Standard**, the BIOS uses the system manufacturer’s<br>key and certificates to authenticate pre-boot images. When Secure Boot policy is set to<br>**Custom**, the BIOS uses the user-defined key and certificates. Secure Boot policy is set<br>to**Standard** by default.|
|**Secure Boot Mode**|Configures how the BIOS uses the Secure Boot Policy Objects (PK, KEK, db, dbx).|
|**Secure Boot Mode**|If the current mode is set to**Deployed Mode**, the available options are**User Mode** and<br>**Deployed Mode**. If the current mode is set to**User Mode**, the available options are<br>**User Mode**, **Audit Mode**, and**Deployed Mode**.|
|**Secure Boot Mode**|Below are the details of different boot modes available in the**Secure Boot Mode**<br>option.<br>**User Mode**<br>In**User Mode**, PK must be installed, and BIOS performs<br>signature verification on programmatic attempts to update<br>policy objects. The BIOS allows unauthenticated programmatic<br>transitions between modes.<br>**Audit mode**<br>In**Audit Mode**, PK is not present. BIOS does not authenticate<br>programmatic update to the policy objects and transitions<br>between modes. The BIOS performs a signature verification<br>on pre-boot images and logs the results in the image Execution<br>Information Table, but executes the images whether they pass<br>or fail verification.**Audit Mode** is useful for programmatic<br>determination of a working set of policy objects.<br>**Deployed Mode**<br>**Deployed Mode** is the most secure mode. In**Deployed**<br>**Mode**, PK must be installed and the BIOS performs signature<br>verification on programmatic attempts to update policy<br>objects.**Deployed Mode** restricts the programmatic mode<br>transitions.|
|**Secure Boot Policy Summary**|Specifies the list of certificates and hashes that secure boot uses to authenticate<br>images.|

**Table 81. System Security details (continued)**

|Option|Description|
|---|---|
|**Secure Boot Custom Policy**<br>**Settings**|Configures the Secure Boot Custom Policy. To enable this option, set the Secure Boot<br>Policy to**Custom** option.|
|**Intel Trust Domain**<br>**Extension(TDX)**|**Intel Trust Domain Extension (TDX)** is a hardware-based trusted execution<br>environment. It is designed to protect sensitive data and applications in Trust<br>Domain(TD) or Virtual Machine(VM) from unauthorized access.**Memory Encryption**<br>must be set to**Multiple Keys** for TDX to be enabled. TDX is set to**Disabled** by default.<br>**NOTE:** To enable the TDX option, processor must be TDX capable, memory<br>population must be compatible as SGX setting (minimum x8 identical DIMM1 to<br>DIMM8 per CPU socket, not support on persistent memory configuration)|
|**TME-MT/TDX Key Spilt to non-**<br>**zero value**|When the TME-MT/TDX Key Spilt to non-zero value is set to**1**, **2**, **3**, **4**, **5**, or**6**, it<br>designates the number of bits for TDX usage, while the rest will be used by TME-MT. It<br>is set to**1** by default.|
|**TDX Secure Arbitration Mode**<br>**Loader(SEAM)**|This SW module runs in a new CPU Secure Arbitration Mode (SEAM) as peer virtual<br>machine manager (VMM). This SEAM module supports TD entry and exit using the<br>existing virtualization infrastructure. It is set to**Disabled** by default.|
|**Intel(R) In-Field Scan**|The Intel(R) In-field Scan feature allows software to scan processor cores for latent<br>faults. The scan can be performed in the field after the server is deployed. When<br>**Enabled**, the BIOS configures all processors to respond to software scan requests.<br>When this setting is**Disabled**, the processors will not respond to software scan<br>requests. It is set to**Disabled** by default.|



##### Creating a system and setup password

**Prerequisites**

Ensure that the password jumper is enabled. The password jumper enables or disables the system password and setup password
features. For more information, see the System board jumper settings section.

**NOTE:** If the password jumper setting is disabled, the existing system password and setup password are deleted and you
need not provide the system password to boot the system.

**Steps**

1. To enter System Setup, press F2 immediately after turning on or rebooting your system.
2. On the **System Setup Main Menu** screen, click **System BIOS** - **System Security** .
3. On the **System Security** screen, verify that **Password Status** is set to **Unlocked** .
4. In the **System Password** field, type your system password, and press Enter or Tab.

Use the following guidelines to assign the system password:

  - A password can have up to 32 characters.

A message prompts you to reenter the system password.

5. Reenter the system password, and click **OK** .
6. In the **Setup Password** field, type your setup password and press Enter or Tab.

A message prompts you to reenter the setup password.

7. Reenter the setup password, and click **OK** .
8. Press Esc to return to the System BIOS screen. Press Esc again.

A message prompts you to save the changes.

**NOTE:** Password protection does not take effect until the system reboots.



##### Using your system password to secure your system

**About this task**

If you have assigned a setup password, the system accepts your setup password as an alternate system password.

**Steps**

1. Turn on or reboot your system.
2. Type the system password and press Enter.
**Next steps**

When **Password Status** is set to **Locked**, type the system password and press Enter when prompted at reboot.

**NOTE:** If an incorrect system password is typed, the system displays a message and prompts you to reenter your password.
You have three attempts to type the correct password. After the third unsuccessful attempt, the system displays an error
message that the system has stopped functioning and must be turned off. Even after you turn off and restart the system,
the error message is displayed until the correct password is entered.



##### Deleting or changing system and setup password

**Prerequisites**

**NOTE:** You cannot delete or change an existing system or setup password if the **Password Status** is set to **Locked** .

**Steps**

1. To enter System Setup, press F2 immediately after turning on or restarting your system.
2. On the **System Setup Main Menu** screen, click **System BIOS** - **System Security** .
3. On the **System Security** screen, ensure that **Password Status** is set to **Unlocked** .
4. In the **System Password** field, alter or delete the existing system password, and then press Enter or Tab.
5. In the **Setup Password** field, alter or delete the existing setup password, and then press Enter or Tab.

If you change the system and setup password, a message prompts you to reenter the new password. If you delete the
system and setup password, a message prompts you to confirm the deletion.

6. Press Esc to return to the **System BIOS** screen. Press Esc again, and a message prompts you to save the changes.
7. Select **Setup Password**, change, or delete the existing setup password and press Enter or Tab.
**NOTE:** If you change the system password or setup password, a message prompts you to reenter the new password. If
you delete the system password or setup password, a message prompts you to confirm the deletion.



##### Operating with setup password enabled

If **Setup Password** is set to **Enabled**, type the correct setup password before modifying the system setup options.

If you do not type the correct password in three attempts, the system displays the following message:

```
 Invalid Password! Number of unsuccessful password attempts: <x> System Halted! Must
 power down.

```

Even after you power off and restart the system, the error message is displayed until the correct password is typed. The
following options are exceptions:

- If **System Password** is not set to **Enabled** and is not locked through the **Password Status** option, you can assign a
system password. For more information, see the System Security Settings screen section.

- You cannot disable or change an existing system password.
**NOTE:** You can use the password status option with the setup password option to protect the system password from
unauthorized changes.



#### Redundant OS Control

To view the **Redundant OS Control** screen, power on the system, press F2, and click **System Setup Main Menu** - **System**
**BIOS** - **Redundant OS Control** .

**Table 82. Redundant OS Control details**

|Option|Description|
|---|---|
|**Redundant OS Location**|Enables you to select a backup disk from the following devices:<br>- **None**<br>- **SATA Ports in AHCI mode**<br>- **BOSS PCIe Cards (Internal M.2 Drives)**<br>- **Internal USB**<br>**NOTE:** RAID configurations and NVMe cards are not included, as BIOS does not<br>have the ability to distinguish between individual drives in those configurations.<br>- **Internal SD card**|
|**Redundant OS State**|**NOTE:** This option is disabled if**Redundant OS Location** is set to**None**.<br>When set to**Visible**, the backup disk is visible to the boot list and OS. When set to<br>**Hidden**, the backup disk is disabled and is not visible to the boot list and OS. This option<br>is set to**Visible** by default.<br>**NOTE:** BIOS disables the device in hardware, so it is not accessed by the OS.|
|**Redundant OS Boot**|**NOTE:** This option is disabled if**Redundant OS Location** is set to**None** or if<br>**Redundant OS State** is set to**Hidden**.<br>When set to**Enabled**, BIOS boots to the device specified in**Redundant OS Location**. <br>When set to**Disabled**, BIOS preserves the current boot list settings. This option is set to<br>**Disabled** by default.|
|**System Time**|Enables you to set the time on the system.|
|**System Date**|Enables you to set the date on the system.|
|**Time Zone**|Enables you to select required Time Zone.|
|**Daylight Savings Time**|Enables or disables Daylight Savings Time. This option is set to**Disabled** by<br>default.|
|**Asset Tag**|Specifies the asset tag and enables you to modify it for security and tracking<br>purposes.|
|**Keyboard NumLock**|Enables you to set whether the system boots with the NumLock enabled or<br>disabled. This option is set to**On** by default.<br>**NOTE:** This option does not apply to 84-key keyboards.|
|**F1/F2 Prompt on Error**|Enables or disables the F1/F2 prompt on error. This option is set to**Enabled** by<br>default. The F1/F2 prompt also includes keyboard errors.|
|**Load Legacy Video Option ROM**|This option determines whether th system BIOS will load legacy video (INT 10h)<br>option ROM from the video controller. This option is set to**Disabled** by default.<br>**NOTE:** This option cannot be set to Enabled, when the Boot mode is UEFI<br>and Secure Boot is enabled.|
|**Dell Wyse P25/P45 BIOS Access**|Enables or disables the Dell Wyse P25/P45 BIOS Access. This option is set to<br>**Enabled** by default.|

**Table 83. Miscellaneous Settings details (continued)**

|Option|Description|
|---|---|
|**Power Cycle Request**|Enables or disables the Power Cycle Request. This option is set to**None** by<br>default.|



### iDRAC Settings

The iDRAC settings is an interface to set up and configure the iDRAC parameters by using UEFI. You can enable or disable
various iDRAC parameters by using the iDRAC settings.

**NOTE:** Accessing some of the features on the iDRAC settings needs the iDRAC Enterprise License upgrade.

For more information about using iDRAC, see _Dell Integrated Dell Remote Access Controller User's Guide_ [at iDRAC Manuals.](https://www.dell.com/idracmanuals)



### Device Settings

**Device Settings** enables you to configure device parameters such as storage controllers or network cards.



### Service Tag Settings

**Service Tag Settings** enables you to configure the System Service Tag.



## Dell Lifecycle Controller

Dell Lifecycle Controller (LC) provides advanced embedded systems management capabilities including system deployment,
configuration, update, maintenance, and diagnosis. LC is delivered as part of the iDRAC out-of-band solution and Dell system
embedded Unified Extensible Firmware Interface (UEFI) applications.



### Embedded system management

The Dell Lifecycle Controller provides advanced embedded system management throughout the lifecycle of the system. The Dell
Lifecycle Controller is started during the boot sequence and functions independently of the operating system.

**NOTE:** Certain platform configurations may not support the full set of features provided by the Dell Lifecycle Controller.

For more information about setting up the Dell Lifecycle Controller, configuring hardware and firmware, and deploying the
[operating system, see the Dell Lifecycle Controller documentation at iDRAC Manuals.](https://www.dell.com/idracmanuals)



## Boot Manager

The **Boot Manager** option enables you to select boot options and diagnostic utilities.

To enter **Boot Manager**, power on the system and press F11.

|Table 84. Boot Manager details|Col2|
|---|---|
|**Option**|**Description**|
|**Continue Normal Boot**|The system attempts to boot to devices starting with the first item in the boot<br>order. If the boot attempt fails, the system continues with the next item in the<br>boot order until the boot is successful or no more boot options are found.|
|**One-shot UEFI Boot Menu**|Enables you to access boot menu, where you can select a one-time boot device<br>to boot from.|
|**Launch System Setup**|Enables you to access System Setup.|

**Table 84. Boot Manager details (continued)**

|Option|Description|
|---|---|
|**Launch Lifecycle Controller**|Exits the Boot Manager and invokes the Dell Lifecycle Controller program.|
|**System Utilities**|Enables you to launch System Utilities menu such as Launch Diagnostics, BIOS<br>update File Explorer, Reboot System.|



## PXE boot

You can use the Preboot Execution Environment (PXE) option to boot and configure the networked systems remotely.

To access the **PXE boot** option, boot the system and then press F12 during POST instead of using standard Boot Sequence
from BIOS Setup. It does not pull any menu or allows managing of network devices.



# 6 Minimum to POST and system management configuration validation

This section describes the minimum to POST system requirement and system management configuration validation of the Dell
system.

**Topics:**

- Minimum configuration to POST
- Configuration validation



## Minimum configuration to POST

The components listed below are the minimum configuration to POST:

- One processor in processor socket 1
- One memory modules (DIMM) in slot A1
- One power supply unit
- System board + RIO card



## Configuration validation

The new generation of Dell systems have added interconnect flexibility and advanced iDRAC management features to collect
precise system configuration information and report configuration errors.

When the system is powered on, information about installed cables, risers, backplanes, power supplies, floating card (fPERC,
adapter PERC, BOSS), and processor is obtained from the CPLD and backplane memory maps are analyzed. This information
forms a unique configuration, which is compared with one of the qualified configurations that are stored in a table that is
maintained by iDRAC.

One or more sensors are assigned to each of the configuration elements. During POST, any configuration validation error is
logged in the System Event Log (SEL)/LifeCycle (LC) log. The reported events are categorized in the configuration validation
error table.

**Table 85. Configuration validation error**

|Error|Description|Possible cause and<br>recommendations|Example|
|---|---|---|---|
|Config Error|A configuration element within the<br>closest match contains something that is<br>unexpected and does not match any Dell<br>qualified configuration.|Wrong configuration|Config Error: Backplane cable<br>CTRS_SRC_SA1 and BP-DST_SA1|
|Config Error|A configuration element within the<br>closest match contains something that is<br>unexpected and does not match any Dell<br>qualified configuration.|The element reported<br>in HWC8010 errors are<br>assembled incorrectly.<br>Verify element (cable,<br>risers, etc) placement in<br>the system.|Config Error : SL Cable<br>PLANAR_SL7 and CTRL_DST_PA1|
|Config<br>Missing|iDRAC found a configuration element<br>missing within the closest match<br>detected.|Missing or damaged cable,<br>device, or part|Config Missing: Float card front<br>PERC/HBAadapter PERC/HBA|
|Config<br>Missing|iDRAC found a configuration element<br>missing within the closest match<br>detected.|Missing element or cable<br>is reported in HWC8010<br>error logs. Install the|Config Missing : SL cable<br>PLANAR_SL8 and CTRL_DST_PA1|
|||missing element (cable,<br>risers, etc).||
|Comm Error|A configuration element is not responding<br>to iDRAC using the management interface<br>while running an inventory check.|System management<br>sideband communication|Comm Error: Backplane 2|
|Comm Error|A configuration element is not responding<br>to iDRAC using the management interface<br>while running an inventory check.|Unplug AC Power, reseat<br>the element and replace<br>the element if the problem<br>persists.|Unplug AC Power, reseat<br>the element and replace<br>the element if the problem<br>persists.|



### Error messages

This section describes the error messages that are displayed on the screen during POST or captured in the system event log
(SEL)/LifeCycle (LC) log.

**Table 86. Error message HWC8010**

|Error code|HWC8010|
|---|---|
|Message|The System Configuration Check operation that is resulted in the following issue involving the indicated<br>component type|
|Arguments|Riser, floating card (fPERC, adapter PERC, BOSS), backplane, processor, cable, or other components|
|Detailed Description|The issue that is identified in the message is observed in the System Configuration Check operation.|
|Recommended<br>Response Action|Do the following and retry the operation:<br>1.<br>Disconnect the input power.<br>2. Check for proper cable connection and component placement. If the issue persists, contact the<br>service provider.|
|Category|System Health (HWC = Hardware Config)|
|Severity|Critical|
|Trap/EventID|2329|

**Table 87. Error message HWC8011**

|Error code|HWC8011|
|---|---|
|Message|The System Configuration Check operation that is resulted in multiple issues involving the indicated<br>component type|
|Arguments|Riser, floating card (fPERC, adapter PERC, BOSS), backplane, processor, cable, or other components|
|Detailed Description|Multiple issues are observed in the System Configuration Check operation.|
|Recommended<br>Response Action|Do the following and retry the operation:<br>1.<br>Disconnect the input power.<br>2. Check for proper cable connection and component placement. If the issue persists, contact the<br>service provider.|
|Category|System Health (HWC = Hardware Config)|
|Severity|Critical|

**Minimum to POST and system management configuration validation** **93**



# 7 Disassembly and reassembly

**Topics:**

- Safety instructions
- Before working inside your system
- After working inside your system
- Recommended tools
- Optional front bezel
- System cover
- Drive backplane cover
- Air shrouds
- Cooling fans
- Drives
- Rear drive module
- Drive backplane
- Side wall brackets
- Cable routings
- PERC module
- EDSFF E3.S backplane module
- System memory
- Processor and heat sink module
- Expansion cards and expansion card risers
- Data processing unit (DPU)
- Optional serial COM port
- Optional VGA port for Direct Liquid Cooling module
- M.2 SSD module
- Optional BOSS-N1 module
- System battery
- Optional internal USB card
- Intrusion switch
- Optional OCP NIC card
- Power supply unit
- Trusted Platform Module
- System board
- LOM card, MIC card and rear I/O board
- Control panel



## Safety instructions

**NOTE:** Whenever you need to lift the system, get others to assist you. To avoid injury, do not attempt to lift the system by
yourself.

**CAUTION: Ensure that two or more people lift the system horizontally from the box and place it on a flat**
**surface, rack lift, or into the rails.**

**WARNING: Opening or removing the system cover while the system is powered on may expose you to a risk of**
**electric shock.**

**WARNING: Do not operate the system without the cover for a duration exceeding five minutes. Operating the**
**system without the system cover can result in component damage.**

**CAUTION: Many repairs may only be done by a certified service technician. You should only perform**
**troubleshooting and simple repairs as authorized in your product documentation, or as directed by the online or**
**telephone service and support team. Damage due to servicing that is not authorized by Dell is not covered by**
**your warranty. Read and follow the safety instructions that are shipped with your product.**

**NOTE:** It is recommended that you always use an antistatic mat and antistatic strap while working on components inside
the system.

**CAUTION: To ensure proper operation and cooling, all system bays and fans must always be populated with a**
**component or a blank.**

**NOTE:** While replacing the hot swappable PSU, after next server boot, the new PSU automatically updates to the same
firmware and configuration of the replaced one. For updating to the latest firmware and changing the configuration, see the
_Lifecycle Controller User's Guide_ [at iDRAC Manuals.](https://www.dell.com/idracmanuals)

**NOTE:** While replacing faulty storage controller, FC, or NIC card with the same type of card, after you power on the
system, the new card automatically updates to the same firmware and configuration of the faulty one. For updating to the
latest firmware and changing the configuration, see the _Lifecycle Controller User's Guide_ [at iDRAC Manuals.](https://www.dell.com/idracmanuals)

**CAUTION: Do not install GPUs, network cards, or other PCIe devices on your system that are not validated**
**and tested by Dell. Damage caused by unauthorized and invalidated hardware installation will null and void the**
**system warranty.**

**NOTE:** Only use certified Optical Fiber Transceiver Class I Laser Products.



## Before working inside your system

**Prerequisites**

Follow the safety guidelines listed in the Safety instructions.

**Steps**

1. Power off the system and all attached peripherals.
2. Disconnect the system from the electrical outlet and disconnect the peripherals.
3. If applicable, remove the system from the rack.

For more information, see the _Rail Installation Guide_ [relevant to your rail solutions at PowerEdge Manuals.](https://www.dell.com/poweredgemanuals)

4. Remove the system cover.
**NOTE:**

While removing the hot-swappable components from the front or rear of the system, do not remove the system cover.



## After working inside your system

**Prerequisites**

Follow the safety guidelines listed in Safety instructions.

**Steps**

1. Replace the system cover.
2. If applicable, install the system into the rack.

For more information, see the _Rail Installation Guide_ [relevant to your system at PowerEdge Manuals.](https://www.dell.com/poweredgemanuals)

3. Reconnect the peripherals and connect the system to the electrical outlet, and then power on the system.



## Recommended tools

You may need some or all the following tools to perform the removal and installation procedures:

- Key to the bezel lock. The key is required only if your system includes a bezel.
- Phillips 1 screwdriver
- Phillips 2 screwdriver
- Torx T30 screwdriver
- 5 mm hexadecimal nut screwdriver
- Plastic scribe
- 1/4-inch flat blade screwdriver
- Wrist grounding strap that is connected to the ground
- ESD mat
- Needle-nose pliers

You need the following tools to assemble the cables for a DC power supply unit:

- AMP 90871-1 hand-crimping tool or equivalent
- Tyco Electronics 58433-3 or equivalent
- Wire-stripper pliers to remove insulation from size 10 AWG solid or stranded, insulated copper wire.
**NOTE:** Use alpha wire part number 3080 or equivalent (65/30 stranding).

**NOTE:** [For information about DC PSU cabling instructions, go to PowerEdge Manuals >](https://www.dell.com/poweredgemanuals) **Rack Servers**  - PowerEdge R760

  - **Select This Product**   - **Documentation**   - **Manuals and Documents**   - _Cabling instructions for – 48 – 60 V DC power_
_supply_ .



## Optional front bezel

**NOTE:** LCD panel is optional on the front bezel. If the front bezel has an LCD panel, see LCD panel section.



### Removing the front bezel

The procedure to remove the front bezel with and without the LCD panel is the same.

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Keep the bezel key handy.
**NOTE:** The bezel key is part of the LCD bezel package.

**Steps**

1. Unlock the bezel.
2. Press the release button, and disengage the left end of the bezel.
3. Unhook the right end, and remove the bezel.
**Figure 37. Removing the front bezel with the LCD panel**

**Next steps**

Replace front bezel.



### Installing the front bezel

The procedure to install the front bezel with and without the LCD panel is the same.

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Locate and remove the bezel key.
**NOTE:** The bezel key is part of the LCD bezel package.

**Steps**

1. Align and insert the tabs on the bezel into the slots on the system.
2. Press the bezel until the release button clicks in place.
3. Lock the bezel.
**Figure 38. Installing the front bezel with the LCD panel**



## System cover





### Removing the system cover

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Power off the system, and any attached peripherals.
3. Disconnect the system from the electrical outlet and peripherals.
**Steps**

1. Using a 1/4-inch flat head or a Phillips #2 screwdriver, rotate the lock counterclockwise to the unlock position.
2. Lift the release latch until the system cover slides back.
3. Lift the cover from the system.



### Figure 39. Removing the system cover

**Next steps**

1. Replace the system cover.



### Installing the system cover

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Ensure that all internal cables are connected and routed properly, and no tools or extra parts are left inside the system.
**Steps**

1. Align the tabs on the system cover with the guide slots on the system and slide the system cover.
2. Close the system cover release latch.
3. Using a 1/4-inch flat head or Phillips #2 screwdriver, rotate the lock clockwise to the lock position.



### Figure 40. Installing the system cover

**Next steps**

1. Follow the procedure listed in After working inside your system.



## Drive backplane cover





### Removing the drive backplane cover

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
**Steps**

1. Slide the drive backplane cover in the direction of the arrows marked on the drive backplane cover.
2. Lift the drive backplane cover from the system.



### Figure 41. Removing the drive backplane cover

**Next steps**

1. Replace the drive backplane cover.



### Installing the drive backplane cover

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
**Steps**

1. Align the drive backplane cover with the guide slots on the system.
2. Slide the drive backplane cover to the front of the system until the drive backplane cover fits into place.



### Figure 42. Installing the drive backplane cover

**Next steps**

1. Follow the procedure listed in After working inside your system.



## Air shrouds





### Removing the air shroud

**Prerequisites**

**CAUTION: Never operate your system with the air shroud removed. The system may get overheated quickly,**
**resulting in shutdown of the system and loss of data.**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
**Steps**

Hold the edges of the air shroud, and lift the air shroud out of the system.



### Figure 43. Removing the air shroud

**Next steps**

1. Replace the air shroud.



### Installing the air shroud

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
**Steps**

1. Tilt the air shroud at an angle and align the air shroud tabs with the system slots.
**NOTE:** Ensure that the air shroud tabs are below the surface of the cooling cage fan assembly.

2. Lower the air shroud into the system until it is firmly seated.



### Figure 44. Installing the air shroud

**Next steps**

1. Follow the procedure listed in After working inside your system.



### Removing the GPU air shroud top cover

**Prerequisites**

**CAUTION: Never operate your system with the air shroud removed. The system may get overheated quickly,**
**resulting in shutdown of the system and loss of data.**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
**Steps**

Press the tabs on either sides of the top cover, and lift the top cover out of the GPU air shroud.



### Figure 45. Removing the GPU air shroud top cover

**Next steps**

1. Replace the GPU air shroud top cover.



### Installing the GPU air shroud top cover

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
**Steps**

1. Align the tabs on the top cover with the slots on the GPU air shroud.
2. Press the top cover into the GPU air shroud until it is firmly seated.



### Figure 46. Installing the GPU air shroud top cover

**Next steps**

1. Follow the procedure listed in After working inside your system.



### Removing the GPU air shroud filler

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
**CAUTION: Never operate your system with the air shroud removed. The system may get overheated quickly,**
**resulting in shutdown of the system and loss of data.**

3. Remove the GPU air shroud top cover.
**NOTE:** The GPU air shroud filler must be removed to avoid interference with full length double-width GPU card installation.

**Steps**

Hold and lift the filler from the GPU air shroud.



### Figure 47. Removing the GPU air shroud filler

**Next steps**

1. Replace the GPU air shroud filler.



### Installing the GPU air shroud filler

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the GPU air shroud top cover.
4. If required, remove the full length expansion card risers.
**NOTE:** The GPU air shroud filler must be installed, if single-width GPU card or empty riser is used.

**Steps**

Align and install the filler into the slots on the GPU air shroud.



### Figure 48. Installing the GPU air shroud filler

**Next steps**

1. Install the GPU air shroud top cover.
2. Follow the procedure listed in After working inside your system.



### Removing the GPU air shroud

**Prerequisites**

**CAUTION: Never operate your system with the air shroud removed. The system may get overheated quickly,**
**resulting in shutdown of the system and loss of data.**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the GPU air shroud top cover.
4. Remove the full length expansion card risers.
**Steps**

Hold the edges of the GPU air shroud, and lift the air shroud out of the system.



### Figure 49. Removing the GPU air shroud

**Next steps**

1. Replace the GPU air shroud.



### Installing the GPU air shroud

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the GPU air shroud top cover.
4. Remove the full length expansion card risers.
**Steps**

1. Align the tabs on the GPU air shroud with the slots on the system.
2. Lower the GPU air shroud into the system until it is firmly seated.



### Figure 50. Installing the GPU air shroud

**Next steps**

1. Install the full length expansion risers.
2. Install the GPU air shroud top cover.
3. Follow the procedure listed in After working inside your system.



### Removing the 2 x 2.5-inch rear drive module air shroud

**Prerequisites**

**CAUTION: Never operate your system with the air shroud removed. The system may get overheated quickly,**
**resulting in shutdown of the system and loss of data.**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
**Steps**

Press the orange release tabs and lift the air shroud out of the rear drive module.



### Figure 51. Removing the 2 x 2.5-inch rear drive module air shroud

**Next steps**

1. Replace the 2 x 2.5-inch rear drive module air shroud.



### Installing the 2 x 2.5-inch rear drive module air shroud

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
**Steps**

1. Align the tabs on the air shroud with the slots on the rear drive module.
2. Lower the air shroud into the rear drive module until it is firmly seated.



### Figure 52. Installing the 2 x 2.5-inch rear drive module air shroud

**Next steps**

1. Follow the procedure listed in After working inside your system.



### Removing the 4 x 2.5-inch rear drive module air shroud

**Prerequisites**

**CAUTION: Never operate your system with the air shroud removed. The system may get overheated quickly,**
**resulting in shutdown of the system and loss of data.**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
**Steps**

Press the orange release tabs and lift the air shroud out of the rear drive module.



### Figure 53. Removing the 4 x 2.5-inch rear drive module air shroud

**Next steps**

1. Replace the 4 x 2.5-inch rear drive module air shroud.



### Installing the 4 x 2.5-inch rear drive module air shroud

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
**Steps**

1. Align the tabs on the air shroud with the slots on the rear drive module.
2. Lower the air shroud into the rear drive module until it is firmly seated.



### Figure 54. Installing the 4 x 2.5-inch rear drive module air shroud

**Next steps**

1. Follow the procedure listed in After working inside your system.



### Removing the EDSFF E3.S rear drive module air shroud

**Prerequisites**

**CAUTION: Never operate your system with the air shroud removed. The system may get overheated quickly,**
**resulting in shutdown of the system and loss of data.**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
**Steps**

Press the release tabs and lift the air shroud out of the rear drive module.



### Figure 55. Removing the EDSFF E3.S rear drive module air shroud

**Next steps**

1. Replace the EDSFF E3.S rear drive module air shroud.



### Installing the EDSFF E3.S rear drive module air shroud

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
**Steps**

1. Align the tabs on the air shroud with the slots on the rear drive module.
2. Lower the air shroud into the rear drive module until it is firmly seated.



### Figure 56. Installing the EDSFF E3.S rear drive module air shroud

**Next steps**

1. Follow the procedure listed in After working inside your system.



## Cooling fans





### Removing the cooling fan cage assembly

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. If required, remove the air shroud or remove the GPU air shroud.
**Steps**

1. Lift the blue release levers to unlock the cooling fan cage assembly from the system.
2. Hold the release levers, and lift the cooling fan cage assembly away from the system.



### Figure 57. Removing the cooling fan cage assembly

**Next steps**

1. Replace the cooling fan cage assembly.



### Installing the cooling fan cage assembly

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
**CAUTION: Ensure that the cables inside the system are correctly installed and retained by the cable**
**retention bracket before installing the cooling fan cage assembly. Incorrectly installed cables may get**
**damaged.**

2. Follow the procedure listed in Before working inside your system.
3. If installed, remove the air shroud or remove the GPU air shroud.
**Steps**

1. Holding the blue release lever of the cooling fan cage, align the guide rails with the guides on the system.
2. Lower the cooling fan cage assembly into the system until seated firmly.
3. Lower the blue release lever and press to lock the cooling fan cage assembly into the system.



### Figure 58. Installing the cooling fan cage assembly

**Next steps**

1. If removed, install the air shroud or install the GPU air shroud.
2. Follow the procedure listed in After working inside your system.



### Removing a cooling fan

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
**NOTE:** The procedure for removing a standard (STD), high-performance silver grade (HPR SLVR), or high-performance
gold grade (HPR GOLD) fan is the same.

**Steps**

Press the orange release tab and lift the cooling fan to disconnect the fan from the connector on the system board.



### Figure 59. Removing a cooling fan

**Next steps**

1. Replace a cooling fan.



### Installing a cooling fan

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
**NOTE:** The procedure to installing a standard (STD), high-performance silver grade (HPR SLVR), or high-performance gold
grade (HPR GOLD) fan is the same.

**Steps**

Align and lower the cooling fan into the cooling fan assembly until the fan clicks into place.



### Figure 60. Installing a cooling fan

**Next steps**

1. Follow the procedure listed in After working inside your system.



### Removing a 2 x 2.5-inch rear drive module cooling fan

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the 2 x 2.5-inch rear drive module air shroud.
**Steps**

Press the orange release tab and lift the rear drive module cooling fan to disconnect from the connector on the rear drive
module.



### Figure 61. Removing a 2 x 2.5-inch rear drive module cooling fan

**Next steps**

1. Replace 2 x 2.5-inch rear drive module cooling fan.



### Installing a 2 x 2.5-inch rear drive module cooling fan

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the 2 x 2.5-inch rear drive module air shroud.
**Steps**

Align and lower the rear drive module cooling fan into the rear drive module until the fan clicks into place.



### Figure 62. Installing a 2 x 2.5-inch rear drive module cooling fan

**Next steps**

1. Install the 2 x 2.5-inch rear drive module air shroud.
2. Follow the procedure listed in After working inside your system.



### Removing a 4 x 2.5-inch rear drive module cooling fan

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the 4 x 2.5-inch rear drive module air shroud.
**Steps**

Press the orange release tab and lift the rear drive module cooling fan to disconnect from the connector on the rear drive
module.



### Figure 63. Removing a 4 x 2.5-inch rear drive module cooling fan

**Next steps**

1. Replace 4 x 2.5-inch rear drive module cooling fan.



### Installing a 4 x 2.5-inch rear drive module cooling fan

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the 4 x 2.5-inch rear drive module air shroud.
**Steps**

Align and lower the rear drive module cooling fan into the rear drive module until the fan clicks into place.



### Figure 64. Installing a 4 x 2.5-inch rear drive module cooling fan

**Next steps**

1. Install the 4 x 2.5-inch rear drive module air shroud.
2. Follow the procedure listed in After working inside your system.



### Removing the EDSFF E3.S rear drive module cooling fan

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the EDSFF E3.S rear drive module air shroud.
**NOTE:** EDSFF E3.S rear drive cooling fan is a cold swap.

**Steps**

1. Disconnect the cooling fan cable.
2. Press the blue release tab and lift the rear drive module cooling fan from the rear drive module.



### Figure 65. Removing the EDSFF E3.S rear drive module cooling fan

**Next steps**

1. Replace EDSFF E3.S rear drive module cooling fan.



### Installing the EDSFF E3.S rear drive module cooling fan

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the EDSFF E3.S rear drive module air shroud
**NOTE:** EDSFF E3.S rear drive cooling fan is a cold swap.

.

**Steps**

1. Align and lower the rear drive module cooling fan into the rear drive module until the fan clicks into place.



### Figure 66. Installing the EDSFF E3.S rear drive module cooling fan

2. Connect the cooling fan cable.
**Next steps**

1. Install the EDSFF E3.S rear drive module air shroud.
2. Follow the procedure listed in After working inside your system.



## Drives





### Removing a drive blank

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. If installed, remove the front bezel.
**CAUTION: To maintain proper system cooling, drive blanks must be installed in all empty drive slots.**

**Steps**

Press the release button, and slide the drive blank out of the drive slot.



### Figure 67. Removing a drive blank

**Next steps**

1. Replace the drive blank.



### Installing a drive blank

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. If installed, remove the front bezel.
**Steps**

Slide the drive blank into the drive slot until the release button clicks into place.



### Figure 68. Installing a drive blank

**Next steps**

1. If removed, install the front bezel.



### Removing a drive carrier

**Prerequisites**

1. Follow the safety guidelines listed in Safety instructions.
2. Remove the front bezel.
3. Using the management software, prepare the drive for removal. If the drive is online, the green activity or fault indicator
flashes while the drive is turning off. When the drive indicators are off, the drive is ready for removal. For more information,
see the storage controller documentation.

**CAUTION: Before attempting to remove or install a drive while the system is running, see the documentation**
**for the storage controller card to ensure that the host adapter is configured correctly to support drive**
**removal and insertion.**

**CAUTION: To prevent data loss, ensure that your operating system supports drive installation. See the**
**documentation supplied with your operating system.**

**Steps**

1. Press the release button to open the drive carrier release handle.
2. Holding the drive carrier release handle, slide the drive carrier out of the drive slot.
**NOTE:** If you are not replacing the drive immediately, install a drive blank in the empty drive slot to maintain proper
system cooling.



### Figure 69. Removing a drive carrier

**Next steps**

Replace the drive or a drive blank.



### Installing the drive carrier

**Prerequisites**

**[CAUTION: Before removing or installing a drive while the system is running, see the Storage Controller Manuals](https://www.dell.com/support/home/en-in/products/data_center_infra_int/data_center_infra_storage_adapters)**
**documentation for the storage controller card to ensure that the host adapter is configured correctly to support**
**drive removal and insertion.**

**CAUTION: Combining SAS and SATA drives in the same RAID volume is not supported.**

**CAUTION: When installing a drive, ensure that the adjacent drives are fully installed. Inserting a drive carrier**
**and attempting to lock its handle next to a partially installed carrier can damage the partially installed carrier's**
**shield spring and make it unusable.**

**NOTE:** Ensure that the drive carrier's release handle is in the open position before inserting the carrier into the slot.

**CAUTION: To prevent data loss, ensure that your operating system supports hot-swap drive installation. See the**
**documentation supplied with your operating system.**

**CAUTION: When a replacement hot swappable drive is installed and the system is powered on, the drive**
**automatically begins to rebuild. Ensure that the replacement drive is blank or contains data that you wish to**
**overwrite. Any data on the replacement drive is immediately lost after the drive is installed.**

1. Follow the safety guidelines listed in Safety instructions.
2. Remove the front bezel.
3. Remove the drive carrier or remove the drive blank when you want to assemble the drive into the system.
**Steps**

1. Slide the drive carrier into the drive slot and push until the drive connects with the backplane.
2. Close the drive carrier release handle to lock the drive in place.
**Figure 70. Installing a drive carrier**

**Next steps**

Install the front bezel.



### Removing the drive from the drive carrier

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Remove the drive carrier.
**Steps**

1. Using a Phillips #1 screwdriver, remove the screws from the slide rails on the drive carrier.
**NOTE:** If the drive carrier has Torx screw, use Torx 6 (for 2.5-inch drive) or Torx 8 (for 3.5-inch drive) screwdriver to
remove the drive.

2. Lift the drive out of the drive carrier.



### Figure 71. Removing the drive from the drive carrier

**Next steps**

Install the drive into the drive carrier.



### Installing the drive into the drive carrier

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Remove the drive blank or Remove the drive carrier.
**Steps**

1. Insert the drive into the drive carrier with the drive connector facing towards the rear of the carrier.
2. Align the screw holes on the drive with the screws holes on the drive carrier.
3. Using a Phillips #1 screwdriver, secure the drive to the drive carrier with the screws.
**NOTE:** When installing a drive into the drive carrier, ensure that the screws are torqued to 4 in-lbs.

**NOTE:** If the drive carrier has Torx screw, use Torx 6 (for 2.5-inch drive) or Torx 8 (for 3.5-inch drive) screwdriver to
install the drive.



### Figure 72. Installing a drive into the drive carrier

**Next steps**

1. Install the drive carrier.



### Removing an EDSFF E3.S drive blank

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. If installed, remove the front bezel.
**CAUTION: To maintain proper system cooling, drive blanks must be installed in all empty drive slots.**

**Steps**

Lift the release button, and slide the drive blank out of the drive slot.



### Figure 73. Removing an EDSFF E3.S drive blank

**Next steps**

1. Replace the EDSFF E3.S drive blank.



### Installing an EDSFF E3.S drive blank

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. If installed, remove the front bezel.
**Steps**

Slide the drive blank into the drive slot until the release button clicks into place.



### Figure 74. Installing an EDSFF E3.S drive blank

**Next steps**

1. If removed, install the front bezel.



### Removing an EDSFF E3.S drive carrier

**Prerequisites**

1. Follow the safety guidelines listed in Safety instructions.
2. Remove the front bezel.
3. Using the management software, prepare the drive for removal. If the drive is online, the green activity or fault indicator
flashes while the drive is turning off. When the drive indicators are off, the drive is ready for removal. For more information,
see the storage controller documentation.

**CAUTION: Before attempting to remove or install a drive while the system is running, see the documentation**
**for the storage controller card to ensure that the host adapter is configured correctly to support drive**
**removal and insertion.**

**CAUTION: To prevent data loss, ensure that your operating system supports drive installation. See the**
**documentation supplied with your operating system.**

**Steps**

1. Lift the release button to open the drive carrier release handle.
2. Holding the drive carrier release handle, slide the drive carrier out of the drive slot.
**NOTE:** If you are not replacing the drive immediately, install an EDSFF E3.S drive blank in the empty drive slot to
maintain proper system cooling.



### Figure 75. Removing an EDSFF E3.S drive carrier

**Next steps**

Replace the EDSFF E3.S drive or an EDSFF E3.S drive blank.



### Installing an EDSFF E3.S drive carrier

**Prerequisites**

**[CAUTION: Before removing or installing a drive while the system is running, see the Storage Controller Manuals](https://www.dell.com/support/home/en-in/products/data_center_infra_int/data_center_infra_storage_adapters)**
**documentation for the storage controller card to ensure that the host adapter is configured correctly to support**
**drive removal and insertion.**

**CAUTION: Combining SAS and SATA drives in the same RAID volume is not supported.**

**CAUTION: When installing a drive, ensure that the adjacent drives are fully installed. Inserting a drive carrier**
**and attempting to lock its handle next to a partially installed carrier can damage the partially installed carrier's**
**shield spring and make it unusable.**

**NOTE:** Ensure that the drive carrier's release handle is in the open position before inserting the carrier into the slot.

**CAUTION: To prevent data loss, ensure that your operating system supports hot-swap drive installation. See the**
**documentation supplied with your operating system.**

**CAUTION: When a replacement hot swappable drive is installed and the system is powered on, the drive**
**automatically begins to rebuild. Ensure that the replacement drive is blank or contains data that you wish to**
**overwrite. Any data on the replacement drive is immediately lost after the drive is installed.**

1. Follow the safety guidelines listed in Safety instructions.
2. Remove the front bezel.
3. Remove the drive carrier or remove the drive blank when you want to assemble the drive into the system.
**Steps**

1. Slide the drive carrier into the drive slot and push until the drive connects with the backplane.
2. Close the drive carrier release handle to lock the drive in place.



### Figure 76. Installing an EDSFF E3.S drive carrier

**Next steps**

Install the front bezel.



### Removing an EDSFF E3.S drive from the drive carrier

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Remove the drive carrier.
**Steps**

1. Using a Torx 6 screwdriver, remove the screws from the slide rails on the drive carrier.
2. Lift the drive out of the drive carrier.



### Figure 77. Removing an EDSFF E3.S drive from the drive carrier

**Next steps**

Install an EDSFF E3.S drive into the drive carrier.



### Installing an EDSFF E3.S drive into the drive carrier

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Remove an EDSFF E3.S drive blank or Remove an EDSFF E3.S drive carrier.
**Steps**

1. Insert the drive into the drive carrier with the drive connector facing towards the rear of the carrier.
2. Align the screw holes on the drive with the screws holes on the drive carrier.
3. Using a Torx 6 screwdriver, secure the drive to the drive carrier with the screws.
**NOTE:** When installing a drive into the drive carrier, ensure that the screws are torqued to 4 in-lbs.



### Figure 78. Installing an EDSFF E3.S drive into the drive carrier

**Next steps**

1. Install the drive carrier.



## Rear drive module





### Removing the 2 x 2.5-inch rear drive module

**Prerequisites**

1. Follow the safety guidelines listed in Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. If required, remove the air shroud.
4. Remove the drives.
5. Disconnect the cables from the rear drive module.
**NOTE:** Refer to cable routing section for more information.

**Table 88. Rear drive bay slot numbers**

|Configurations with two rear drives|Bay0 slot numbers|Bay1 slot numbers|
|---|---|---|
|12 x 3.5-inch SAS/SATA + 2 x 2.5-inch SAS/<br>SATA|Slot 0 and 1|N/A|
|12 x 3.5-inch SAS/SATA + 2 x 2.5-inch NVMe|Slot 0 and 1|N/A|
|24 x 2.5-inch SAS/SATA + 2 x 2.5-inch SAS/<br>SATA|N/A|Slot 24 and 25|
|24 x 2.5-inch SAS/SATA + 2 x 2.5-inch NVMe|Slot 0 and 1|N/A|

**Steps**

1. Using a Phillips #2 screwdriver, loosen the captive screws that secure the rear drive module to the system.
2. Press the blue release tab and holding the edges lift the rear drive module away from the system.



### Figure 79. Removing the 2 x 2.5-inch rear drive module

**Next steps**

1. Replace the 2 x 2.5-inch rear drive module.



### Installing the 2 x 2.5-inch rear drive module

**Prerequisites**

1. Follow the safety guidelines listed in Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. If required, remove the air shroud.
4. Remove the drives.
5. Disconnect the cables from the rear drive module.
**NOTE:** Refer to cable routing section for more information.

**Steps**

1. Align the slot on the rear drive module with the guide on the system.
2. Lower and press the rear drive module on top of the riser until firmly seated.
3. Using a Phillips #2 screwdriver, tighten the captive screws that secure the rear drive module into the system.



### Figure 80. Installing the 2 x 2.5-inch rear drive module

**Next steps**

1. Connect and route all the cables to the rear drive module.
2. Install the drives.
3. If removed, install the air shroud.
4. Follow the procedure listed in After working inside your system.



### Removing the 4 x 2.5-inch rear drive module

**Prerequisites**

1. Follow the safety guidelines listed in Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. If required, remove the air shroud.
4. Remove the drives.
5.

**NOTE:** If BOSS-N1 module is installed, make sure to disconnect the BOSS-N1 power
cable and Signal cable before removing the 4 x 2.5-inch rear drive module.

6. Disconnect the cables from the rear drive module.
**NOTE:** See cable routing section for more information.

**Table 89. Rear drive bay slot numbers**

|Configurations with four rear drives|Bay0 slot numbers|Bay1 slot numbers|
|---|---|---|
|12 x 3.5-inch SAS/SATA + 4 x 2.5-inch SAS/<br>SATA|Slot 0, 1, 2 and 3|N/A|
|12 x 3.5-inch SAS/SATA + 4 x 2.5-inch NVMe|Slot 0, 1, 2 and 3|N/A|
|24 x 2.5-inch SAS/SATA + 4 x 2.5-inch SAS/<br>SATA|N/A|Slot 24, 25, 26 and 27|
|24 x 2.5-inch SAS/SATA + 4 x 2.5-inch NVMe|Slot 0, 1, 2 and 3|N/A|

**Steps**

1. Using a Phillips #2 screwdriver, loosen the captive screws that secure the rear drive module to the system.
2. Lift the rear drive module from the system.



### Figure 81. Removing the 4 x 2.5-inch rear drive module

**Next steps**

1. Replace the 4 x 2.5-inch rear drive module.



### Installing the 4 x 2.5-inch rear drive module

**Prerequisites**

1. Follow the safety guidelines listed in Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. If required, remove the air shroud.
4. Remove the drives.
5. If installed, remove the BOSS-N1 module.
6. Disconnect the cables from the rear drive module.
**NOTE:** Refer to cable routing section for more information.

**Steps**

1. Align and lower the rear drive module with the guide on the system.
2. Using a Phillips #2 screwdriver, tighten the captive screws that secure the rear drive module into the system.



### Figure 82. Installing the 4 x 2.5-inch rear drive module

**Next steps**

1. Connect and route all the cables to the rear drive module.
2. Install the drives.
3. If removed, install the BOSS-N1 module.
4. If removed, install the air shroud.
5. Follow the procedure listed in After working inside your system.



### Removing the EDSFF E3.S rear drive module

**Prerequisites**

1. Follow the safety guidelines listed in Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the cooling fan cage assembly.
4. Remove the air shroud.
5. Remove the expansion card riser 1.
6. Remove an EDSFF E3. S drive carrier.
7. Disconnect the rear drive cables from the system board.
**NOTE:** See cable routing section for more information.

**Table 90. Rear drive bay slot numbers**

|Configurations with four rear drives|Bay0 slot numbers|
|---|---|
|12 x 3.5-inch SAS/SATA + 4 x EDSFF E3.S NVMe|Slot 0, 1, 2 and 3|
|24 x 2.5-inch SAS/SATA + 4 x EDSFF E3.S NVMe|Slot 0, 1, 2 and 3|

**Steps**

1. Using a Phillips #2 screwdriver, loosen the captive screw that secures the rear drive module to the system.
2. Press the blue release tab and holding the edges lift the rear drive module away from the system.



### Figure 83. Removing the EDSFF E3.S rear drive module

**Next steps**

1. Replace the EDSFF E3.S rear drive module.



### Installing the EDSFF E3.S rear drive module

**Prerequisites**

1. Follow the safety guidelines listed in Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the cooling fan cage assembly.
4. Remove the air shroud.
5. Remove the expansion card riser 1.
6. Remove an EDSFF E3. S drive carrier.
7. Disconnect the rear drive cables from the system board.
**NOTE:** See cable routing section for more information.

**Steps**

1. Align the slot on the rear drive module with the guide on the system.
2. Lower and press the rear drive module on top of the riser until firmly seated.
3. Using a Phillips #2 screwdriver, tighten the captive screw that secures the rear drive module into the system.



### Figure 84. Installing the EDSFF E3.S rear drive module

**Next steps**

1. Connect and route all the cables from the rear drive module.
2. Install an EDSFF E3.S drive carrier.
3. Install the expansion card riser 1.
4. Install the air shroud.
5. Install the cooling fan cage assembly.
6. Follow the procedure listed in After working inside your system.



### Drive backplane

This is a service technician replaceable part only.

### Drive backplane

Depending on your system configuration, the drive backplanes that are supported are listed here:

**Table 91. Supported backplane options**

|System|Supported hard drives options|
|---|---|
|PowerEdge R760|2.5-inch (x 8) NVMe backplane|
|PowerEdge R760|2.5-inch (x 8) SAS, or SATA backplane|
|PowerEdge R760|3.5-inch (x 12) SAS or SATA backplane|
|PowerEdge R760|2.5-inch (x 24) SAS, SATA, or NVMe backplane|
|PowerEdge R760|2.5-inch (x 24) NVMe Gen5 Switched backplane|
|PowerEdge R760|EDSFF E3.S (x8) NVMe backplane|
||2.5-inch (x8) Universal backplane|

**Figure 85. 8 x 2.5-inch NVMe drive backplane**

1. BP_PWR_CTRL
2. BP_DST_SA1 (PERC to backplane)
3. BP_PWR_1 (backplane power cable to system board)
**Figure 86. 8 x 2.5-inch SAS/SATA drive backplane**

1. BP_PWR_CTRL 2. BP_DST_SA1 (PERC to backplane)
3. BP_DST_PA1 (PCIe/NVMe connector) 4. BP_ DST_PB1 (PCIe/NVMe connector)
5. BP_ DST_PA2 (PCIe/NVMe connector) 6. BP_PWR_1 (backplane power cable to system board)
7. BP_DST_PB2 (PCIe/NVMe connector)
**Figure 87. 12 x 3.5-inch drive backplane**

1. BP_DST_SB1
2. BP_DST_SA1
3. BP_PWR_1 (backplane power cable to system board)
**Figure 88. 24 x 2.5-inch drive backplane (front view)**

**Figure 89. 24 x 2.5-inch drive backplane (top view)**

1. BP_CTRL 2. BP_PWR_1 (backplane power cable to system board)
3. BP_DST_PA1 (PCIe/NVMe connector) 4. BP_PWR_2 (backplane power cable to system board)
5. BP_ DST_PB1 (PCIe/NVMe connector) 6. BP_PWR_CTRL
7. BP_ DST_PA2 (PCIe/NVMe connector) 8. BP_ DST_PB2 (PCIe/NVMe connector)
9. BP_DST_SB1 10. BP_SRC_SA2
11. BP_DST_SA1
**Figure 90. 24 x 2.5-inch NVMe passive backplane**

1. BP_DST_PB6 (PCIe/NVMe connector) 2. BP_DST_PA6 (PCIe/NVMe connector)
3. BP_DST_PB5 (PCIe/NVMe connector) 4. BP_PWR_2
5. BP_DST_PA5 (PCIe/NVMe connector) 6. BP_DST_PB4 (PCIe/NVMe connector)
7. BP_DST_PA4 (PCIe/NVMe connector) 8. BP_DST_PB3 (PCIe/NVMe connector)
9. BP_DST_PA3 (PCIe/NVMe connector) 10. BP_DST_PB2 (PCIe/NVMe connector)
11. BP_PWR_1 12. BP_DST_PA2 (PCIe/NVMe connector)
13. BP_DST_PB1 (PCIe/NVMe connector) 14. BP_DST_PA1 (PCIe/NVMe connector)
**Figure 91. 24 x 2.5-inch NVMe Gen5 Switched drive backplane (front view)**

**Figure 92. 24 x 2.5-inch NVMe Gen5 Switched drive backplane (top view)**

1. BP_PWR_2 (backplane power cable to system board) 2. BP_PWR_1 (backplane power cable to system board)
3. BP_ DST_ PA3 (PCIe/NVMe connector) 4. BP_ DST_PB1 (PCIe/NVMe connector)
5. BP_ DST_PA4 (PCIe/NVMe connector) 6. BP_ DST_PB2 (PCIe/NVMe connector)
7. BP_ DST_PA2 (PCIe/NVMe connector) 8. BP_DST_PA1 (PCIe/NVMe connector)
**Figure 93. EDSFF E3.S NVMe drive backplane**

1. BP_PWR_CTRL 2. BP_PWR_1 (backplane power cable to system board)
3. BP_PB2 (PCIe/NVMe connector) 4. BP_ PA2 (PCIe/NVMe connector)
5. BP_ PB1 (PCIe/NVMe connector) 6. BP_PA1 (PCIe/NVMe connector)
**Figure 94. 8 x 2.5-inch Universal drive backplane**

1. BP_PWR_CTRL 2. BP_DST_SA1 (PERC to backplane)
3. BP_DST_PA1 (PCIe/NVMe connector) 4. BP_ DST_PB1 (PCIe/NVMe connector)
5. BP_ DST_PA2 (PCIe/NVMe connector) 6. BP_PWR_1 (backplane power cable to system board)
7. BP_DST_PB2 (PCIe/NVMe connector)



### Removing the drive backplane

**Prerequisites**

**CAUTION: To prevent damage to the drives and backplane, remove the drives from the system before removing**
**the backplane.**

**CAUTION: Note the number of each drive and temporarily label them before you remove the drive so that you**
**can reinstall them in the same location.**

**NOTE:** The procedure to remove the backplane is similar for all backplane configurations.

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the drive backplane cover.
4. If installed, remove the air shroud or remove the GPU air shroud.
5. Remove the cooling fan cage assembly.
6. Remove the drives.
7. If required, remove the rear mounting front PERC module.
8. Observe and disconnect the drive backplane cables from the connector on the system board and backplane.
**NOTE:** See cable routing section for more information.

**Steps**

1. Press one or more release tabs to disengage the drive backplane from the hooks on the system.
2. Lift and pull the drive backplane out of the system.
**NOTE:** To avoid damaging the backplane, remove the disconnected backplane cables from the cable routing clips before
removing the backplane.



### Figure 95. Removing the drive backplane





### Figure 96. Removing the drive backplane

**Next steps**

1. Replace the drive backplane.



### Installing the drive backplane

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the drive backplane cover.
4. If installed, remove the air shroud or remove the GPU air shroud.
5. Remove the cooling fan cage assembly.
6. Remove the drives.
7. If required, remove the rear mounting front PERC module.
8. Observe and disconnect the drive backplane cables from the connector on the system board and backplane.
**NOTE:** See cable routing section for more information.

**NOTE:** To avoid damaging the backplane, remove the disconnected backplane cables from the cable routing clips.

**NOTE:** Route the cable properly when you replace it to prevent the cable from being pinched or crimped.

**Steps**

1. Align the slots on the drive backplane with the guides on the system.
2. Slide the drive backplane into the guides on the system and lower the backplane until one or more blue release tabs clicks
into place.



### Figure 97. Installing the drive backplane





### Figure 98. Installing the drive backplane

**Next steps**

1. Connect the drive backplane cables to the connectors on the system board and backplane.
2. Install the drives.
3. If required, install the rear mounting front PERC module.
4. Install the cooling fan cage assembly.
5. If removed, install the air shroud or install the GPU air shroud
6. Install the drive backplane cover.
7. Follow the procedure listed in After working inside your system.



## Side wall brackets





### Removing the side wall bracket

There are two side wall brackets on either side of the system. The procedure to remove is similar.

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. If required, remove the drive backplane cover.
4. If installed, remove the air shroud or remove the GPU air shroud.
5. Remove the cooling fan cage assembly.
**NOTE:** Ensure that you note the routing of the cables as you remove them from the system board. Route the cables
properly when you replace them to prevent the cables from being pinched or crimped.

**Steps**

1. Press the blue side tabs to release the side wall cable holder.
**NOTE:** Move the cables out of the side wall cable holder.

2. Press the center tab to release the bracket from the chassis, and lift it away from the system.



### Figure 99. Removing the side wall bracket

**Next steps**

1. Replace the side wall bracket.



### Installing the side wall bracket

There are two side wall brackets on either side of the system. The procedure to install is similar.

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. If required, remove the drive backplane cover.
4. If installed, remove the air shroud or remove the GPU air shroud.
5. Remove the cooling fan cage assembly.
**NOTE:** Ensure that you note the routing of the cables as you remove them from the system board. Route the cables
properly when you replace them to prevent the cables from being pinched or crimped.

**Steps**

1. Align the guide slots on the side wall bracket with the guides on the system and slide until the cover is seated firmly.
**NOTE:** Route the cables through the side wall cable holder.

2. Close the side wall cable holder until the holder clicks into place.



### Figure 100. Installing the side wall bracket

**Next steps**

1. Replace the cooling fan cage assembly.
2. If removed, install the air shroud or install the GPU air shroud.
3. If removed, install the drive backplane cover.
4. Follow the procedure listed in the After working inside your system.



## Cable routings

**Figure 101. Configuration 0: 12 x 3.5-inch SAS/SATA with APERC11/12 in Riser 2**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 92. 12 x 3.5-inch SAS/SATA with APERC11/12 in Riser 2**

|Order|From|To|
|---|---|---|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|CTRL_SRC_SA1_PA1 (adapter PERC controller<br>connector)|BP_DST_SA1 (backplane signal connector)|
|3|CTRL_SRC_SB1_PB1 (adapter PERC controller<br>connector)|BP_DST_SB1 (backplane signal connector)|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|CTRL_SRC_SB1_PB1 (adapter PERC controller<br>connector) and BP_DST_SB1 (backplane signal<br>connector)|BP_DST_SA1 (rear backplane signal connector) and<br>BP_DST_SB1 (rear backplane signal connector)|
|3|CTRL_SRC_SA1_PA1 (adapter PERC controller<br>connector)|BP_DST_SA1 (backplane signal connector)|
|4|SIG_PWR_0 (system board power connector)|BP_PWR_1 (rear backplane power connector)|

**Figure 103. Configuration 2: 12 x 3.5-inch SAS/SATA + 2 x 2.5-inch NVMe with APERC11/12 in Riser 2**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 94. 12 x 3.5-inch SAS/SATA + 2 x 2.5-inch NVMe with APERC11/12 in Riser 2**

|Order|From|To|
|---|---|---|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|CTRL_SRC_SB1_PB1 (adapter PERC controller<br>connector)|BP_DST_SB1 (backplane signal connector)|
|3|CTRL_SRC_SA1_PA1 (adapter PERC controller<br>connector)|BP_DST_SA1 (backplane signal connector)|
|4|SIG_PWR_0 (system board power connector)|BP_PWR_1 (rear backplane power connector)|
|5|SL11_CPU1_PB7 (signal connector on system board)|BP_DST_SA1 (rear backplane signal connector) and<br>BP_DST_SB1 (rear backplane signal connector)|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|CTRL_SRC_SB1_PB1 (adapter PERC controller<br>connector) and BP_DST_SB1 (backplane signal<br>connector)|BP_DST_SA1 (rear backplane signal connector) and<br>BP_DST_SB1 (rear backplane signal connector)|
|3|CTRL_SRC_SA1_PA1 (adapter PERC controller<br>connector)|BP_DST_SA1 (backplane signal connector)|
|4|SIG_PWR_0 (system board power connector)|BP_PWR_1 (rear backplane power connector)|

**Figure 105. Configuration 4: 12 x 3.5-inch SAS/SATA + 4 x 2.5-inch NVMe with APERC11/12 in Riser 2**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 96. 12 x 3.5-inch SAS/SATA + 4 x 2.5-inch NVMe with APERC11/12 in Riser 2**

|Order|From|To|
|---|---|---|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SIG_PWR_0 (system board power connector)|BP_PWR_1 (rear backplane power connector)|
|3|SL4_CPU1_PA2 (signal connector on system<br>board)|BP_DST_PA1 (rear backplane signal connector) and<br>BP_DST_PB1 (rear backplane signal connector)|
|4|SL11_CPU1_PB7 (signal connector on system<br>board)|BP_DST_PA2 (rear backplane signal connector) and<br>BP_DST_PB2 (rear backplane signal connector)|
|5|CTRL_SRC_SB1_PB1 (adapter PERC controller<br>connector)|BP_DST_SB1 (backplane signal connector)|
|6|CTRL_SRC_SA1_PA1 (adapter PERC controller<br>connector)|BP_DST_SA1 (backplane signal connector)|
|1|SL1_CPU2_PA1 (signal connector on system<br>board)|BP_DST_PA1 (backplane signal connector)|
|2|SL2_CPU2_PB1 (signal connector on system<br>board)|BP_DST_PB1 (backplane signal connector)|
|3|SL3_CPU1_PA2 (signal connector on system<br>board) and SL4_CPU1_PB2 (signal connector on<br>system board)|BP_DST_PA2 (backplane signal connector) and<br>BP_DST_PB2 (backplane signal connector)|
|4|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|

**Figure 107. Configuration 6: 8 x 2.5-inch NVMe RAID with fPERC (H755N)**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 98. 8 x 2.5-inch NVMe RAID with fPERC (H755N)**

|Order|From|To|
|---|---|---|
|1|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input<br>connector)|
|2|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power<br>connector)|
|3|CTRL_SRC_PB1 (fPERC controller connector)|BP_DST_PA2 (backplane signal<br>connector) and BP_DST_PB2<br>(backplane signal connector)|
|4|CTRL_SRC_PA1 (fPERC controller connector)|BP_DST_PA1 (backplane signal<br>connector) and BP_DST_PB1 (backplane<br>signal connector)|
|1|SL4_CPU1_PB2 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC input<br>connector)|
|2|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input<br>connector)|
|3|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power<br>connector)|
|4|CTRL_SRC_PB1 (fPERC controller connector)|BP_DST_PA2 (backplane signal<br>connector) and BP_DST_PB2<br>(backplane signal connector)|
|5|CTRL_SRC_PA1 (fPERC controller connector)|BP_DST_PA1 (backplane signal<br>connector) and BP_DST_PB1 (backplane<br>signal connector)|

**NOTE:** An 8 x 2.5-inch NVMe backplane with fPERC H965i should be assembled outside and inserted into the system,
along with all necessary cables.

**Figure 109. Configuration 8: 8 x 2.5-inch Universal (SAS/SATA/NVMe) with fPERC (HBA355i, H355, H755)**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 100. 8 x 2.5-inch Universal (SAS/SATA/NVMe) with fPERC (HBA355i, H355, H755)**

|Order|From|To|
|---|---|---|
|1|SL5_CPU2_PB3 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input<br>connector)|
|2|SL3_CPU1_PA2 (signal connector on system<br>board) and SL4_CPU1_PB2 (signal connector on<br>system board)|BP_DST_PA2 (backplane signal<br>connector) and BP_DST_PB2<br>(backplane signal connector)|
|3|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power<br>connector)|
|4|SL2_CPU2_PB1 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC input<br>connector)|
|5|SL1_CPU2_PA1 (signal connector on system board)|CTRL_DST_PA1 (fPERC input<br>connector)|
|1|SL1_CPU2_PA1 (signal connector on<br>system board)|CTRL_DST_PA1 (fPERC input connector)|
|2|SIG_PWR_1 (system board power<br>connector)|BP_PWR_1 (backplane power connector)|
|3|CTRL_SRC_PB1 (fPERC controller<br>connector)|BP_DST_PA2 (backplane signal connector) and<br>BP_DST_PB2 (backplane signal connector)|
|4|CTRL_SRC_PA1 (fPERC controller<br>connector)|BP_DST_PA1 (backplane signal connector) and<br>BP_DST_PB1 (backplane signal connector)|
|5|SL3_CPU1_PA2 (signal connector on<br>system board)|CTRL_DST_PA1 (fPERC input connector)|
|6|SIG_PWR_2 (system board power<br>connector)|BP_PWR_1 (backplane power connector)|
|7|CTRL_SRC_PB1 (fPERC controller<br>connector)|BP_DST_PA2 (backplane signal connector) and<br>BP_DST_PB2 (backplane signal connector)|
|8|CTRL_SRC_PA1 (fPERC controller<br>connector)|BP_DST_PA1 (backplane signal connector) and<br>BP_DST_PB1 (backplane signal connector)|

**NOTE:** An 8 x 2.5-inch NVMe backplanes with fPERC H755 should be assembled outside and inserted into the system,
along with all necessary cables.

**Figure 111. Configuration 10: 16 x 2.5-inch NVMe**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 102. 16 x 2.5-inch NVMe**

|Order|From|To|
|---|---|---|
|1|SL2_CPU2_PA1 (signal connector on system<br>board)|BP_DST_PA1 (backplane signal connector)|
|2|SL1_CPU2_PB1 (signal connector on system<br>board)|BP_DST_PB1 (backplane signal connector)|
|3|SL5_CPU2_PB3 (signal connector on system<br>board)|BP_DST_PA2 (backplane signal connector)|
|4|SL6_CPU2_PA3 (signal connector on system<br>board)|BP_DST_PB2 (backplane signal connector)|
|5|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|6|SL3_CPU1_PA2 (signal connector on system<br>board)|BP_DST_PA2 (backplane signal connector) and<br>BP_DST_PB2 (backplane signal connector)|
|7|SL4_CPU1_PB2 (signal connector on system<br>board)|BP_DST_PB1 (backplane signal connector)|
|8|SL7_CPU1_PB4 (signal connector on system<br>board)|BP_DST_PA2 (backplane signal connector)|
|9|SIG_PWR_2 (system board power connector)|BP_PWR_1 (backplane power connector)|
|10|SL8_CPU1_PA4 (signal connector on system<br>board)|BP_DST_PB2 (backplane signal connector)|
|1|SL1_CPU2_PB1 (signal connector on system board)|CTRL_DST_PA1 (fPERC input<br>connector)|
|2|SL2_CPU2_PA1 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC input<br>connector)|
|3|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power<br>connector)|
|4|CTRL_SRC_PB1 (fPERC controller connector)|BP_DST_PA2 (backplane signal<br>connector) and BP_DST_PB2<br>(backplane signal connector)|
|5|CTRL_SRC_PA1 (fPERC controller connector)|BP_DST_PA1 (backplane signal<br>connector) and BP_DST_PB1 (backplane<br>signal connector)|
|6|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input<br>connector)|
|7|SL4_CPU1_PB2 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC input<br>connector)|
|8|SIG_PWR_2 (system board power connector)|BP_PWR_1 (backplane power<br>connector)|
|9|CTRL_SRC_PB1 (fPERC controller connector)|BP_DST_PA2 (backplane signal<br>connector) and BP_DST_PB2<br>(backplane signal connector)|
|10|CTRL_SRC_PA1 (fPERC controller connector)|BP_DST_PA1 (backplane signal<br>connector) and BP_DST_PB1 (backplane<br>signal connector)|

**NOTE:** An 8 x 2.5-inch NVMe backplanes with fPERC H965i should be assembled outside and inserted into the system,
along with all necessary cables.

**Figure 113. Configuration 12: 16 x 2.5-inch SAS/SATA with fPERC (HBA355i, H355, H755)**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 104. 16 x 2.5-inch SAS/SATA with fPERC (HBA355i, H355, H755)**

|Order|From|To|
|---|---|---|
|1|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_SRC_PA1 (fPERC controller<br>connector)|
|2|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power<br>connector)|
|3|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SA1 (backplane signal<br>connector)|
|4|SIG_PWR_2 (system board power connector)|BP_PWR_1 (backplane power<br>connector)|
|1|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|2|SL4_CPU1_PB2 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC input connector)|
|3|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|4|CTRL_SRC_SA1 (fPERC controller connector)|BP_DST_SA1 (backplane signal connector)|
|5|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SA1 (backplane signal connector)|
|6|SIG_PWR_2 (system board power connector)|BP_PWR_1 (backplane power connector)|

**Figure 115. Configuration 14: 16 x 2.5-inch SAS/SATA + 8 x 2.5-inch NVMe with fPERC (HBA355i, H355, H755)**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 106. 16 x 2.5-inch SAS/SATA + 8 x 2.5-inch NVMe with fPERC (HBA355i, H355, H755)**

|Order|From|To|
|---|---|---|
|1|SL1_CPU2_PB1 (signal connector on system<br>board) and SL2_CPU2_PA1 (signal connector on<br>system board)|BP_DST_PB1 (backplane signal connector)<br>and BP_DST_PA1 (backplane signal<br>connector)|
|2|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|3|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|4|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SA1 (backplane signal connector)|
|5|SIG_PWR_2 (system board power connector)|BP_PWR_1 (backplane power connector)|
|6|SL4_CPU1_PB2 (signal connector on system<br>board)|CTRL_DST_PA2 (fPERC input connector)|
|7|SL7_CPU1_PB4 (signal connector on system<br>board)|CTRL_DST_PB2 (fPERC input connector)|
|8|SIG_PWR_0 (system board power connector)|BP_PWR_1 (backplane power connector)|
|1|SL1_CPU2_PB1 (signal connector on system<br>board) and SL2_CPU2_PA1 (signal connector on<br>system board)|BP_DST_PB1 (backplane signal connector)<br>and BP_DST_PA1 (backplane signal<br>connector)|
|2|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|3|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|

**Table 107. 16 x 2.5-inch SAS/SATA + 8 x 2.5-inch NVMe with fPERC (H965i) (continued)**

|Order|From|To|
|---|---|---|
|4|SL4_CPU1_PB2 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC input connector)|
|5|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SA1 (backplane signal connector)|
|6|CTRL_SRC_SA1 (fPERC controller connector)|BP_DST_SA1 (backplane signal connector)|
|7|SIG_PWR_2 (system board power connector)|BP_PWR_1 (backplane power connector)|
|8|SL7_CPU1_PB4 (signal connector on system<br>board)|BP_DST_PA2 (backplane signal connector)|
|9|SIG_PWR_0 (system board power connector)|BP_PWR_1 (backplane power connector)|
|10|SL8_CPU1_PA4 (signal connector on system<br>board)|BP_DST_BA2 (backplane signal connector)|
|1|SL1_CPU2_PA1 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|2|SL5_CPU2_PB3 (signal connector on system<br>board)|BP_DST_PA1 (backplane signal connector)|
|3|SL6_CPU2_PA3 (signal connector on system<br>board)|BP_DST_PB1 (backplane signal connector)|
|4|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|5|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SA1 (backplane signal connector)|
|6|SL3_CPU1_PA2 (signal connector on system<br>board)|BP_DST_PA2 (backplane signal connector)|

**Table 108. 16 x 2.5-inch SAS/SATA + 8 x 2.5-inch NVMe with fPERC (HBA355i, H355, H755) (continued)**

|Order|From|To|
|---|---|---|
|7|SL4_CPU1_PB2 (signal connector on system<br>board)|BP_DST_PB2 (backplane signal connector)|
|8|SIG_PWR_2 (system board power connector)|BP_PWR_1 (backplane power connector)|
|9|SIG_PWR_0 (system board power connector)|BP_PWR_1 (backplane power connector)|
|1|SL1_CPU2_PA1 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|2|SL2_CPU2_PB1 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC input connector)|
|3|SL5_CPU2_PB3 (signal connector on system<br>board)|BP_DST_PA1 (backplane signal connector)|
|4|SL6_CPU2_PA3 (signal connector on system<br>board)|BP_DST_PB1 (backplane signal connector)|
|5|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|6|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SA1 (backplane signal connector)|
|7|CTRL_SRC_SA1 (fPERC controller connector)|BP_DST_SA1 (backplane signal connector)|
|8|SL3_CPU1_PA2 (signal connector on system<br>board)|BP_DST_PA2 (backplane signal connector)|
|9|SL4_CPU1_PB2 (signal connector on system<br>board)|BP_DST_PB2 (backplane signal connector)|

**Table 109. 16 x 2.5-inch SAS/SATA + 8 x 2.5-inch NVMe with fPERC (H965i) (continued)**

|Order|From|To|
|---|---|---|
|10|SIG_PWR_2 (system board power connector)|BP_PWR_1 (backplane power connector)|
|11|SIG_PWR_0 (system board power connector)|BP_PWR_1 (backplane power connector)|
|1|SL1_CPU2_PA1 (signal connector on system<br>board) and SL2_CPU2_PB1 (signal connector on<br>system board)|BP_DST_PA1 (backplane signal connector)<br>and BP_DST_PB1 (backplane signal<br>connector)|
|2|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|3|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|4|SL4_CPU1_PB2 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC input connector)|
|5|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SB1 (backplane expander signal<br>connector)|
|6|CTRL_SRC_SA1 (fPERC controller connector)|BP_DST_SA1 (backplane expander signal<br>connector)|
|7|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|8|SL7_CPU1_PB4 (signal connector on system<br>board)|BP_DST_PA2 (backplane signal connector)|
|9|SL8_CPU1_PA4 (signal connector on system<br>board)|BP_DST_PB2 (backplane signal connector)|

**Figure 120. Configuration 19: 24 x 2.5-inch SAS/SATA with fPERC (H965i)**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 111. 24 x 2.5-inch SAS/SATA with fPERC (H965i)**

|Order|From|To|
|---|---|---|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|3|SL4_CPU1_PB2 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC input connector)|
|4|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SB1 (backplane expander signal<br>connector)|
|5|CTRL_SRC_SA1 (fPERC controller connector)|BP_DST_SA1 (backplane expander signal<br>connector)|
|6|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SL3_CPU1_PA2 (signal connector on system board)|CTRL_DST_PA1 (fPERC input connector)|
|3|SL4_CPU1_PB2 (signal connector on system board)|CTRL_DST_PB1 (fPERC input connector)|
|4|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SB1 (backplane expander signal connector)|
|5|CTRL_SRC_SA1 (fPERC controller connector)|BP_DST_SA1 (backplane expander signal connector)|
|6|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|7|SL11_CPU1_PB7 (signal connector on system board)|BP_DST_SA1 (rear backplane signal connector) and<br>BP_DST_SB1 (rear backplane signal connector)|
|8|SIG_PWR_0 (system board power connector)|BP_PWR_1 (rear backplane power connector)|

**Figure 122. Configuration 21: 24 x 2.5-inch SAS/SATA + 2 x 2.5-inch SAS/SATA with fPERC (H965i)**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 113. 24 x 2.5-inch SAS/SATA + 2 x 2.5-inch SAS/SATA with fPERC (H965i)**

|Order|From|To|
|---|---|---|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SL3_CPU1_PA2 (signal connector on system board)|CTRL_DST_PA1 (fPERC input connector)|
|3|SL4_CPU1_PB2 (signal connector on system board)|CTRL_DST_PB1 (fPERC input connector)|
|4|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SB1 (backplane expander signal connector)|
|5|CTRL_SRC_SA1 (fPERC controller connector)|BP_DST_SA1 (backplane expander signal connector)|
|6|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|7|BP_SRC_SA2 (backplane expander signal connector)|BP_DST_SA1 (rear backplane signal connector) and<br>BP_DST_SB1 (rear backplane signal connector)|
|8|SIG_PWR_0 (system board power connector)|BP_PWR_1 (rear backplane power connector)|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SL3_CPU1_PA2 (signal connector on system board)|CTRL_DST_PA1 (fPERC input connector)|
|3|SL4_CPU1_PB2 (signal connector on system board)|CTRL_DST_PB1 (fPERC input connector)|
|4|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SB1 (backplane expander signal connector)|
|5|CTRL_SRC_SA1 (fPERC controller connector)|BP_DST_SA1 (backplane expander signal connector)|
|6|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|7|BP_SRC_SA2 (backplane expander signal connector)|BP_DST_SA1 (rear backplane signal connector) and<br>BP_DST_SB1 (rear backplane signal connector)|
|8|SIG_PWR_0 (system board power connector)|BP_PWR_1 (rear backplane power connector)|

**Figure 124. Configuration 23: 24 x 2.5-inch SAS/SATA + 4 x 2.5-inch NVMe with fPERC (H965i)**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 115. 24 x 2.5-inch SAS/SATA + 4 x 2.5-inch NVMe with fPERC (H965i)**

|Order|From|To|
|---|---|---|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SL3_CPU1_PA2 (signal connector on system board)|CTRL_DST_PA1 (fPERC input connector)|
|3|SL4_CPU1_PB2 (signal connector on system board)|CTRL_DST_PB1 (fPERC input connector)|
|4|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SB1 (backplane expander signal connector)|
|5|CTRL_SRC_SA1 (fPERC controller connector)|BP_DST_SA1 (backplane expander signal connector)|
|6|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|7|SL11_CPU1_PB7 (signal connector on system board)|BP_DST_PA2 (rear backplane signal connector) and<br>BP_DST_PB2 (rear backplane signal connector)|
|8|SL8_CPU1_PA4 (signal connector on system board)|BP_DST_PA1 (rear backplane signal connector) and<br>BP_DST_PB1 (rear backplane signal connector)|
|9|SIG_PWR_0 (system board power connector)|BP_PWR_1 (rear backplane power connector)|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SL3_CPU1_PA2 (signal connector on system board)|CTRL_DST_PA1 (fPERC input connector)|
|3|SL4_CPU1_PB2 (signal connector on system board)|CTRL_DST_PB1 (fPERC input connector)|
|4|CTRL_SRC_SA1 (fPERC controller connector)|BP_DST_SA1 (backplane expander signal connector)|
|5|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|6|CTRL_SRC_SA1_PA1 (adapter PERC controller<br>connector)|BP_DST_SB1 (backplane expander signal connector)|

**Figure 126. Configuration 25: 24 x 2.5-inch SAS/SATA + 2 x 2.5-inch SAS/SATA dual controller with fPERC (H965i)**
**and APERC in Riser 2**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 117. 24 x 2.5-inch SAS/SATA + 2 x 2.5-inch SAS/SATA dual controller with fPERC (H965i) and**
**APERC in Riser 2**

|Order|From|To|
|---|---|---|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SL3_CPU1_PA2 (signal connector on system board)|CTRL_DST_PA1 (fPERC input connector)|
|3|SL4_CPU1_PB2 (signal connector on system board)|CTRL_DST_PB1 (fPERC input connector)|
|4|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SB1 (backplane expander signal connector)|
|5|CTRL_SRC_SA1 (fPERC controller connector)|BP_DST_SA1 (backplane expander signal connector)|
|6|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|7|SIG_PWR_0 (system board power connector)|BP_PWR_1 (rear backplane power connector)|
|8|CTRL_SRC_SA1_PA1 (adapter PERC controller<br>connector)|BP_DST_SA1 (rear backplane signal connector) and<br>BP_DST_SB1 (rear backplane signal connector)|
|1|SL1_CPU2_PA1 (signal connector on system<br>board) and SL2_CPU2_PB1 (signal connector on<br>system board)|BP_DST_PA1 (backplane signal connector)<br>and BP_DST_PB1 (backplane signal<br>connector)|
|2|SL5_CPU2_PB3 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|3|SL6_CPU2_PA3 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC input connector)|
|4|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|5|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SB1 (backplane expander signal<br>connector)|
|6|CTRL_SRC_SA1 (fPERC controller connector)|BP_DST_SA1 (backplane expander signal<br>connector)|
|7|SL3_CPU1_PA2 (signal connector on system<br>board)|BP_DST_PA2 (backplane signal connector)|
|8|SL4_CPU1_PB2 (signal connector on system<br>board)|BP_DST_PB2 (backplane signal connector)|
|9|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|

**Figure 128. Configuration 27: 24 x 2.5-inch SAS/SATA with 8 universal (SAS/SATA/NVMe) slot and fPERC**
**(HBA355i, H355, H755)**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 119. 24 x 2.5-inch SAS/SATA with eight universal slots (SAS/SATA/NVMe) and fPERC (HBA355i,**
**H355, H755)**

|Order|From|To|
|---|---|---|
|1|SL1_CPU2_PA1 (signal connector on system<br>board) and SL2_CPU2_PB1 (signal connector on<br>system board)|BP_DST_PA1 (backplane signal connector)<br>and BP_DST_PB1 (backplane signal<br>connector)|
|2|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|3|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|4|SL4_CPU1_PB2 (signal connector on system<br>board)|BP_DST_PA2 (backplane signal connector)|
|5|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SB1 (backplane expander signal<br>connector)|
|6|CTRL_SRC_SA1 (fPERC controller connector)<br>**NOTE:** Tightening the connector screws<br>secures the connector end.|BP_DST_SA1 (backplane expander signal<br>connector)|
|7|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|8|SL7_CPU1_PB4 (signal connector on system<br>board)|BP_DST_PB2 (backplane signal connector)|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|3|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SB1 (backplane expander signal<br>connector)|
|4|CTRL_SRC_SA1 (fPERC controller connector)<br>**NOTE:** Tightening the connector screws<br>secures the connector end.|BP_DST_SA1 (backplane expander signal<br>connector)|
|5|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|

**Figure 130. Configuration 29: 24 x 2.5-inch SAS/SATA + 2 x 2.5-inch NVMe with fPERC (HBA355i, H355, H755)**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 121. 24 x 2.5-inch SAS/SATA + 2 x 2.5-inch NVMe with fPERC (HBA355i, H355, H755)**

|Order|From|To|
|---|---|---|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|3|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SB1 (backplane expander signal<br>connector)|
|4|CTRL_SRC_SA1 (fPERC controller connector)<br>**NOTE:** Tightening the connector screws<br>secures the connector end.|BP_DST_SA1 (backplane expander signal<br>connector)|
|5|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|6|SIG_PWR_0 (system board power connector)|BP_PWR_1 (rear backplane power connector)|
|7|SL11_CPU1_PB7 (signal connector on system<br>board)|BP_DST_SA1 (rear backplane signal connector)<br>and BP_DST_SB1 (rear backplane signal<br>connector)|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|3|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SB1 (backplane expander signal<br>connector)|
|4|CTRL_SRC_SA1 (fPERC controller connector)<br>**NOTE:** Tightening the connector screws<br>secures the connector end.|BP_DST_SA1 (backplane expander signal<br>connector)|
|5|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|6|SIG_PWR_0 (system board power connector)|BP_PWR_1 (rear backplane power connector)|
|7|BP_SRC_SA2 (backplane expander signal<br>connector)|BP_DST_SA1 (rear backplane signal connector)<br>and BP_DST_SB1 (rear backplane signal<br>connector)|

**Figure 132. Configuration 31: 24 x 2.5-inch SAS/SATA + 4 x 2.5-inch SAS/SATA with fPERC (HBA355i, H355, H755)**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 123. 24 x 2.5-inch SAS/SATA + 4 x 2.5-inch SAS/SATA with fPERC (HBA355i, H355, H755)**

|Order|From|To|
|---|---|---|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|3|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SB1 (backplane expander signal<br>connector)|
|4|CTRL_SRC_SA1 (fPERC controller connector)<br>**NOTE:** Tightening the connector screws<br>secures the connector end.|BP_DST_SA1 (backplane expander signal<br>connector)|
|5|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|6|SIG_PWR_0 (system board power connector)|BP_PWR_1 (rear backplane power connector)|
|7|BP_SRC_SA2 (backplane expander signal<br>connector)|BP_DST_SA1 (rear backplane signal connector)<br>and BP_DST_SB1 (rear backplane signal<br>connector)|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|3|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SB1 (backplane expander signal<br>connector)|
|4|CTRL_SRC_SA1 (fPERC controller connector)<br>**NOTE:** Tightening the connector screws<br>secures the connector end.|BP_DST_SA1 (backplane expander signal<br>connector)|
|5|SL4_CPU1_PB2 (signal connector on system<br>board)|BP_DST_PA1 (rear backplane signal connector)<br>and BP_DST_PB1 (rear backplane signal<br>connector)|
|6|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|7|SIG_PWR_0 (system board power connector)|BP_PWR_1 (rear backplane power connector)|
|8|SL11_CPU1_PB7 (signal connector on system<br>board)|BP_DST_PA2 (rear backplane signal connector)<br>and BP_DST_PB2 (rear backplane signal<br>connector)|

**Figure 134. Configuration 33: 24 x 2.5-inch SAS/SATA dual controller with fPERC (HBA355i, H355, H755) and**
**APERC in Riser 2**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 125. 24 x 2.5-inch SAS/SATA dual controller with fPERC (HBA355i, H355, H755) and APERC in**
**Riser 2**

|Order|From|To|
|---|---|---|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SL3_CPU1_PA2 (signal connector on system board)|CTRL_DST_PA1 (fPERC input connector)|
|3|CTRL_SRC_SA1 (fPERC controller connector)<br>**NOTE:** Tightening the connector screws secures<br>the connector end.|BP_DST_SA1 (backplane expander signal connector)|
|4|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|5|CTRL_SRC_SA1_PA1 (adapter PERC controller<br>connector)|BP_DST_SB1 (backplane expander signal connector)|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SL3_CPU1_PA2 (signal connector on system board)|CTRL_DST_PA1 (fPERC input connector)|
|3|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SB1 (backplane expander signal connector)|
|4|CTRL_SRC_SA1 (fPERC controller connector)|BP_DST_SA1 (backplane expander signal connector)|
|5|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|6|SIG_PWR_0 (system board power connector)|BP_PWR_1 (rear backplane power connector)|
|7|CTRL_SRC_SA1_PA1 (adapter PERC controller<br>connector)|BP_DST_SA1 (rear backplane signal connector) and<br>BP_DST_SB1 (rear backplane signal connector)|

**Figure 136. Configuration 35: 24 x 2.5-inch SAS/SATA with 8 universal (SAS/SATA/NVMe) slots and fPERC**
**(HBA355i, H355, H755)**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 127. 24 x 2.5-inch SAS/SATA with 8 universal (SAS/SATA/NVMe) slots and fPERC (HBA355i,**
**H355, H755)**

|Order|From|To|
|---|---|---|
|1|SL1_CPU2_PA1 (signal connector on system<br>board) and SL2_CPU2_PB1 (signal connector on<br>system board)|BP_DST_PA1 (backplane signal connector)<br>and BP_DST_PB1 (backplane signal<br>connector)|
|2|SL5_CPU2_PB3 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|3|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|4|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SB1 (backplane expander signal<br>connector)|
|5|CTRL_SRC_SA1 (fPERC controller connector)<br>**NOTE:** Tightening the connector screws<br>secures the connector end.|BP_DST_SA1 (backplane expander signal<br>connector)|
|6|SL3_CPU1_PA2 (signal connector on system<br>board)|BP_DST_PA2 (backplane signal connector)|
|7|SL4_CPU1_PB2 (signal connector on system<br>board)|BP_DST_PB2 (backplane signal connector)|
|8|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|3|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SB1 (backplane expander signal<br>connector)|
|4|CTRL_SRC_SA1 (fPERC controller connector)<br>**NOTE:** Tightening the connector screws<br>secures the connector end.|BP_DST_SA1 (backplane expander signal<br>connector)|
|5|SL4_CPU1_PB2 (signal connector on system<br>board)|BP_DST_PA2 (backplane signal connector)|
|6|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|7|SL7_CPU1_PB4 (signal connector on system<br>board)|BP_DST_PB2 (backplane signal connector)|
|8|SIG_PWR_0 (system board power connector)|BP_PWR_1 (rear backplane power<br>connector)|
|9|BP_SRC_SA2 (backplane expander signal<br>connector)|BP_DST_SA1 (rear backplane signal<br>connector) and BP_DST_SB1 (rear backplane<br>signal connector)|

**Figure 138. Configuration 37: 16 x 2.5-inch (8 x SAS/SATA with fPERC (HBA355i, H355, H755) + 8 x NVMe RAID**
**with fPERC (H755N))**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 129. 16 x 2.5-inch (8 x SAS/SATA with fPERC (HBA355i, H355, H755) + 8 x NVMe RAID with fPERC**
**(H755N))**

|Order|From|To|
|---|---|---|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|3|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|4|CTRL_SRC_PB1 (fPERC controller connector)|BP_DST_PA2 (backplane signal connector)<br>and BP_DST_PB2 (backplane signal<br>connector)|
|5|CTRL_SRC_PA1 (fPERC controller connector)|BP_DST_PA1 (backplane signal connector)<br>and BP_DST_PB1 (backplane signal<br>connector)|
|6|SL7_CPU1_PB4 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|3|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|4|CTRL_SRC_PB1 (fPERC controller connector)|BP_DST_PA2 (backplane signal connector)<br>and BP_DST_PB2 (backplane signal<br>connector)|
|5|CTRL_SRC_PA1 (fPERC controller connector)|BP_DST_PA1 (backplane signal connector)<br>and BP_DST_PB1 (backplane signal<br>connector)|
|6|SL7_CPU1_PB4 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|7|SL8_CPU1_PA4 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC input connector)|

**NOTE:** An 8 x 2.5-inch NVMe backplane with fPERC H965i should be assembled outside and inserted into the system,
along with all necessary cables.

**Figure 140. Configuration 39: 16 x 2.5-inch (8 x SAS/SATA with fPERC (H965i) + 8 x NVMe RAID with fPERC**
**(H755N))**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 131. 16 x 2.5-inch (8 x SAS/SATA with fPERC (H965i) + 8 x NVMe RAID with fPERC (H755N))**

|Order|From|To|
|---|---|---|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|3|SL4_CPU1_PB2 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC input connector)|
|4|CTRL_SRC_PA1 (fPERC controller connector)|BP_DST_SA1 (backplane signal connector)|
|5|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|6|CTRL_SRC_PB1 (fPERC controller connector)|BP_DST_PA2 (backplane signal connector)<br>and BP_DST_PB2 (backplane signal<br>connector)|
|7|CTRL_SRC_PA1 (fPERC controller connector)|BP_DST_PA1 (backplane signal connector)<br>and BP_DST_PB1 (backplane signal<br>connector)|
|8|SL7_CPU1_PB4 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|3|SL4_CPU1_PB2 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC input connector)|
|4|CTRL_SRC_PA1 (fPERC controller connector)|BP_DST_SA1 (backplane signal connector)|
|5|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|6|CTRL_SRC_PB1 (fPERC controller connector)|BP_DST_PA2 (backplane signal connector)<br>and BP_DST_PB2 (backplane signal<br>connector)|
|7|CTRL_SRC_PA1 (fPERC controller connector)|BP_DST_PA1 (backplane signal connector)<br>and BP_DST_PB1 (backplane signal<br>connector)|
|8|SL7_CPU1_PB4 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|9|SL8_CPU1_PA4 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC input connector)|

**NOTE:** An 8 x 2.5-inch NVMe backplane with fPERC H965i should be assembled outside and inserted into the system,
along with all necessary cables.

**Figure 142. Configuration 41: 24 x 2.5-inch (NVMe Gen4) Passive**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 133. 24 x 2.5-inch (NVMe Gen4) Passive**

|Order|From|To|
|---|---|---|
|1|IO_RISER4(CPU2) (Riser 4 connector on system<br>board)|BP_DST_PA1 (backplane signal connector)<br>and BP_DST_PB1 (backplane signal<br>connector)|
|2|SL1_CPU2_PA1 (signal connector on system<br>board)|BP_DST_PA2 (backplane signal connector)|
|3|SL2_CPU2_PB1 (signal connector on system<br>board)|BP_DST_PB2 (backplane signal connector)|
|4|SL5_CPU2_PB3 (signal connector on system<br>board)|BP_DST_PA3 (backplane signal connector)|
|5|SL6_CPU2_PA3 (signal connector on system<br>board)|BP_DST_PB3 (backplane signal connector)|
|6|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|7|SL3_CPU1_PA2 (signal connector on system<br>board)|BP_DST_PA4 (backplane signal connector)|
|8|SL4_CPU1_PB2 (signal connector on system<br>board)|BP_DST_PB4 (backplane signal connector)|
|9|SL7_CPU1_PB4 (signal connector on system<br>board)|BP_DST_PA5 (backplane signal connector)|
|10|SL8_CPU1_PA4 (signal connector on system<br>board)|BP_DST_PB5 (backplane signal connector)|
|11|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|12|IO_RISER1(CPU1) (Riser 1 connector on system<br>board)|BP_DST_PA6 (backplane signal connector)<br>and BP_DST_PB6 (backplane signal<br>connector)|
|1|BOSS_PWR_MB (BOSS power connector on<br>system board)|BOSS_PWR (BOSS module power<br>connector)|
|2|SL12_PCH_PA6 (signal connector on system<br>board)|BOSS (BOSS module signal connector)|

**NOTE:** The BOSS-N1 power and signal cables are routed beneath Riser 1 and ensure not to damage the cables.

**Figure 144. Configuration 43: BOSS-N1 module in 4 x 2.5-inch rear drive module**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 135. BOSS-N1 module in 4 x 2.5-inch rear drive module**

|Order|From|To|
|---|---|---|
|1|BOSS_PWR_MB (BOSS power connector on<br>system board)|BOSS_PWR (BOSS module power<br>connector)|
|2|SL12_PCH_PA6 (signal connector on system<br>board)|BOSS (BOSS module signal connector)|
|1|BP_DST_SA1 (backplane signal connector)|CTRL_SRC_SB1 (fPERC controller<br>connector)|
|2|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC controller<br>connector)|
|3|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|4|SIG_PWR_2 (system board power connector)|BP_PWR_1 (backplane power connector)|

**Figure 146. Configuration 45: 16 x EDSFF E3.S NVMe without fPERC**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 137. 16 x EDSFF E3.S NVMe without fPERC**

|Order|From|To|
|---|---|---|
|1|SL1_CPU2_PA1 (signal connector on system<br>board)|BP_DST_PA1 (backplane signal connector)|
|2|SL2_CPU2_PB1 (signal connector on system<br>board)|BP_DST_PB1 (backplane signal connector)|
|3|SL5_CPU2_PB3 (signal connector on system<br>board)|BP_DST_PA2 (backplane signal connector)|
|4|SL6_CPU2_PA3 (signal connector on system<br>board)|BP_DST_PB2 (backplane signal connector)|
|5|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|6|SL3_CPU1_PA2 (signal connector on system<br>board)|BP_DST_PA1 (backplane signal connector)|
|7|SL4_CPU1_PB2 (signal connector on system<br>board)|BP_DST_PB1 (backplane signal connector)|
|8|SL7_CPU1_PB4 (signal connector on system<br>board)|BP_DST_PA2 (backplane signal connector)|
|9|SL8_CPU1_PA4 (signal connector on system<br>board)|BP_DST_PB2 (backplane signal connector)|
|10|SIG_PWR_2 (system board power connector)|BP_PWR_1 (backplane power connector)|
|1|SL1_CPU2_PA1 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC controller<br>connector)|
|2|SL2_CPU2_PB1 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC controller<br>connector)|
|3|BP_PWR_CTRL (power controller connector on<br>backplane)|POWER (fPERC power controller connector)|
|4|BP_DST_PA1 (backplane signal connector) and<br>BP_DST_PB1 (backplane signal connector)|CTRL_SRC_PA1 (fPERC controller<br>connector)|
|5|BP_DST_PA2 (backplane signal connector) and<br>BP_DST_PB2 (backplane signal connector)|CTRL_SRC_PB1 (fPERC controller<br>connector)|
|6|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|7|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC controller<br>connector)|
|8|SL4_CPU1_PB2 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC controller<br>connector)|
|9|BP_PWR_CTRL (power controller connector on<br>backplane)|POWER (fPERC power controller connector)|
|10|BP_DST_PA1 (backplane signal connector) and<br>BP_DST_PB1 (backplane signal connector)|CTRL_SRC_PA1 (fPERC controller<br>connector)|
|11|BP_DST_PA2 (backplane signal connector) and<br>BP_DST_PB2 (backplane signal connector)|CTRL_SRC_PB1 (fPERC controller<br>connector)|
|12|SIG_PWR_2 (system board power connector)|BP_PWR_1 (backplane power connector)|

**Figure 148. Configuration 47: 24 x 2.5-inch (NVMe Gen5) switched**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 139. 24 x 2.5-inch (NVMe Gen5) switched**

|Order|From|To|
|---|---|---|
|1|SL1_CPU2_PA1 (signal connector on system<br>board)|BP_DST_PA1 (backplane signal connector)|
|2|SL2_CPU2_PB1 (signal connector on system<br>board)|BP_DST_PB1 (backplane signal connector)|
|3|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|4|SL3_CPU1_PA2 (signal connector on system<br>board)|BP_DST_PA2 (backplane signal connector)|
|5|SL4_CPU1_PB2 (signal connector on system<br>board)|BP_DST_PB2 (backplane signal connector)|
|6|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|1|BP_DST_PB1 (backplane signal connector)|CTRL_SRC_PB1 (fPERC controller<br>connector)|
|2|BP_DST_PA1 (backplane signal connector)|CTRL_SRC_PA1 (fPERC controller<br>connector)|
|3|SL1_CPU2_PA1 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC controller<br>connector)|
|4|SL2_CPU2_PB1 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC controller<br>connector)|
|5|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|6|BP_DST_PB2 (backplane signal connector)|CTRL_SRC_PB1 (fPERC controller<br>connector)|
|7|BP_DST_PA2 (backplane signal connector)|CTRL_SRC_PA1 (fPERC controller<br>connector)|
|8|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC controller<br>connector)|
|9|SL4_CPU1_PB2 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC controller<br>connector)|
|10|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|

**Figure 150. Configuration 49: Dell DPU (Mellanox: 25 Gb, Pensando: 100 Gb or 25 Gb) without power cable**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 141. Dell DPU (Mellanox: 25 Gb, Pensando: 100 Gb or 25 Gb) without power cable**

|Order|From|To|
|---|---|---|
|1|J_REAR_SERIAL1 ( UART connector on rear I/O<br>card)|UART (UART connector on MIC card)|
|2|CON1 (Connector 1 on MIC card)|Connector on DPU card in riser 1|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|CTRL_SRC_SB1_PB1 (adapter PERC controller<br>connector)|BP_DST_SB1 (backplane signal connector)|
|3|CTRL_SRC_SA1_PA1 (adapter PERC controller<br>connector)|BP_DST_SA1 (backplane signal connector)|
|4|SIG_PWR_0 (system board power connector)|Power connector on E3 rear module*|
|5|SL4_CPU1_PA2 (signal connector on system<br>board)|Rear backplane signal connector on E3 rear module*|
|6|SL11_CPU1_PB7 (signal connector on system<br>board)|Rear backplane signal connector on E3 rear module*|

**NOTE:** *Disconnect cables from the system board side since the cables in the rear E3 module are preassembled.

**Figure 152. Configuration 51: 24 x 2.5-inch SAS/SATA + 4 x EDSFF E3.S with fPERC (HBA355i, H355, H755)**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 143. 24 x 2.5-inch SAS/SATA + 4 x EDSFF E3.S with fPERC (HBA355i, H355, H755)**

|Order|From|To|
|---|---|---|
|1|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SB1 (backplane expander signal connector)|
|2|CTRL_SRC_SA1 (fPERC controller connector)|BP_DST_SA1 (backplane expander signal connector)|
|3|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|4|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|5|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|6|SIG_PWR_0 (system board power connector)|Power connector on E3 rear module*|
|7|SL4_CPU1_PA2 (signal connector on system<br>board)|Rear backplane signal connector on E3 rear module*|
|8|SL11_CPU1_PB7 (signal connector on system<br>board)|Rear backplane signal connector on E3 rear module*|

**NOTE:** *Disconnect cables from the system board side since the cables in the rear E3 module are preassembled.

**Figure 153. Configuration 52: 24 x 2.5-inch SAS/SATA + 4 x EDSFF E3.S with fPERC (H965i)**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 144. 24 x 2.5-inch SAS/SATA + 4 x EDSFF E3.S with fPERC (H965i)**

|Order|From|To|
|---|---|---|
|1|CTRL_SRC_SB1 (fPERC controller connector)|BP_DST_SB1 (backplane expander signal connector)|
|2|CTRL_SRC_SA1 (fPERC controller connector)|BP_DST_SA1 (backplane expander signal connector)|
|3|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|4|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|5|SIG_PWR_2 (system board power connector)|BP_PWR_2 (backplane power connector)|
|6|SL4_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC input connector)|
|7|SIG_PWR_0 (system board power connector)|Power connector on E3 rear module*|
|8|SL8_CPU1_PA4 (signal connector on system<br>board)|Rear backplane signal connector on E3 rear module*|
|9|SL11_CPU1_PB7 (signal connector on system<br>board)|Rear backplane signal connector on E3 rear module*|
|1|SL5_CPU2_PB3 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input<br>connector)|
|2|SL6_CPU2_PA3 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC input<br>connector)|
|3|CTRL_SRC_SA1 (fPERC controller connector)|BP_DST_SA1 (backplane expander signal<br>connector)|
|4|SL3_CPU1_PA2 (signal connector on system<br>board) and SL4_CPU1_PB2 (signal connector on<br>system board)|BP_DST_PA2 (backplane signal<br>connector) and BP_DST_PB2<br>(backplane signal connector)|
|5|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power<br>connector)|
|6|SL2_CPU2_PB1 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC input<br>connector)|
|7|SL1_CPU2_PA1 (signal connector on system board)|CTRL_DST_PA1 (fPERC input<br>connector)|

**Figure 155. Configuration 54: 16 x 2.5-inch (8 x SAS/SATA + 8 x NVMe RAID) dual controller with fPERC (H965i)**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 146. 16 x 2.5-inch (8 x SAS/SATA + 8 x NVMe RAID) dual controller with fPERC (H965i)**

|Order|From|To|
|---|---|---|
|1|SIG_PWR_1 (system board power connector)|BP_PWR_1 (backplane power connector)|
|2|SL1_CPU2_PA1 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|3|SL2_CPU2_PB1 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC input connector)|
|4|CTRL_SRC_PA1 (fPERC controller connector)|BP_DST_SA1 (backplane signal connector)|
|5|SL3_CPU1_PA2 (signal connector on system<br>board)|CTRL_DST_PA1 (fPERC input connector)|
|6|SL4_CPU1_PB2 (signal connector on system<br>board)|CTRL_DST_PB1 (fPERC input connector)|
|7|CTRL_SRC_PB1 (fPERC controller connector)|BP_DST_PA2 (backplane signal connector)<br>and BP_DST_PB2 (backplane signal<br>connector)|
|8|CTRL_SRC_PA1 (fPERC controller connector)|BP_DST_PA1 (backplane signal connector)<br>and BP_DST_PB1 (backplane signal<br>connector)|
|9|SIG_PWR_2 (system board power connector)|BP_PWR_1 (backplane power connector)|
|1|SL11_CPU1_PB7 (signal connector on system<br>board)|SL13_CPU1_PB7 (signal connector on system<br>board)|

**Figure 157. Configuration 56: Dell Data Processing Unit (DPU) (Intel: 200 Gb) with power cable**

**NOTE:** Follow the sequential order as shown in the table to remove the cables, to install the cables follow the reverse
sequential order.

**Table 148. Dell DPU (Intel: 200 Gb) with power cable**

|Order|From|To|
|---|---|---|
|1|J_REAR_SERIAL1 ( UART connector on rear I/O<br>card)|UART (UART connector on MIC card)|
|2|CON1 (Connector 1 on MIC card)|Connector on DPU card in riser 1|
|3|PWR1_B (system board power connector) and<br>PSU1_SIG (power signal connector)|Aux power connector on DPU card in riser 1|

**NOTE:** Dell DPUs only support vSphere 8.0+ and Partner or Channel DPU only supports Linux-based OS.



## PERC module

This is a service technician replaceable part only.



### Removing the rear mounting front PERC module

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the cooling fan cage assembly.
4. Remove the drive backplane cover.
5. If required, remove the air shroud or remove the GPU air shroud..
6. Disconnect all the cables, observe the cable routing.
**NOTE:** See cable routing section for more information.

**Steps**

1. Using a Phillips #2 screwdriver, loosen the captive screws on the rear mounting front PERC module.
2. Slide the rear mounting front PERC module to disengage from the connector on the drive backplane.



### Figure 158. Removing the rear mounting front PERC module

3. Disconnect the battery power cable from the front PERC card.
4. Tilt and lift the battery holder assembly from the front PERC shroud.
**NOTE:** Remove the battery cable from the front PERC shroud cable holder.

**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.

**Figure 159. Removing the battery holder assembly from the PERC shroud**

5. Press and remove the battery from the battery holder.
**Figure 160. Removing the battery from the battery holder**

6. Using a Phillips #2 screwdriver, remove the four screws on the front PERC shroud.
7. Remove the front PERC shroud from the PERC.card.
8. Remove the front PERC card from the PERC tray.
**Figure 161. Removing the front PERC card**

**Next steps**

1. Replace the rear mounting front PERC module.



### Installing the rear mounting front PERC module

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the cooling fan cage assembly.
4. Remove the drive backplane cover.
5. If required, remove the air shroud or remove the GPU air shroud.
6. Route the cable properly to prevent the cable from being pinched or crimped.
**NOTE:** See cable routing section for more information.

**Steps**

1. Align and install front PERC card to the guides on the front PERC tray.
2. Align and Install the front PERC shroud on the front PERC card.
3. Using a Phillips #2 screwdriver, tighten the four screws to secure the front PERC shroud to the front PERC card.
**Figure 162. Installing the front PERC card**

4. Insert the battery tab into the battery holder and ensure the battery that it is firmly seated.
**Figure 163. Installing the battery into the battery holder**

5. Tilt the battery holder assembly to align the tabs with the front PERC shroud.
6. Press the battery holder assembly on to the front PERC shroud.
7. Connect the battery power cable to the front PERC card.
**NOTE:** Route the battery cable into the front PERC shroud cable holder.

**Figure 164. Installing the battery holder assembly into the PERC shroud**

8. Align the connectors and guide slots on the rear mounting front PERC module with the connectors and guide pins on the
drive backplane.

9. Slide the rear mounting front PERC module until the module is connected to the drive backplane.
10. Using a Phillips #2 screwdriver, tighten the captive screws on the rear mounting front PERC module.
**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.



### Figure 165. Installing the rear mounting front PERC module

**Next steps**

1. Connect all the cables, observe the cable routing.
2. Install the cooling fan cage assembly.
3. If removed, install the air shroud or install the GPU air shroud.
4. Install the drive backplane cover.
5. Follow the procedure listed in After working inside your system.



### Removing the adapter PERC module

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. If required, remove the air shroud or remove the GPU air shroud.
4. If required, remove the drive backplane cover.
5. Remove the cooling fan cage assembly.
6. Remove the expansion card riser.
7. Disconnect all the cables from the adapter PERC (APERC) card, observe the cable routing.
**NOTE:** Refer to cable routing section for more information.

**NOTE:** APERC module must be installed only in expansion card riser 1 or riser 2.

**Steps**

1. Tilt the expansion card retention latch lock to open.
2. Pull the card holder before removing the card from the riser.
3. Hold the APERC module by the edges. and pull the module from the expansion card connector on the riser.
**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.

**Figure 166. Removing the APERC module**

4. If the APERC module is not going to be replaced, install a filler bracket and close the card retention latch.
**NOTE:** You must install a filler bracket over an empty expansion card slot to maintain Federal Communications
Commission (FCC) certification of the system. The brackets also keep dust and dirt out of the system and aid in
proper cooling and airflow inside the system.

**Figure 167. Installing the filler bracket**

**Next steps**

1. Replace the APERC module.



### Installing the adapter PERC module

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. If required, remove the air shroud or remove the GPU air shroud.
4. If required, remove the drive backplane cover.
5. Remove the cooling fan cage assembly.
6. Remove the expansion card riser.
7. Disconnect all the cables from the adapter PERC (APERC) card, observe the cable routing.
**NOTE:** Refer to cable routing section for more information.

8. If installing a new APERC module, unpack it and prepare the module for installation.
**NOTE:** For instructions, see the documentation accompanying the card.

**NOTE:** APERC module must be installed only in expansion card riser 1 or riser 2.

**Steps**

1. Pull and lift up the expansion card retention latch lock to open.
2. If installed, remove the filler bracket.
**NOTE:** Store the filler bracket for future use. Filler brackets must be installed in empty expansion card slots to maintain
Federal Communications Commission (FCC) certification of the system. The brackets also keep dust and dirt out of the
system and aid in proper cooling and airflow inside the system.

**Figure 168. Removing the filler bracket**

3. Hold the adapter PERC (APERC) module by the edges, and align the module edge connector with the expansion card
connector on the riser.

4. Insert the module into the expansion card connector until firmly seated.
5. Close the expansion card retention latch.
6. Push the card holder to hold the module in the riser.
**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.

**Figure 169. Installing the APERC module**

**Next steps**

1. Connect the cables to the APERC module and route the cables properly.
**NOTE:** Refer to cable routing section for more information.

2. Install the cooling fan cage assembly.
3. If removed, install the air shroud or install the GPU air shroud.
4. Install the drive backplane cover.
5. Install the expansion card riser.
6. Follow the procedure listed in After working inside your system.



### Removing the EDSFF E3.S PERC module

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the cooling fan cage assembly.
4. Remove the drive backplane cover.
5. If required, remove the air shroud or remove the GPU air shroud..
6. Disconnect all the cables from the PERC, observe the cable routing.
**NOTE:** See cable routing section; configuration 46 for more information.

**Steps**

1. Press the release tabs on both the sides of the PERC tray, and lift the PERC tray out of the system.
2. Disconnect the PERC power cable from the EDSFF E3.S backplane module.
**Figure 170. Removing the PERC tray**

3. Press the blue tabs and remove the PERC power cable from the PERC card.
4. Disconnect the battery power cable from the PERC card.
5. Tilt and lift the battery holder assembly from the front PERC tray.
**NOTE:** Remove the battery cable from the front PERC shroud cable holder.

**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.

**Figure 171. Removing the battery holder assembly from the PERC shroud**

6. Press and remove the battery from the battery holder.
**Figure 172. Removing the battery from the battery holder**

7. Using a Phillips #2 screwdriver, remove the four screws on the front PERC shroud.
8. Remove the PERC shroud from the PERC.card.
9. Remove the PERC card from the PERC tray.
**Figure 173. Removing the PERC card from PERC tray**

**NOTE:** The procedure to remove the other PERC card is similar.

**Next steps**

1. Replace the EDSFF E3.S PERC module.



### Installing the EDSFF E3.S PERC module

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the cooling fan cage assembly.
4. Remove the drive backplane cover.
5. If required, remove the air shroud or remove the GPU air shroud.
6. Route the cable properly to prevent the cable from being pinched or crimped.
**NOTE:** See cable routing section; configuration 46 for more information.

**Steps**

1. Align and install PERC card to the guides on the front PERC tray.
2. Align and Install the PERC shroud on the PERC card.
3. Using a Phillips #2 screwdriver, tighten the four screws to secure the PERC shroud to the PERC tray.
**Figure 174. Installing the PERC card into PERC tray**

4. Insert the battery tab into the battery holder and ensure the battery that it is firmly seated.
**Figure 175. Installing the battery into the battery holder**

5. Align and connect the PERC power cable to the PERC card.
6.
**NOTE:** Route the PERC cable into the PERC tray cable holder.

Tilt the battery holder assembly to align the tabs with the PERC tray.

7. Press the battery holder assembly on to the PERC tray.
8. Connect the battery power cable to the front PERC card.
**NOTE:** Route the battery cable into the front PERC shroud cable holder.

**Figure 176. Installing the battery holder assembly into the PERC shroud**

**NOTE:** Procedure to install the other PERC card is similar.

9. Align the guide slots on the PERC tray with the guide pins on the system.
**NOTE:** Connect all the cables to the PERC card before lowering the tray. See cable routing section for more
information.

10. Lower the PERC tray, until it is firmly seated and secured.
**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.

**Figure 177. Installing the PERC tray**

**Next steps**

1. Connect all the cables, observe the cable routing.
**NOTE:** See cable routing section; configuration 46 for more information.

2. Install the cooling fan cage assembly.
3. If removed, install the air shroud or install the GPU air shroud.
4. Install the drive backplane cover.
5. Follow the procedure listed in After working inside your system.



## EDSFF E3.S backplane module

This is a service technician replaceable part only.



### Removing the EDSFF E3.S backplane module

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the front bezel.
4. Remove the cooling fan cage assembly.
5. Remove the drive backplane cover.
6. If required, remove the air shroud or remove the GPU air shroud.
7. Remove the EDSFF E3.S PERC module.
8. Remove the EDSFF E3.S drives.
9. Disconnect the cables, observe the cable routing.
**NOTE:** See cable routing section; configuration 45 or 46 for more information.

**Steps**

1. Using a Phillips #2 screwdriver, loosen the captive screws on the EDSFF E3.S backplane module.
2. Slide and remove the EDSFF E3.S backplane module from the system.



### Figure 178. Removing the EDSFF E3.S backplane module

**Next steps**

1. Replace the EDSFF E3.S backplane module.



### Installing the EDSFF E3.S backplane module

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the front bezel.
4. Remove the cooling fan cage assembly.
5. Remove the drive backplane cover.
6. If required, remove the air shroud or remove the GPU air shroud.
7. Remove the EDSFF E3.S PERC module.
8. Remove the EDSFF E3.S drives.
9. Disconnect the cables, observe the cable routing.
**NOTE:** See cable routing section; configuration 45 or 46 for more information.

**Steps**

1. Align the guides on the EDSFF E3.S backplane module with the slots on the system.
2. Slide the EDSFF E3.S backplane module towards the rear of the system, until it is firmly secured.
3. Using a Phillips #2 screwdriver, tighten the captive screw on the EDSFF E3.S backplane module.



### Figure 179. Installing the EDSFF E3.S backplane module

**Next steps**

1. Connect all the cables, observe the cable routing.
**NOTE:** See cable routing section; configuration 45 or 46 for more information.

2. Install the EDSFF E3.S PERC module.
3. Install the cooling fan cage assembly.
4. If removed, install the air shroud or install the GPU air shroud.
5. Install the drive backplane cover.
6. Install the EDSFF E3.S drives .
7. Install the front bezel.
8. Follow the procedure listed in After working inside your system.



## System memory





### System memory guidelines

The PowerEdge R760 system supports DDR5 registered DIMMs (RDIMMs).

Your system memory is organized into eight channels per processor (two memory sockets per channel), 16 memory sockets per
processor and 32 memory sockets per system.

**Figure 180. Memory channels**

Memory channels are organized as follows:

**Table 149. Memory channels**

|Processor|Channel<br>A|Channel B|Channel C|Channel D|Channel E|Channel F|Channel G|Channel H|
|---|---|---|---|---|---|---|---|---|
|Processor<br>1|Slots A1<br>and A9|Slots A7<br>and A15|Slots A3<br>and A11|Slots A5 and<br>A13|Slots A4 and<br>A12|Slots A6<br>and A14|Slots A2 and<br>A10|Slots A8 and<br>A16|
|Processor<br>2|Slots B1<br>and B9|Slots B7<br>and B15|Slots B3<br>and B11|Slots B5 and<br>B13|Slots B4 and<br>B12|Slots B6<br>and B14|Slots B2 and<br>B10|Slots B8 and<br>B16|

**Table 150. Supported memory matrix**

|DIMM type|Rank|Capacity|DIMM rated<br>voltage and<br>speed|Operating Speed|Col6|
|---|---|---|---|---|---|
|**DIMM type**|**Rank**|**Capacity**|**DIMM rated**<br>**voltage and**<br>**speed**|**1 DIMM per**<br>**channel (DPC)**|**2 DIMMs per**<br>**channel (DPC)**|
|RDIMM|1 R|16 GB|DDR5 (1.1 V), 4800<br>MT/s|4800 MT/s|4400 MT/s|
|**DIMM type**|**Rank**|**Capacity**|**DIMM rated**<br>**voltage and**<br>**speed**|**1 DIMM per**<br>**channel (DPC)**|**2 DIMMs per**<br>**channel (DPC)**|
||2 R|32 GB, 64 GB|DDR5 (1.1 V), 4800<br>MT/s|4800 MT/s|4400 MT/s|
||4 R|128 GB|DDR5 (1.1 V), 4800<br>MT/s|4800 MT/s|4400 MT/s|
||8 R|256 GB|DDR5 (1.1 V), 4800<br>MT/s|4800 MT/s|4400 MT/s|
||1 R|16 GB|DDR5 (1.1 V), 5600<br>MT/s|5600 MT/s|4400 MT/s|
||2 R|32 GB, 64 GB, 96<br>GB|DDR5 (1.1 V), 5600<br>MT/s|5600 MT/s|4400 MT/s|
||4 R|128 GB|DDR5 (1.1 V), 5600<br>MT/s|5600 MT/s|4400 MT/s|
||8 R|256 GB|DDR5 (1.1 V), 5600<br>MT/s|5600 MT/s|4400 MT/s|

**NOTE:** 5600 MT/s RDIMMs are applicable for 5 [th] Gen Intel® Xeon® Scalable Processors.

**NOTE:** The processor may reduce the performance of the rated DIMM speed.

**NOTE:** No support for 96 GB memory when configured with either 2 DIMMs per CPU RDIMM or 4 DIMMs per CPU
RDIMM.



### General memory module installation guidelines

To ensure optimal performance of your system, observe the following general guidelines when configuring your system memory.
If your system's memory configuration fails to observe these guidelines, your system might not boot, stop responding during
memory configuration, or operate with reduced memory.

The memory bus may operate at speeds of 5600 MT/s, 4800 MT/s, 4400 MT/s, or 4000 MT/s depending on the following
factors:

- System profile selected (for example, Performance, Performance Per Watt Optimized (OS), or Custom [can be run at high
speed or lower])

- Maximum supported DIMM speed of the processors
- Maximum supported speed of the DIMMs
**NOTE:** MT/s indicates DIMM speed in MegaTransfers per second.

**NOTE:** Fault Resilient Memory supports only eight and sixteen DIMMs per processor.

- All DIMMs must be DDR5.
- Memory mixing is not supported for:
  - Different DIMM capacities
  - X4 and X8 DRAM memory modules
  - 3DS and non-3DS RDIMMs
**NOTE:** 3DS is a DRAM technology that is used to manufacture the highest capacity DIMMs. See your DIMM
documentation for additional details.

- Modes supported in Xeon Max processor:
  - Xeon Max only without DIMMs
  - Cache with DIMMs
  - Flat with DIMMs
- Supported RDIMM configurations per Xeon Max processor:
  - 0 DIMM (Xeon Max only mode)
  - 1 DIMM (Flat mode)
  - 2 DIMM (Flat mode)
  - 4 DIMMs (Cache or Flat mode)
  - 8 DIMMs (Cache or Flat mode)
  - 16 DIMMs (Cache or Flat mode)
- DDR5/Xeon Max memory ratio must be in the range of 2:1 ~ 64:1 for cache mode.
**NOTE:** Each Xeon Max processor has 64 GB integrated memory.

- The combination of Flat and Cache modes is not supported.
**NOTE:** Fault Resilient Memory is only available in Flat mode, with eight or sixteen DIMMs per processor.

- If memory modules with different speeds are installed, they operate at the speed of the slowest installed memory module(s).
- Populate memory module sockets only if a processor is installed.
  - For single-processor systems, sockets A1 to A16 are available.
  - For dual-processor systems, sockets A1 to A16 and sockets B1 to B16 are available.
  - A minimum of one DIMM must be populated for each installed processor.
- In **Optimizer Mode**, the DRAM controllers operate independently in the 64-bit mode and provide optimized memory
performance.

**Table 151. Memory population rules**

|Processor|Memory population|Memory population information|
|---|---|---|
|Single processor|A{1}, A{2}, A{3}, A{4}, A{5}, A{6},<br>A{7}, A{8}, A{9}, A{10}, A{11}, A{12},<br>A{13}, A{14}, A{15}, A{16}|1, 2, 4, 6, 8, 12 or 16 DIMMs are<br>allowed.|
|Dual processor (Start with processor1.<br>Processor 1 and processor 2 population<br>should match)|A{1}, B{1}, A{2}, B{2}, A{3}, B{3}, A{4},<br>B{4}, A{5}, B{5}, A{6}, B{6}, A{7},<br>B{7} A{8}, B{8}, A{9}, B{9}, A{10},<br>B{10}, A{11}, B{11}, A{12}, B{12}, A{13},<br>B{13}, A{14}, B{14}, A{15}, B{15},<br>A{16}, B{16}|2, 4, 8, 12, 16, 24 or 32 DIMMs are<br>supported per system.|
|Single processor|A{1}, A{2}, A{3}, A{4}, A{5}, A{6},<br>A{7}, A{8}, A{9}, A{10}, A{11}, A{12},<br>A{13}, A{14}, A{15}, A{16}|0, 1, 2, 4, 8, or 16 DIMMs are allowed.|
|Dual processor (Start with processor1.<br>Processor 1 and processor 2 population<br>should match)|A{1}, B{1}, A{2}, B{2}, A{3}, B{3}, A{4},<br>B{4}, A{5}, B{5}, A{6}, B{6}, A{7},<br>B{7} A{8}, B{8}, A{9}, B{9}, A{10},<br>B{10}, A{11}, B{11}, A{12}, B{12}, A{13},<br>B{13}, A{14}, B{14}, A{15}, B{15},<br>A{16}, B{16}|0, 2, 4, 8, 16, or 32 DIMMs are<br>supported per system.|

- Populate all the sockets with white release tabs first, followed by the sockets with black release tabs.
- Unbalanced or odd memory configurations result in a performance loss, and the system may not identify the memory
modules being installed. Always populate memory channels identically with equal DIMMs for the best performance.

- Supported RDIMM configurations are 1, 2, 4, 6, 8, 12, or 16 DIMMs per processor.



### Removing a memory module

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the air shroud or remove the GPU air shroud.
**WARNING: The memory modules are hot to touch for some time after the system has been powered off. Allow**
**the memory modules to cool before handling them.**

**NOTE:** To ensure proper system cooling, memory module blanks must be installed in any memory socket that is not
populated. The memory module blanks compatible with the R760 are DDR5 gray color blanks. Remove the memory module
blanks only if you intend to install memory module in these sockets.

**Steps**

1. Locate the appropriate memory module socket.
2. To release the memory module from the socket, simultaneously press the ejectors on both ends of the memory module
socket to fully open.

**CAUTION: Handle each memory module only by the card edges, ensuring not to touch the middle of the**
**memory module or metallic contacts.**

3. Lift the memory module away from the system.



### Figure 181. Removing a memory module

**Next steps**

Replace the memory module.



### Installing a memory module

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the air shroud or remove the GPU air shroud.
**Steps**

1. Locate the appropriate memory module socket.
**CAUTION: Handle each memory module only by the card edges, ensuring not to touch the middle of the**
**memory module or metallic contacts.**

**NOTE:** Ensure that the socket ejector latches are fully open before installing the memory module.

2. Align the edge connector of the memory module with the alignment key of the memory module socket, and insert the
memory module in the socket.

**CAUTION: To prevent damage to the memory module or the memory module socket during installation, do**
**not bend or flex the memory module. Insert both ends of the memory module simultaneously.**

**NOTE:** The memory module socket has an alignment key that enables you to install the memory module in the socket in
only one orientation.

**CAUTION: Do not apply pressure at the center of the memory module; apply pressure at both ends of the**
**memory module evenly.**

3. Press the memory module with your thumbs until the ejectors firmly click into place. When the memory module is properly
seated in the socket, the memory module socket levers align with the levers on the other sockets that have memory modules
that are installed.



### Figure 182. Installing a memory module

**Next steps**

1. Install the air shroud or install the GPU air shroud.
2. Follow the procedure listed in After working inside your system.
3. To verify that the memory module has been installed properly, press **F2** during reboot and click **System Setup Main**
**Menu > System BIOS > Memory Settings** . In the **Memory Settings** screen, the **System Memory Size** must reflect the
updated capacity of the installed memory.
4. If the **System Memory Size** is incorrect, one or more of the memory modules may not be installed properly. Shut down the
system and ensure that the memory modules are firmly seated in the correct sockets.
5. Run the system memory test in system diagnostics.



## Processor and heat sink module

This is a service technician replaceable part only.



### Removing the processor and heat sink module

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the air shroud or remove the GPU air shroud.
**NOTE:** The heat sink and processor are hot to touch for some time after the system has been powered off. Allow the
heat sink and processor to cool down before handling them.

**Steps**

1. Ensure all four anti-tilt wires are in the locked position (outward position), and then using a Torx T30 screwdriver, loosen the
captive nuts on the processor heat sink module (PHM) in the order that is mentioned below:

a. Loosen the first nut three turns.
b. Loosen the nut diagonally opposite to the nut you loosened first.
c. Repeat the procedure for the remaining two nuts.
d. Return to the first nut and loosen it completely.

**NOTE:** Ensure that the anti-tilt wires on the PHM are in locked position when loosening the captive nuts.

2. Set all the anti-tilt wires to unlocked position (inward position).



### Figure 183. Removing the processor heat sink module

3. Lift the PHM from the system and set the PHM aside with the processor side facing up.
**Figure 184. Removing a heat sink**

**Next steps**

If you are removing a faulty heat sink, replace the heat sink, if not, remove the processor.



### Removing the processor

**Prerequisites**

**WARNING: Remove the processor from processor and heat sink module (PHM) only if you are replacing the**
**processor or heat sink.**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the air shroud or remove the GPU air shroud.
4. Remove the processor heat sink module.
**CAUTION: You may find the CMOS battery loss or CMOS checksum error that is displayed during the first**
**instance of powering on the system after the processor or system board replacement which is expected. To fix**
**this, simply go to setup option to configure the system settings.**

**Steps**

1. Place the heat sink with the processor side facing up.
2. Using your thumb, lift the thermal interface material (TIM) break lever to release the processor from the TIM and retaining
clip.

**NOTE:** For Xeon Max processor, turn the side lever (up to 60 degrees) with the flat blade screwdriver to release the
Xeon Max processor from the TIM and retaining clip.

3. Holding the processor by the edges, lift the processor away from the retaining clip.
**NOTE:** Ensure to hold the retaining clip to the heat sink as you lift the TIM break lever.

**Figure 185. Removing the processor**

**Figure 186. Removing the Xeon Max processor**

**NOTE:** Ensure to return the TIM break lever or side lever on the retaining clip back to original position.

4. Using your thumb and index finger, first hold the retaining clip release tab at the pin 1 connector, pull out the tip of the
retaining clip release tab, and then lift the retaining clip partially from the heat sink.

5. Repeat the procedure at the remaining three corners of the retaining clip.
6. After all the corners are released from the heat sink, lift the retaining clip from the pin 1 corner of the heat sink.
**Figure 187. Removing the retaining clip**

**Figure 188. Removing the Xeon Max retaining clip**

**Next steps**

Replace the processor.



### Installing the processor

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the air shroud or remove the GPU air shroud.
4. Remove the processor heat sink module.
**Steps**

1. Place the processor in the processor tray.
**NOTE:** Ensure the pin 1 indicator on the processor tray is aligned with the pin 1 indicator on the processor.

2. Place the retaining clip on top of the processor in the processor tray aligning pin 1 indicator on the processor.
**NOTE:** Ensure the pin 1 indicator on the retaining clip is aligned with the pin 1 indicator on the processor before placing
the retaining clip on the processor.

**NOTE:** Before you install the heat sink, ensure to place the processor and retaining clip in the tray.

**Figure 189. Installing the retaining clip**

**Figure 190. Installing the Xeon Max retaining clip**

3. Align the processor with retaining clip, by using your fingers press the retaining clip on all the four sides until it clicks into
place.

**NOTE:** Ensure that the processor is securely latched to the retaining clip.

**Figure 191. Press the retaining clip on the four sides**

4. If you are using an existing heat sink, remove the thermal grease from the heat sink by using a clean lint-free cloth.
5. Apply the thermal grease in a thin spiral design on the bottom of the heat sink.
**CAUTION: Applying too much thermal grease can result in excess grease coming in contact with and**
**contaminating the processor socket.**

**NOTE:** The thermal grease syringe is intended for single use only. Dispose the syringe after you use it.

**Figure 192. Applying thermal grease**

6. For new heat sink, pull and remove the plastic cover from the base of heat sink.
**Figure 193. Removing the cover**

7. Place the heat sink on the processor and press the base of the heat sink until the retaining clip locks onto the heat sink at all
the four corners.

**CAUTION: To avoid damaging the fins on the heat sink, do not press down on the heat sink fins.**

**NOTE:**

   - Ensure latching features on retaining clip, and heat sink are aligned during assembly.
   - Ensure that the pin 1 indicator on the heat sink is aligned with the pin 1 indicator on the retaining clip before placing
the heat sink onto the retaining clip.

**Figure 194. Installing the heat sink onto the processor**

**Next steps**

**Figure 195. Installing the heat sink onto the Xeon Max processor**

1. Install the processor heat sink module.
2. Install the air shroud or install the GPU air shroud.
3. Follow the procedure listed in After working inside your system.



### Installing the processor and heat sink module

**Prerequisites**

Never remove the heat sink from a processor unless you intend to replace the processor or heat sink. The heat sink is necessary
to maintain proper thermal conditions.
1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the air shroud or remove the GPU air shroud.
4. If installed, remove the processor dust cover.
**Steps**

1. Set the anti-tilt wires to the unlocked position on the heat sink (inward position).
2. Align the pin 1 indicator of the heat sink to the system board, and then place the processor heat sink module (PHM) on the
processor socket.

**CAUTION: To avoid damaging the fins on the heat sink, do not press down on the heat sink fins.**

**NOTE:** Ensure that the PHM is held parallel to the system board to prevent damaging the components.

**Figure 196. Installing the processor heat sink**

3. Set the anti-tilt wires to the locked position (outward position), and then using the Torx T30 screwdriver, tighten the captive
nuts (8 in-lbf) on the heat sink in the order below:

a. In a random order, tighten the first nut three turns.
b. Tighten the nut diagonally opposite to the nut that you tighten first.
c. Repeat the procedure for the remaining two nuts.
d. Return to the first nut to tighten it completely.
e. Check all the nuts to ensure they are firmly secured.

**Figure 197. Set the anti-tilt wires to the locked position and tightening the nuts**

**Next steps**

1. Install the air shroud or install the GPU air shroud.
2. Follow the procedure listed in the After working inside your system.



### Removing the Direct Liquid Cooling module

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the air shroud or remove the GPU air shroud.
4. Remove the expansion card riser.
**WARNING: The Direct liquid cooling (DLC) module and processor are too hot to touch for some time after the**
**system has been powered off. Allow the liquid cooling module and processor to cool down before handling them.**

**NOTE:** Rear I/O (RIO) board is different for the system with DLC module.

**Steps**

1. Using a Phillips #2 screw driver, loosen the captive screw on the DLC ring holder.
2. Tilt the DLC ring holder to loosen the DLC tubes.
3. Disconnect the DLC leak detection cable from the LC RIO board.
4. Remove the DLC tubes from the clip and LC RIO board.
5. Slightly lift the DLC tubes surrounding the DIMM slots.
6. Ensure all four anti-tilt wires are in the locked position (outward position), and then using a Torx T30 screwdriver, loosen the
captive nuts on the DLC module in the order that is mentioned below:

a. Loosen the first nut three turns.
b. Loosen the nut diagonally opposite to the nut you loosened first.
c. Repeat the procedure for the remaining two nuts.
d. Return to the first nut and loosen it completely.

**NOTE:** Ensure that the anti-tilt wires on the DLC module are in locked position when loosening the captive nuts.

7. Set the anti-tilt wires on the DLC module to the unlock position and lift the DLC module from the system.
**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.

**Figure 198. Removing the DLC module**

**Next steps**

1. If you are removing a faulty liquid cooling module, replace the Direct Liquid Cooling module, else remove the processor.



### Removing the processor

**Prerequisites**

**WARNING: Remove the processor from Direct Liquid Cooling (DLC) module only if you are replacing the**
**processor or DLC module.**

**NOTE:** Removing the Xeon Max processor from DLC module is the same as processor and heat sink module (PHM).

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the air shroud or remove the GPU air shroud.
4. Remove the Direct Liquid Cooling module.
**CAUTION: You may find the CMOS battery loss or CMOS checksum error that is displayed during the first**
**instance of powering on the system after the processor or system board replacement which is expected. To fix**
**this, go to setup option to configure the system settings.**

**Steps**

1. Place the DLC module with the processor side facing up.
2. Using your thumb, lift the thermal interface material (TIM) break lever to release the processor from the TIM and retaining
clip.

3. Holding the processor by the edges, lift the processor away from the retaining clip.
**NOTE:** Ensure to hold the retaining clip to the heat sink as you lift the TIM break lever.

**Figure 199. Lift up the TIM break lever**

**NOTE:** Ensure to return the TIM break lever back to original position.

4. Using your thumb and index finger, first hold the retaining clip release tab at the pin 1 connector, pull out the tip of the
retaining clip release tab, and then lift the retaining clip partially from the DLC module.

5. Repeat the procedure at the remaining three corners of the retaining clip.
6. After all the corners are released from the DLC module, lift the retaining clip from the pin 1 corner of the DLC module.
**Figure 200. Removing the retaining clip**

**Next steps**

Replace the processor.



### Installing the processor

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the air shroud or remove the GPU air shroud.
4. Remove the liquid cooling module.

module (PHM).

**Steps**

1. Place the processor in the processor tray.
**NOTE:** Ensure the pin 1 indicator on the processor tray is aligned with the pin 1 indicator on the processor.

2. Place the retaining clip on top of the processor in the processor tray aligning pin 1 indicator on the processor.
**NOTE:** Ensure the pin 1 indicator on the retaining clip is aligned with the pin 1 indicator on the processor before placing
the retaining clip on the processor.

**NOTE:** Before you install the DLC module, ensure to place the processor and retaining clip in the tray.

**Figure 201. Installing the retaining clip**

3. Align the processor with retaining clip, by using your fingers press the retaining clip on all the four sides until it clicks into
place.

**NOTE:** Ensure that the processor is securely latched to the retaining clip.

**Figure 202. Press the retaining clip on the four sides**

4. If you are using an existing DLC module, remove the thermal grease from the DLC module by using a clean lint-free cloth.
5. Apply the thermal grease syringe in a quadrilateral design on the top of the processor.
6. Use the thermal grease syringe included with your processor kit to apply the grease in a thin spiral design on the bottom of
the DLC module.

**CAUTION: Applying too much thermal grease can result in excess grease coming in contact with and**
**contaminating the processor socket.**

**NOTE:** The thermal grease syringe is intended for single use only. Dispose the syringe after you use it.

**Figure 203. Applying thermal grease**

7. For new DLC module, pull and remove the plastic cover from the base of DLC module.
**Figure 204. Removing the cover**

8. Place the DLC module on the processor and press the base of the DLC module until the retaining clip locks onto the DLC
module at all the four corners.

**NOTE:**

   - Ensure latching features on retaining clip, and DLC module are aligned during assembly.
   - Ensure that the pin 1 indicator on the DLC module is aligned with the pin 1 indicator on the retaining clip before
placing the DLC module onto the retaining clip.

**Figure 205. Installing the DLC module onto the processor**

**Next steps**

1. Install the Direct Liquid Cooling module.
2. Install the air shroud or install the GPU air shroud.
3. Follow the procedure listed in After working inside your system.



### Installing the Direct Liquid Cooling module

**Prerequisites**

Never uninstall the Direct Liquid Cooling (DLC) module from a processor unless you intend to replace the processor or system
board. The DLC module is necessary to maintain proper thermal conditions.

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the air shroud or remove the GPU air shroud.
4. Remove the expansion card riser.
5. If installed, remove the processor dust cover.
**NOTE:** Ensure anti-tilt wires on the DLC module are in the unlocked position.

**Steps**

1. Align the DLC module with the standoff screws on the system board.
**NOTE:** Ensure that the DLC tubes and liquid cooling leak detection cable are placed towards the rear of the system.

2. Place the module on the processor slot and set all the anti-tilt wires to locked position (outward position).
3. Route the DLC tubes to the front of the system and along the DIMM latches.
4. Using the Torx T30 screwdriver, tighten the captive nuts (8 in-lbf) on the DLC module in the order below:

a. In a random order, tighten the first nut three turns.
b. Tighten the nut diagonally opposite to the nut that you tighten first.
c. Repeat the procedure for the remaining two nuts.
d. Return to the first nut to tighten it completely.
e. Check all the nuts to ensure they are firmly secured.

5. The tubes leading towards the rear of the chassis and the DLC leak detection cable are placed in between the PSU 2 and the
clip of rear I/O board (RIO).

**NOTE:** Leak detection cable must be placed first into the clip (underneath the cooling tubes), and then place tube 2 and
tube 1 into the clip to ensure that cable does not interfere with the PCIe risers.

6. Route the rear end of the DLC tubes through the RIO board.
**NOTE:** Follow the number labels on the DLC tubes and ring holders (1,2).

7. Connect the DLC leak detection cable to the connector on RIO.
8. Align the rubber ring on the tubes with the ring holder.
9. Tilt the DLC ring holder and using a Phillips #2 screwdriver, tighten the captive screw on the DLC ring holder to secure it in
place.

**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.

**Figure 206. Installing the DLC module**

**Next steps**

1. Install the expansion card riser.
2. Install the air shroud or install the GPU air shroud.
3. Follow the procedure listed in the After working inside your system.



## Expansion cards and expansion card risers

**NOTE:** When an expansion card is not supported or missing, the iDRAC and Lifecycle Controller logs an event. This does
not prevent your system from booting. However, if a F1/F2 pause occurs with an error message, see Troubleshooting
[expansion cards section in the PowerEdge Servers Troubleshooting Guide at PowerEdge Manuals.](https://www.dell.com/poweredgemanuals)



### Expansion card installation guidelines

**Figure 207. Expansion card riser slot connectors**

1. Riser 4
2. Riser 3
3. Riser 2
4. Riser 1
**Figure 208. Riser 1B**

1. Slot 1
2. Slot 2
**Figure 209. Riser 1R**

1. Slot 1
2. Slot 2
**Figure 210. Riser 1R FL**

1. Slot 1
2. Slot 2
**Figure 211. Riser 1P**

1. Slot 2
**Figure 212. Riser 1P FL**

1. Slot 2
**Figure 213. Riser 1Q**

1. Slot 1
2. Slot 2
**Figure 214. Riser 2A**

1. Slot 6
2. Slot 3
**Figure 215. Riser 3A**

1. Slot 5
**Figure 216. Riser 3A FL**

1. Slot 5
**Figure 217. Riser 3B**

1. Slot 4
2. Slot 5
**Figure 218. Riser 4P**

1. Slot 7
**Figure 219. Riser 4P - FL**

1. Slot 7
**Figure 220. Riser 4B**

1. Slot 8
2. Slot 7
**Figure 221. Riser 4Q**

1. Slot 7
2. Slot 8
**Figure 222. Riser 4R**

1. Slot 7
2. Slot 8
**NOTE:** The expansion-card slots are not hot-swappable.

The following table provides guidelines for installing expansion cards to ensure proper cooling and mechanical fit. The expansion
cards with the highest priority should be installed first using the slot priority indicated. All the other expansion cards should be
installed in the card priority and slot priority order.

**Table 153. Expansion card riser configurations**

|Configurations|Expansion card<br>risers|PCIe Slots|Form factor|Controlling<br>processor|Slot's electrical<br>bandwidth/<br>physical<br>connector|
|---|---|---|---|---|---|
|Config1. 6 x8 FH + 2<br>x16 LP|R1B|1|Full height|Processor 1|PCIe Gen4 x8 (x16<br>connector)|
|Config1. 6 x8 FH + 2<br>x16 LP|R1B|2|Full height|Processor 1|PCIe Gen4 x8 (x16<br>connector)|
|Config1. 6 x8 FH + 2<br>x16 LP|R2A|3|Low profile|Processor 1|PCIe Gen4 x16 (x16<br>connector)|
|Config1. 6 x8 FH + 2<br>x16 LP|R2A|6|Low profile|Processor 2|PCIe Gen4 x16 (x16<br>connector)|
|Config1. 6 x8 FH + 2<br>x16 LP|R3B|4|Full height|Processor 2|PCIe Gen4 x8 (x16<br>connector)|
|Config1. 6 x8 FH + 2<br>x16 LP|R3B|5|Full height|Processor 2|PCIe Gen4 x8 (x16<br>connector)|
|Config1. 6 x8 FH + 2<br>x16 LP|R4B|7|Full height|Processor 2|PCIe Gen4 x8 (x16<br>connector)|
|Config1. 6 x8 FH + 2<br>x16 LP|R4B|8|Full height|Processor 2|PCIe Gen4 x8 (x16<br>connector)|
|Config2. 4 x8 FH<br>(Gen5) + 2x 8FH + 2<br>x16 LP|R1Q|1|Full height|Processor 1|PCIe Gen5 x8 (x16<br>connector)|
|Config2. 4 x8 FH<br>(Gen5) + 2x 8FH + 2<br>x16 LP|R1Q|2|Full height|Processor 1|PCIe Gen5 x8 (x16<br>connector)|
|Config2. 4 x8 FH<br>(Gen5) + 2x 8FH + 2<br>x16 LP|R2A|3|Low profile|Processor 1|PCIe Gen4 x16 (x16<br>connector)|
|||6|Low profile|Processor 2|PCIe Gen4 x16 (x16<br>connector)|
||R3B|4|Full height|Processor 2|PCIe Gen4 x8 (x16<br>connector)|
||R3B|5|Full height|Processor 2|PCIe Gen4 x8 (x16<br>connector)|
||R4Q|7|Full height|Processor 2|PCIe Gen5 x8 (x16<br>connector)|
||R4Q|8|Full height|Processor 2|PCIe Gen5 x8 (x16<br>connector)|
|Config3-1. 2 x16 LP +<br>2 x8 FH + 2 x16 FH<br>(Gen5)|R1P|2|Full height|Processor 1|PCIe Gen5 x16 (x16<br>connector)|
|Config3-1. 2 x16 LP +<br>2 x8 FH + 2 x16 FH<br>(Gen5)|R2A|3|Low profile|Processor 1|PCIe Gen4 x16 (x16<br>connector)|
|Config3-1. 2 x16 LP +<br>2 x8 FH + 2 x16 FH<br>(Gen5)|R2A|6|Low profile|Processor 2|PCIe Gen4 x16 (x16<br>connector)|
|Config3-1. 2 x16 LP +<br>2 x8 FH + 2 x16 FH<br>(Gen5)|R3B|4|Full height|Processor 2|PCIe Gen4 x8 (x16<br>connector)|
|Config3-1. 2 x16 LP +<br>2 x8 FH + 2 x16 FH<br>(Gen5)|R3B|5|Full height|Processor 2|PCIe Gen4 x8 (x16<br>connector)|
|Config3-1. 2 x16 LP +<br>2 x8 FH + 2 x16 FH<br>(Gen5)|R4P|7|Full height|Processor 2|PCIe Gen5 x16 (x16<br>connector)|
|Config3-2. 2 x16 LP +<br>2 x8 FH + 2 x16 DW<br>(Gen5)|R1P|2|Full height (DW)|Processor 1|PCIe Gen5 x16 (x16<br>connector)|
|Config3-2. 2 x16 LP +<br>2 x8 FH + 2 x16 DW<br>(Gen5)|R2A|3|Low profile|Processor 1|PCIe Gen4 x16 (x16<br>connector)|
|Config3-2. 2 x16 LP +<br>2 x8 FH + 2 x16 DW<br>(Gen5)|R2A|6|Low profile|Processor 2|PCIe Gen4 x16 (x16<br>connector)|
|Config3-2. 2 x16 LP +<br>2 x8 FH + 2 x16 DW<br>(Gen5)|R3B|4|Full height|Processor 2|PCIe Gen4 x8 (x16<br>connector)|
|Config3-2. 2 x16 LP +<br>2 x8 FH + 2 x16 DW<br>(Gen5)|R3B|5|Full height|Processor 2|PCIe Gen4 x8 (x16<br>connector)|
|Config3-2. 2 x16 LP +<br>2 x8 FH + 2 x16 DW<br>(Gen5)|R4P|7|Full height (DW)|Processor 2|PCIe Gen5 x16 (x16<br>connector)|
|Config4-1. 2x16LP +<br>3x8FH +1x16 FH<br>(Gen5) +1x16 Gen5<br>SNAPI|R1P|2|Full height|Processor 1|PCIe Gen5 x16 (x16<br>connector)|
|Config4-1. 2x16LP +<br>3x8FH +1x16 FH<br>(Gen5) +1x16 Gen5<br>SNAPI|R2A|3|Low profile|Processor 1|PCIe Gen4 x16 (x16<br>connector)|
|Config4-1. 2x16LP +<br>3x8FH +1x16 FH<br>(Gen5) +1x16 Gen5<br>SNAPI|R2A|6|Low profile|Processor 2|PCIe Gen4 x16 (x16<br>connector)|
|Config4-1. 2x16LP +<br>3x8FH +1x16 FH<br>(Gen5) +1x16 Gen5<br>SNAPI|R3B|4|Full height|Processor 2|PCIe Gen4 x8 (x16<br>connector)|
|Config4-1. 2x16LP +<br>3x8FH +1x16 FH<br>(Gen5) +1x16 Gen5<br>SNAPI|R3B|5|Full height|Processor 2|PCIe Gen4 x8 (x16<br>connector)|

**Table 153. Expansion card riser configurations (continued)**

|Configurations|Expansion card<br>risers|PCIe Slots|Form factor|Controlling<br>processor|Slot's electrical<br>bandwidth/<br>physical<br>connector|
|---|---|---|---|---|---|
||R4R|7|Full height|Processor 1 and 2|PCIe Gen5 x16 (x16<br>connector)|
|Config5-1. 2 x16 LP +<br>2 x16 FH + 2x16 FH<br>(Gen5)|R1R|1|Full height|Processor 1|PCIe Gen4 x16 (x16<br>connector)|
|Config5-1. 2 x16 LP +<br>2 x16 FH + 2x16 FH<br>(Gen5)|R1R|2|Full height|Processor 1|PCIe Gen5 x16 (x16<br>connector)|
|Config5-1. 2 x16 LP +<br>2 x16 FH + 2x16 FH<br>(Gen5)|R2A|3|Low profile|Processor 1|PCIe Gen4 x16 (x16<br>connector)|
|Config5-1. 2 x16 LP +<br>2 x16 FH + 2x16 FH<br>(Gen5)|R2A|6|Low profile|Processor 2|PCIe Gen4 x16 (x16<br>connector)|
|Config5-1. 2 x16 LP +<br>2 x16 FH + 2x16 FH<br>(Gen5)|R3A|5|Full height|Processor 2|PCIe Gen4 x16 (x16<br>connector)|
|Config5-1. 2 x16 LP +<br>2 x16 FH + 2x16 FH<br>(Gen5)|R4P|7|Full height|Processor 2|PCIe Gen5 x16 (x16<br>connector)|
|Config5-2. 2 x16 LP +<br>2 x16 FH + 2x16 FH<br>(Gen5)|R1R|1|Full height|Processor 1|PCIe Gen4 x16 (x16<br>connector)|
|Config5-2. 2 x16 LP +<br>2 x16 FH + 2x16 FH<br>(Gen5)|R1R|2|Full height|Processor 1|PCIe Gen5 x16 (x16<br>connector)|
|Config5-2. 2 x16 LP +<br>2 x16 FH + 2x16 FH<br>(Gen5)|R2A|3|Low profile|Processor 1|PCIe Gen4 x16 (x16<br>connector)|
|Config5-2. 2 x16 LP +<br>2 x16 FH + 2x16 FH<br>(Gen5)|R2A|6|Low profile|Processor 2|PCIe Gen4 x16 (x16<br>connector)|
|Config5-2. 2 x16 LP +<br>2 x16 FH + 2x16 FH<br>(Gen5)|R3A|5|Full height|Processor 2|PCIe Gen4 x16 (x16<br>connector)|
|Config5-2. 2 x16 LP +<br>2 x16 FH + 2x16 FH<br>(Gen5)|R4P|7|Full height (DW)|Processor 2|PCIe Gen5 x16 (x16<br>connector)|
|Config6. 2 x16 LP + 2<br>x8 FH (Gen5)|R2A|3|Low profile|Processor 1|PCIe Gen4 x16 (x16<br>connector)|
|Config6. 2 x16 LP + 2<br>x8 FH (Gen5)|R2A|6|Low profile|Processor 2|PCIe Gen4 x16 (x16<br>connector)|
|Config6. 2 x16 LP + 2<br>x8 FH (Gen5)|R4Q|7|Full height|Processor 2|PCIe Gen5 x8 (x16<br>connector)|
|Config6. 2 x16 LP + 2<br>x8 FH (Gen5)|R4Q|8|Full height|Processor 2|PCIe Gen5 x8 (x16<br>connector)|
|Config7. 2 x16 LP + 4<br>x8 FH (Gen5)|R1Q|1|Full height|Processor 1|PCIe Gen5 x8 (x16<br>connector)|
|Config7. 2 x16 LP + 4<br>x8 FH (Gen5)|R1Q|2|Full height|Processor 1|PCIe Gen5 x8 (x16<br>connector)|
|Config7. 2 x16 LP + 4<br>x8 FH (Gen5)|R2A|3|Low profile|Processor 1|PCIe Gen4 x16 (x16<br>connector)|
|Config7. 2 x16 LP + 4<br>x8 FH (Gen5)|R2A|6|Low profile|Processor 2|PCIe Gen4 x16 (x16<br>connector)|
|Config7. 2 x16 LP + 4<br>x8 FH (Gen5)|R4Q|7|Full height|Processor 2|PCIe Gen5 x8 (x16<br>connector)|
|||8|Full height|Processor 2|PCIe Gen5 x8 (x16<br>connector)|
|Config8. 2 x8 FH + 1<br>x16 LP (Gen4)|R1B|1|Full height|Processor 1|PCIe Gen4 x8 (x16<br>connector)|
|Config8. 2 x8 FH + 1<br>x16 LP (Gen4)|R1B|2|Full height|Processor 1|PCIe Gen4 x8 (x16<br>connector)|
|Config8. 2 x8 FH + 1<br>x16 LP (Gen4)|R2A|3|Low profile|Processor 1|PCIe Gen4 x16 (x16<br>connector)|
|Config9. 3 x8 FH<br>(Gen5) + 1 x16 LP|R1Q|1|Full height|Processor 1|PCIe Gen5 x8 (x16<br>connector)|
|Config9. 3 x8 FH<br>(Gen5) + 1 x16 LP|R1Q|2|Full height|Processor 1|PCIe Gen5 x8 (x16<br>connector)|
|Config9. 3 x8 FH<br>(Gen5) + 1 x16 LP|R2A|3|Low profile|Processor 1|PCIe Gen4 x16 (x16<br>connector)|
|Config9. 3 x8 FH<br>(Gen5) + 1 x16 LP|R4R|7|Full height|Processor 1|PCIe Gen5 x8 (x16<br>connector)|
|Config10-1. 1 x16 LP +<br>1 x8 FH (Gen5) +1 x16<br>FH (Gen5)|R1P|2|Full height|Processor 1|PCIe Gen5 x16 (x16<br>connector)|
|Config10-1. 1 x16 LP +<br>1 x8 FH (Gen5) +1 x16<br>FH (Gen5)|R2A|3|Low profile|Processor 1|PCIe Gen4 x16 (x16<br>connector)|
|Config10-1. 1 x16 LP +<br>1 x8 FH (Gen5) +1 x16<br>FH (Gen5)|R4R|7|Full height|Processor 1|PCIe Gen5 x8 (x16<br>connector)|
|Config11. 2 x16 LP + 2<br>x8 FH|R1-paddle|N/A|N/A|N/A|N/A|
|Config11. 2 x16 LP + 2<br>x8 FH|R2A|3|Low profile|Processor 1|PCIe Gen4 x16 (x16<br>connector)|
|Config11. 2 x16 LP + 2<br>x8 FH|R2A|6|Low profile|Processor 2|PCIe Gen4 x16 (x16<br>connector)|
|Config11. 2 x16 LP + 2<br>x8 FH|R3B|4|Full height|Processor 2|PCIe Gen4 x8 (x16<br>connector)|
|Config11. 2 x16 LP + 2<br>x8 FH|R3B|5|Full height|Processor 2|PCIe Gen4 x8 (x16<br>connector)|
|Config11. 2 x16 LP + 2<br>x8 FH|R4-paddle|N/A|N/A|N/A|N/A|
|Config12. 2 x16 LP +<br>4 x8 FH (Gen5)|R1Q|1|Full height|Processor 1|PCIe Gen5 x8 (x16<br>connector)|
|Config12. 2 x16 LP +<br>4 x8 FH (Gen5)|R1Q|2|Full height|Processor 1|PCIe Gen5 x8 (x16<br>connector)|
|Config12. 2 x16 LP +<br>4 x8 FH (Gen5)|R2A|3|Low profile|Processor 1|PCIe Gen4 x16 (x16<br>connector)|
|Config12. 2 x16 LP +<br>4 x8 FH (Gen5)|R2A|6|Low profile|Processor 2|PCIe Gen4 x16 (x16<br>connector)|
|Config12. 2 x16 LP +<br>4 x8 FH (Gen5)|R4Q|7|Full height|Processor 2|PCIe Gen5 x8 (x16<br>connector)|
|Config12. 2 x16 LP +<br>4 x8 FH (Gen5)|R4Q|8|Full height|Processor 2|PCIe Gen5 x8 (x16<br>connector)|

**NOTE:** In Config11, R1 and R4 paddle cards do not have a physical PCIe slot. See R1 and R4 paddle card installation.

**Table 154. Configuration 0: No risers**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Inventec (LOM Card)|Integrated slot|1|
|Broadcom (OCP: 100Gb)|Integrated slot|1|
|Mellanox (OCP: 100Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Mellanox (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Foxconn (BOSS)|Integrated slot|1|
|Foxconn (Front PERC12 HBA465i)|Integrated slot|1|
|Foxconn (Front PERC12 H965i)|Integrated slot|2|
|Foxconn (Front PERC11 H755N)|Integrated slot|2|
|Foxconn (Front PERC11 H755)|Integrated slot|1|
|Foxconn (Front PERC11 H355)|Integrated slot|1|
|Foxconn (Front PERC11 HBA355i)|Integrated slot|1|
|Inventec (VGA)|8, 4|1|
|Inventec (Serial)|8, 4|1|
|Mellanox (NVIDIA) (Channel DPU: 25Gb)|5, 4, 7, 1, 2|5|
|Inventec (LOM Card)|Integrated slot|1|
|Broadcom (OCP: 100Gb)|Integrated slot|1|
|Mellanox (OCP: 100Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Mellanox (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|

**Table 155. Configuration 1: R1B+R2A+R3B+R4B (continued)**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Foxconn (BOSS)|Integrated slot|1|
|Foxconn (Front PERC12 HBA465i)|Integrated slot|1|
|Foxconn (Front PERC12 H965i)|Integrated slot|2|
|Foxconn (Front PERC11 H755N)|Integrated slot|2|
|Foxconn (Front PERC11 H755)|Integrated slot|1|
|Foxconn (Front PERC11 H355)|Integrated slot|1|
|Foxconn (Front PERC11 HBA355i)|Integrated slot|1|
|Foxconn (PERC Adapter12 HBA465i)|3|1|
|Foxconn (PERC Adapter12 H965i)|3|1|
|Foxconn (PERC Adapter11 H755)|3|1|
|Foxconn (PERC Adapter11 HBA355i)|3|1|
|Foxconn (PERC Adapter11 H355)|3|1|
|Intel (GPU ATS-M)|7, 8, 4, 5, 1, 2|6|
|NVIDIA (GPU A2)|7, 8, 4, 5, 1, 2|6|
|Mellanox (LP NIC: NDR200)|6, 3|2|
|Mellanox (NIC: HDR100 VPI)|6, 3|2|
|Mellanox (LP NIC: HDR VPI), 2P|6, 3|2|
|Mellanox (LP NIC: HDR VPI), 1P|6, 3|2|
|Mellanox (NIC: 100Gb)|6, 3|2|
|Broadcom (NIC: 100Gb)|6, 3|2|
|Intel (NIC: 100Gb)|6, 3|2|
|Broadcom (NIC: 25Gb)|5, 4, 7, 1, 2|5|
|Mellanox (NIC: 25Gb)|5, 4, 7, 1, 2|5|
|Mellanox (NIC: 25Gb)|6, 3|2|
|Intel (NIC: 25Gb)|5, 4, 7, 1, 2|5|
|Intel (NIC: 25Gb)|5, 4, 7, 1, 2|5|
|Intel (NIC: 25Gb)|6, 3|2|
|Broadcom (Emulex) (HBA: FC64)|5, 4, 7, 1, 2|5|
|Broadcom (Emulex) (HBA: FC64)|6, 3|2|
|Broadcom (Emulex) (HBA: FC32)|5, 4, 7, 1, 2|5|
|Broadcom (Emulex) (HBA: FC32)|6, 3|2|
|Broadcom (Emulex) (HBA: FC32)|5, 4, 7, 1, 2|5|
|Broadcom (Emulex) (HBA: FC32)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|5, 4, 7, 1, 2|5|
|Qlogic (Marvell) (HBA: FC32)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|5, 4, 7, 1, 2|5|
|Qlogic (Marvell) (HBA: FC32)|6, 3|2|
|Intel (NIC: 100Gb)|6, 3|2|
|Broadcom (NIC: 25Gb)|5, 4, 7, 1, 2|5|
|Broadcom (NIC: 25Gb)|6, 3|2|
|Broadcom (NIC: 10Gb)|5, 4, 7, 1, 2|5|
|Broadcom (NIC: 10Gb)|6, 3|2|
|Broadcom (NIC: 10Gb)|5, 4, 7, 1, 6, 3, 2|7|
|Broadcom (NIC: 10Gb)|5, 4, 7, 1, 2|5|
|Broadcom (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 10Gb)|5, 4, 7, 1, 2|5|
|Intel (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 10Gb)|5, 4, 7, 1, 2|5|
|Intel (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 1Gb)|5, 4, 7, 1, 2|5|
|Intel (NIC: 1Gb)|6, 3|2|
|Intel (NIC: 1Gb)|5, 4, 7, 1, 2|5|
|Intel (NIC: 1Gb)|6, 3|2|
|Broadcom (NIC: 1Gb)|5, 4, 7, 1, 2|5|
|Broadcom (NIC: 1Gb)|6, 3|2|
|Foxconn (HBA355e DIB External<br>Adapter)|5, 4, 7, 1, 6, 3, 2|7|
|Foxconn (H965e DIB External Adapter)|6, 3|1|

**Table 156. Configuration 2: R1Q+R2A+R3B+R4Q**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Inventec (VGA)|8, 4|1|
|Inventec (Serial)|8, 4|1|
|Mellanox (NVIDIA) (Channel DPU: 25Gb)|5, 4, 7, 1, 2|5|
|Inventec (LOM Card)|Integrated slot|1|
|Broadcom (OCP: 100Gb)|Integrated slot|1|
|Mellanox (OCP: 100Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Mellanox (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Foxconn (BOSS)|Integrated slot|1|
|Foxconn (Front PERC12 HBA465i)|Integrated slot|1|
|Foxconn (Front PERC12 H965i)|Integrated slot|2|
|Foxconn (Front PERC11 H755N)|Integrated slot|2|
|Foxconn (Front PERC11 H755)|Integrated slot|1|
|Foxconn (Front PERC11 H355)|Integrated slot|1|
|Foxconn (Front PERC11 HBA355i)|Integrated slot|1|
|Foxconn (PERC Adapter12 HBA465i)|3|1|
|Foxconn (PERC Adapter12 H965i)|3|1|
|Foxconn (PERC Adapter11 H755)|3|1|
|Foxconn (PERC Adapter11 HBA355i)|3|1|
|Foxconn (PERC Adapter11 H355)|3|1|
|Intel (GPU ATS-M)|7, 8, 4, 5, 1, 2|6|
|NVIDIA (GPU A2)|7, 8, 4, 5, 1, 2|6|
|Mellanox (LP NIC: NDR200)|6, 3|2|
|Mellanox (NIC: HDR100 VPI)|6, 3|2|
|Mellanox (LP NIC: HDR VPI), 2P|6, 3|2|
|Mellanox (LP NIC: HDR VPI), 1P|6, 3|2|
|Mellanox (NIC: 100Gb)|6, 3|2|
|Broadcom (NIC: 100Gb)|6, 3|2|
|Intel (NIC: 100Gb)|6, 3|2|
|Broadcom (NIC: 25Gb)|5, 4, 7, 1, 2|5|
|Mellanox (NIC: 25Gb)|5, 4, 7, 1, 2|5|
|Mellanox (NIC: 25Gb)|6, 3|2|
|Intel (NIC: 25Gb)|5, 4, 7, 1, 2|5|
|Intel (NIC: 25Gb)|5, 4, 7, 1, 2|5|
|Intel (NIC: 25Gb)|6, 3|2|
|Broadcom (Emulex) (HBA: FC64)|5, 4, 7, 1, 2|5|
|Broadcom (Emulex) (HBA: FC64)|6, 3|2|

**Table 156. Configuration 2: R1Q+R2A+R3B+R4Q (continued)**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Broadcom (Emulex) (HBA: FC32)|5, 4, 7, 1, 2|5|
|Broadcom (Emulex) (HBA: FC32)|6, 3|2|
|Broadcom (Emulex) (HBA: FC32)|5, 4, 7, 1, 2|5|
|Broadcom (Emulex) (HBA: FC32)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|5, 4, 7, 1, 2|5|
|Qlogic (Marvell) (HBA: FC32)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|5, 4, 7, 1, 2|5|
|Qlogic (Marvell) (HBA: FC32)|6, 3|2|
|Intel (NIC: ACC100)|6, 3|2|
|Broadcom (NIC: 25Gb)|5, 4, 7, 1, 2|5|
|Broadcom (NIC: 25Gb)|6, 3|2|
|Broadcom (NIC: 10Gb)|5, 4, 7, 1, 2|5|
|Broadcom (NIC: 10Gb)|6, 3|2|
|Broadcom (NIC: 10Gb)|5, 4, 7, 1, 6, 3, 2|7|
|Broadcom (NIC: 10Gb)|5, 4, 7, 1, 2|5|
|Broadcom (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 10Gb)|5, 4, 7, 1, 2|5|
|Intel (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 10Gb)|5, 4, 7, 1, 2|5|
|Intel (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 1Gb)|5, 4, 7, 1, 2|5|
|Intel (NIC: 1Gb)|6, 3|2|
|Intel (NIC: 1Gb)|5, 4, 7, 1, 2|5|
|Intel (NIC: 1Gb)|6, 3|2|
|Broadcom (NIC: 1Gb)|5, 4, 7, 1, 2|5|
|Broadcom (NIC: 1Gb)|6, 3|2|
|Foxconn (HBA355e DIB External<br>Adapter)|5, 4, 7, 1, 6, 3, 2|7|
|Foxconn (H965e DIB External Adapter)|6, 3|1|
|Inventec (VGA)|4|1|
|Inventec (Serial)|4|1|
|Mellanox (NVIDIA) (Dell DPU: 100Gb)|2|1|
|Pensando (Dell DPU: 100Gb)|2|1|
|Mellanox (NVIDIA) (Channel DPU: 25Gb)|5, 4, 7, 2|4|
|Pensando (Dell DPU: 25Gb)|2|1|

**Table 157. Configuration 3-1: R1P+R2A+R3B+R4P (HL) (continued)**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Mellanox (NVIDIA) (Dell DPU: 25Gb)|2|1|
|Inventec (MIC Card)|Integrated slot|1|
|Inventec (LOM Card)|Integrated slot|1|
|Broadcom (OCP: 100Gb)|Integrated slot|1|
|Mellanox (OCP: 100Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Mellanox (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Foxconn (BOSS)|Integrated slot|1|
|Foxconn (Front PERC12 HBA465i)|Integrated slot|1|
|Foxconn (Front PERC12 H965i)|Integrated slot|2|
|Foxconn (Front PERC11 H755N)|Integrated slot|2|
|Foxconn (Front PERC11 H755)|Integrated slot|1|
|Foxconn (Front PERC11 H355)|Integrated slot|1|
|Foxconn (Front PERC11 HBA355i)|Integrated slot|1|
|Foxconn (PERC Adapter12 HBA465i)|3|1|
|Foxconn (PERC Adapter12 H965i)|3|1|
|Foxconn (PERC Adapter11 H755)|3|1|
|Foxconn (PERC Adapter11 HBA355i)|3|1|
|Foxconn (PERC Adapter11 H355)|3|1|
|NVIDIA (GPU L4)|7, 2|2|
|Intel (GPU ATS-M)|7, 4, 5, 2|4|
|NVIDIA (GPU A2)|7, 4, 5, 2|4|
|Mellanox (FH NIC: NDR200)|7, 2|2|
|Mellanox (LP NIC: NDR200)|6, 3|2|
|Mellanox (NIC: NDR400)|7, 2|2|
|Mellanox (NIC: HDR100 VPI)|7, 2|2|
|Mellanox (NIC: HDR100 VPI)|6, 3|2|
|Mellanox (FH NIC: HDR VPI), 2P|7, 2|2|
|Mellanox (LP NIC: HDR VPI), 2P|6, 3|2|
|Mellanox (FH NIC: HDR VPI), 1P|7, 2|2|
|Mellanox (LP NIC: HDR VPI), 1P|6, 3|2|
|Mellanox (NIC: 100Gb)|7, 2|2|
|Mellanox (NIC: 100Gb)|6, 3|2|
|Broadcom (NIC: 100Gb)|6, 3|2|
|Intel (NIC: 100Gb)|2|2|
|Intel (NIC: 100Gb)|6, 3|2|
|Intel (NIC: 100Gb)|7, 2|2|
|Intel (NIC: 25Gb)|7, 2|2|
|Intel (NIC: 25Gb)|7, 2|2|
|Broadcom (NIC: 25Gb)|5, 4, 7, 2|4|
|Intel (NIC: 10Gb)|7, 2|2|
|Mellanox (NIC: 25Gb)|5, 4, 7, 2|4|
|Mellanox (NIC: 25Gb)|6, 3|2|
|Intel (NIC: 25Gb)|5, 4, 7, 2|4|
|Intel (NIC: 25Gb)|5, 4, 7, 2|4|
|Intel (NIC: 25Gb)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|7, 2|2|
|Broadcom (Emulex) (HBA: FC64)|5, 4, 7, 2|4|
|Broadcom (Emulex) (HBA: FC64)|6, 3|2|
|Broadcom (Emulex) (HBA: FC32)|5, 4, 7, 2|4|
|Broadcom (Emulex) (HBA: FC32)|6, 3|2|
|Broadcom (Emulex) (HBA: FC32)|5, 4, 7, 2|4|
|Broadcom (Emulex) (HBA: FC32)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|5, 4, 7, 2|4|
|Qlogic (Marvell) (HBA: FC32)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|5, 4, 7, 2|4|
|Qlogic (Marvell) (HBA: FC32)|6, 3|2|
|Intel (NIC: ACC100)|6, 3|2|
|Broadcom (NIC: 25Gb)|5, 4, 7, 2|4|
|Broadcom (NIC: 25Gb)|6, 3|2|
|Broadcom (NIC: 10Gb)|5, 4, 7, 2|4|
|Broadcom (NIC: 10Gb)|6, 3|2|
|Broadcom (NIC: 10Gb)|5, 4, 7, 6, 3, 2|6|
|Broadcom (NIC: 10Gb)|5, 4, 7, 2|4|
|Broadcom (NIC: 10Gb)|6, 3|2|

**Table 157. Configuration 3-1: R1P+R2A+R3B+R4P (HL) (continued)**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Intel (NIC: 10Gb)|5, 4, 7, 2|4|
|Intel (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 10Gb)|5, 4, 7, 2|4|
|Intel (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 1Gb)|5, 4, 7, 2|4|
|Intel (NIC: 1Gb)|6, 3|2|
|Intel (NIC: 1Gb)|5, 4, 7, 2|4|
|Intel (NIC: 1Gb)|6, 3|2|
|Broadcom (NIC: 1Gb)|5, 4, 7, 2|4|
|Broadcom (NIC: 1Gb)|6, 3|2|
|Foxconn (HBA355e DIB External<br>Adapter)|5, 4, 7, 6, 3, 2|6|
|Foxconn (H965e DIB External Adapter)|7, 6, 3, 2|1|
|Inventec (VGA)|4|1|
|Inventec (Serial)|4|1|
|Intel (Dell DPU: 200Gb)|2|1|
|Mellanox (NVIDIA) (Dell DPU: 100Gb)|2|1|
|Pensando (Dell DPU: 100Gb)|2|1|
|Mellanox (NVIDIA) (Channel DPU: 25Gb)|5, 4, 7, 2|4|
|Pensando (Dell DPU: 25Gb)|2|1|
|Mellanox (NVIDIA) (Dell DPU: 25Gb)|2|1|
|Inventec (MIC Card)|Integrated slot|1|
|Inventec (LOM Card)|Integrated slot|1|
|Broadcom (OCP: 100Gb)|Integrated slot|1|
|Mellanox (OCP: 100Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Mellanox (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|

**Table 158. Configuration 3-2: R1P+R2A+R3B+R4P (FL) (continued)**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Foxconn (BOSS)|Integrated slot|1|
|Foxconn (Front PERC12 HBA465i)|Integrated slot|1|
|Foxconn (Front PERC12 H965i)|Integrated slot|2|
|Foxconn (Front PERC11 H755N)|Integrated slot|2|
|Foxconn (Front PERC11 H755)|Integrated slot|1|
|Foxconn (Front PERC11 H355)|Integrated slot|1|
|Foxconn (Front PERC11 HBA355i)|Integrated slot|1|
|Foxconn (PERC Adapter12 HBA465i)|3|1|
|Foxconn (PERC Adapter12 H965i)|3|1|
|Foxconn (PERC Adapter11 H755)|3|1|
|Foxconn (PERC Adapter11 HBA355i)|3|1|
|Foxconn (PERC Adapter11 H355)|3|1|
|Intel (GPU PVC)|7, 2|2|
|NVIDIA (GPU H100)|7, 2|2|
|NVIDIA (GPU L40S)|7, 2|2|
|NVIDIA (GPU L40)|7, 2|2|
|NVIDIA (GPU A40)|7, 2|2|
|NVIDIA (GPU A800)|7, 2|2|
|NVIDIA (GPU A100)|7, 2|2|
|NVIDIA (GPU A30)|7, 2|2|
|NVIDIA (GPU A16)|7, 2|2|
|NVIDIA (GPU L4)|7, 2|2|
|Intel (GPU ATS-M)|7, 4, 5, 2|4|
|NVIDIA (GPU A2)|7, 4, 5, 2|4|
|Mellanox (FH NIC: NDR200)|7, 2|2|
|Mellanox (LP NIC: NDR200)|6, 3|2|
|Mellanox (NIC: NDR400)|7, 2|2|
|Mellanox (NIC: HDR100 VPI)|7, 2|2|
|Mellanox (NIC: HDR100 VPI)|6, 3|2|
|Mellanox (FH NIC: HDR VPI), 2P|7, 2|2|
|Mellanox (LP NIC: HDR VPI), 2P|6, 3|2|
|Mellanox (FH NIC: HDR VPI), 1P|7, 2|2|
|Mellanox (LP NIC: HDR VPI), 1P|6, 3|2|
|Mellanox (NIC: 100Gb)|7, 2|2|
|Mellanox (NIC: 100Gb)|6, 3|2|
|Broadcom (NIC: 100Gb)|6, 3|2|
|Intel (NIC: 100Gb)|2|2|
|Intel (NIC: 100Gb)|6, 3|2|
|Intel (NIC: 100Gb)|7, 2|2|
|Intel (NIC: 25Gb)|7, 2|2|
|Intel (NIC: 25Gb)|7, 2|2|
|Broadcom (NIC: 25Gb)|5, 4, 7, 2|4|
|Mellanox (NIC: 25Gb)|5, 4, 7, 2|4|
|Mellanox (NIC: 25Gb)|6, 3|2|
|Intel (NIC: 25Gb)|5, 4, 7, 2|4|
|Intel (NIC: 25Gb)|5, 4, 7, 2|4|
|Intel (NIC: 25Gb)|6, 3|2|
|Broadcom (Emulex) (HBA: FC64)|5, 4, 7, 2|4|
|Broadcom (Emulex) (HBA: FC64)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|7, 2|2|
|Broadcom (Emulex) (HBA: FC32)|5, 4, 7, 2|4|
|Broadcom (Emulex) (HBA: FC32)|6, 3|2|
|Broadcom (Emulex) (HBA: FC32)|5, 4, 7, 2|4|
|Broadcom (Emulex) (HBA: FC32)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|5, 4, 7, 2|4|
|Qlogic (Marvell) (HBA: FC32)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|5, 4, 7, 2|4|
|Qlogic (Marvell) (HBA: FC32)|6, 3|2|
|Intel (NIC: ACC100)|6, 3|2|
|Broadcom (NIC: 25Gb)|5, 4, 7, 2|4|
|Broadcom (NIC: 25Gb)|6, 3|2|
|Broadcom (NIC: 10Gb)|5, 4, 7, 2|4|
|Broadcom (NIC: 10Gb)|6, 3|2|
|Broadcom (NIC: 10Gb)|5, 4, 7, 6, 3, 2|6|
|Broadcom (NIC: 10Gb)|5, 4, 7, 2|4|
|Broadcom (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 10Gb)|5, 4, 7, 2|4|
|Intel (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 10Gb)|5, 4, 7, 2|4|
|Intel (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 1Gb)|5, 4, 7, 2|4|
|Intel (NIC: 1Gb)|6, 3|2|
|Intel (NIC: 1Gb)|5, 4, 7, 2|4|

**Table 158. Configuration 3-2: R1P+R2A+R3B+R4P (FL) (continued)**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Intel (NIC: 1Gb)|6, 3|2|
|Broadcom (NIC: 1Gb)|5, 4, 7, 2|4|
|Broadcom (NIC: 1Gb)|6, 3|2|
|Foxconn (HBA355e DIB External<br>Adapter)|5, 4, 7, 6, 3, 2|6|
|Foxconn (H965e DIB External Adapter)|7, 6, 3, 2|1|
|Inventec (VGA)|8, 4|1|
|Inventec (Serial)|8, 4|1|
|Mellanox (NVIDIA) (Channel DPU: 25Gb)|5, 4, 2|3|
|Inventec (LOM Card)|Integrated slot|1|
|Broadcom (OCP: 100Gb)|Integrated slot|1|
|Mellanox (OCP: 100Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Mellanox (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Foxconn (BOSS)|Integrated slot|1|
|Foxconn (Front PERC12 HBA465i)|Integrated slot|1|
|Foxconn (Front PERC12 H965i)|Integrated slot|2|
|Foxconn (Front PERC11 H755N)|Integrated slot|2|
|Foxconn (Front PERC11 H755)|Integrated slot|1|
|Foxconn (Front PERC11 H355)|Integrated slot|1|
|Foxconn (Front PERC11 HBA355i)|Integrated slot|1|
|Foxconn (PERC Adapter12 HBA465i)|3|1|
|Foxconn (PERC Adapter12 H965i)|3|1|
|Foxconn (PERC Adapter11 H755)|3|1|
|Foxconn (PERC Adapter11 HBA355i)|3|1|
|Foxconn (PERC Adapter11 H355)|3|1|

**Table 159. Configuration 4-1: R1P+R2A+R3B+R4R (HL) (continued)**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|NVIDIA (GPU L4)|2|1|
|Intel (GPU ATS-M)|8, 4, 5, 2|4|
|NVIDIA (GPU A2)|8, 4, 5, 2|4|
|Mellanox (FH NIC: NDR200)|7|1|
|Mellanox (LP NIC: NDR200)|6, 3|2|
|Mellanox (NIC: NDR400)|7|1|
|Mellanox (NIC: HDR100 VPI)|7|1|
|Mellanox (NIC: HDR100 VPI)|6, 3|2|
|Mellanox (FH NIC: HDR VPI), 2P|2|1|
|Mellanox (LP NIC: HDR VPI), 2P|6, 3|2|
|Mellanox (FH NIC: HDR VPI), 1P|7|1|
|Mellanox (LP NIC: HDR VPI), 1P|6, 3|2|
|Mellanox (NIC: 100Gb)|2|1|
|Mellanox (NIC: 100Gb)|6, 3|2|
|Broadcom (NIC: 100Gb)|6, 3|2|
|Intel (NIC: 100Gb)|2|1|
|Intel (NIC: 100Gb)|6, 3|2|
|Intel (NIC: 100Gb)|2|1|
|Intel (NIC: 25Gb)|2|1|
|Intel (NIC: 25Gb)|2|1|
|Broadcom (NIC: 25Gb)|5, 4, 2|1|
|Mellanox (NIC: 25Gb)|5, 4, 2|3|
|Mellanox (NIC: 25Gb)|6, 3|2|
|Intel (NIC: 25Gb)|5, 4, 2|3|
|Intel (NIC: 25Gb)|5, 4, 2|3|
|Intel (NIC: 25Gb)|6, 3|2|
|Broadcom (Emulex) (HBA: FC64)|5, 4, 2|3|
|Broadcom (Emulex) (HBA: FC64)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|2|1|
|Broadcom (Emulex) (HBA: FC32)|5, 4, 2|3|
|Broadcom (Emulex) (HBA: FC32)|6, 3|2|
|Broadcom (Emulex) (HBA: FC32)|5, 4, 2|3|
|Broadcom (Emulex) (HBA: FC32)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|5, 4, 2|3|
|Qlogic (Marvell) (HBA: FC32)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|5, 4, 2|3|
|Qlogic (Marvell) (HBA: FC32)|6, 3|2|
|Intel (NIC: ACC100)|6, 3|2|
|Broadcom (NIC: 25Gb)|5, 4, 2|3|
|Broadcom (NIC: 25Gb)|6, 3|2|
|Broadcom (NIC: 10Gb)|5, 4, 2|3|
|Broadcom (NIC: 10Gb)|6, 3|2|
|Broadcom (NIC: 10Gb)|5, 4, 6, 3, 2|5|
|Broadcom (NIC: 10Gb)|5, 4, 2|3|
|Broadcom (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 10Gb)|5, 4, 2|3|
|Intel (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 10Gb)|5, 4, 2|3|
|Intel (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 1Gb)|5, 4, 2|3|
|Intel (NIC: 1Gb)|6, 3|2|
|Intel (NIC: 1Gb)|5, 4, 2|3|
|Intel (NIC: 1Gb)|6, 3|2|
|Broadcom (NIC: 1Gb)|5, 4, 2|3|
|Broadcom (NIC: 1Gb)|6, 3|2|
|Foxconn (HBA355e DIB External<br>Adapter)|5, 4, 6, 3, 2|5|
|Foxconn (H965e DIB External Adapter)|6, 3, 2|1|

**Table 160. Configuration 5-1: R1R+R2A+R3A+R4P (HL)**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Inventec (VGA)|4, 8|1|
|Inventec (Serial)|4, 8|1|
|Intel (Dell DPU: 200Gb)|2|1|
|Mellanox (NVIDIA) (Dell DPU: 100Gb)|2|1|
|Pensando (Dell DPU: 100Gb)|2|1|
|Mellanox (NVIDIA) (Channel DPU: 25Gb)|5, 7, 1, 2|4|
|Pensando (Dell DPU: 25Gb)|2|1|
|Mellanox (NVIDIA) (Dell DPU: 25Gb)|2|1|
|Broadcom (OCP: 100Gb)|Integrated slot|1|
|Mellanox (OCP: 100Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Mellanox (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Foxconn (BOSS)|Integrated slot|1|
|Foxconn (Front PERC12 HBA465i)|Integrated slot|1|
|Foxconn (Front PERC12 H965i)|Integrated slot|2|
|Foxconn (Front PERC11 H755N)|Integrated slot|2|
|Foxconn (Front PERC11 H755)|Integrated slot|1|
|Foxconn (Front PERC11 H355)|Integrated slot|1|
|Foxconn (Front PERC11 HBA355i)|Integrated slot|1|
|Foxconn (PERC Adapter12 HBA465i)|3|1|
|Foxconn (PERC Adapter12 H965i)|3|1|
|Foxconn (PERC Adapter11 H755)|3|1|
|Foxconn (PERC Adapter11 HBA355i)|3|1|
|Foxconn (PERC Adapter11 H355)|3|1|
|NVIDIA (GPU L4)|7, 5, 1, 2|4|
|Intel (GPU ATS-M)|7, 5, 1, 2|4|
|NVIDIA (GPU A2)|7, 5, 1, 2|4|
|Mellanox (FH NIC: NDR200)|5, 7, 1, 2|4|
|Mellanox (LP NIC: NDR200)|6, 3|2|
|Mellanox (NIC: NDR400)|7, 2|2|
|Mellanox (NIC: HDR100 VPI)|5, 7, 1, 2|4|
|Mellanox (NIC: HDR100 VPI)|6, 3|2|
|Mellanox (FH NIC: HDR VPI), 2P|5, 7, 1, 2|4|
|Mellanox (LP NIC: HDR VPI), 2P|6, 3|2|
|Mellanox (FH NIC: HDR VPI), 1P|5, 7, 1, 2|4|
|Mellanox (LP NIC: HDR VPI), 1P|6, 3|2|
|Mellanox (NIC: 100Gb)|5, 7, 1, 2|4|
|Mellanox (NIC: 100Gb)|6, 3|2|
|Broadcom (NIC: 100Gb)|5, 1|2|
|Broadcom (NIC: 100Gb)|6, 3|2|

**Table 160. Configuration 5-1: R1R+R2A+R3A+R4P (HL) (continued)**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Intel (NIC: 100Gb)|5, 7, 1, 2|4|
|Intel (NIC: 100Gb)|6, 3|2|
|Intel (NIC: 100Gb)|5, 7, 1, 2|4|
|Intel (NIC: 25Gb)|5, 7, 1, 2|4|
|Intel (NIC: 25Gb)|5, 7, 1, 2|4|
|Broadcom (NIC: 25Gb)|5, 7, 1, 2|4|
|Intel (NIC: 10Gb)|5, 7, 1, 2|4|
|Mellanox (NIC: 25Gb)|5, 7, 1, 2|4|
|Mellanox (NIC: 25Gb)|6, 3|2|
|Intel (NIC: 25Gb)|5, 7, 1, 2|4|
|Intel (NIC: 25Gb)|5, 7, 1, 2|4|
|Intel (NIC: 25Gb)|6, 3|2|
|Broadcom (Emulex) (HBA: FC64)|5, 7, 1, 2|4|
|Broadcom (Emulex) (HBA: FC64)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|5, 7, 2|3|
|Broadcom (Emulex) (HBA: FC32)|5, 7, 1, 2|4|
|Broadcom (Emulex) (HBA: FC32)|6, 3|2|
|Broadcom (Emulex) (HBA: FC32)|5, 7, 1, 2|4|
|Broadcom (Emulex) (HBA: FC32)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|5, 7, 2|4|
|Qlogic (Marvell) (HBA: FC32)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|5, 7, 1, 2|4|
|Qlogic (Marvell) (HBA: FC32)|6, 3|2|
|CORNELIS (NIC: 100Gb)|6, 3|2|
|Intel (NIC: ACC100)|6, 3|2|
|Broadcom (NIC: 25Gb)|5, 7, 1, 2|4|
|Broadcom (NIC: 25Gb)|6, 3|2|
|Broadcom (NIC: 10Gb)|5, 7, 1, 2|4|
|Broadcom (NIC: 10Gb)|6, 3|2|
|Broadcom (NIC: 10Gb)|5, 7, 1, 6, 3, 2|6|
|Broadcom (NIC: 10Gb)|5, 7, 1, 2|4|
|Broadcom (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 10Gb)|5, 7, 1, 2|4|
|Intel (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 10Gb)|5, 7, 1, 2|4|
|Intel (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 1Gb)|5, 7, 1, 2|4|
|Intel (NIC: 1Gb)|6, 3|2|
|Intel (NIC: 1Gb)|5, 7, 1, 2|4|
|Intel (NIC: 1Gb)|6, 3|2|
|Broadcom (NIC: 1Gb)|5, 7, 1, 2|4|
|Broadcom (NIC: 1Gb)|6, 3|2|
|Foxconn (HBA355e DIB External<br>Adapter)|5, 7, 1, 6, 3, 2|6|
|Foxconn (H965e DIB External Adapter)|5, 7, 1, 6, 3, 2|1|

**Table 161. Configuration 5-2: R1R+R2A+R3A+R4P (FL)**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Inventec (VGA)|4|1|
|Inventec (Serial)|4|1|
|Intel (Dell DPU: 200Gb)|2|1|
|Mellanox (NVIDIA) (Dell DPU: 100Gb)|2|1|
|Pensando (Dell DPU: 100Gb)|2|1|
|Mellanox (NVIDIA) (Channel DPU: 25Gb)|5, 7, 1, 2|4|
|Pensando (Dell DPU: 25Gb)|2|1|
|Mellanox (NVIDIA) (Dell DPU: 25Gb)|2|1|
|Inventec (MIC Card)|Integrated slot|1|
|Inventec (LOM Card)|Integrated slot|1|
|Broadcom (OCP: 100Gb)|Integrated slot|1|
|Mellanox (OCP: 100Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Mellanox (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Foxconn (BOSS)|Integrated slot|1|
|Foxconn (Front PERC12 HBA465i)|Integrated slot|1|
|Foxconn (Front PERC12 H965i)|Integrated slot|2|
|Foxconn (Front PERC11 H755N)|Integrated slot|2|
|Foxconn (Front PERC11 H755)|Integrated slot|1|
|Foxconn (Front PERC11 H355)|Integrated slot|1|
|Foxconn (Front PERC11 HBA355i)|Integrated slot|1|
|Foxconn (PERC Adapter12 HBA465i)|3|1|
|Foxconn (PERC Adapter12 H965i)|3|1|
|Foxconn (PERC Adapter11 H755)|3|1|
|Foxconn (PERC Adapter11 HBA355i)|3|1|
|Foxconn (PERC Adapter11 H355)|3|1|
|Intel (GPU PVC)|7|1|
|NVIDIA (GPU H100)|7|1|
|NVIDIA (GPU L40S)|7|1|
|NVIDIA (GPU L40)|7|1|
|NVIDIA (GPU A40)|7|1|
|NVIDIA (GPU A800)|7|1|
|NVIDIA (GPU A100)|7|1|
|NVIDIA (GPU A30)|7|1|
|NVIDIA (GPU A16)|7|1|
|NVIDIA (GPU L4)|7, 5, 1, 2|4|
|Intel (GPU ATS-M)|7, 5, 1, 2|4|
|NVIDIA (GPU A2)|7, 5, 1, 2|4|
|Mellanox (FH NIC: NDR200)|5, 7, 1, 2|4|
|Mellanox (LP NIC: NDR200)|6, 3|2|
|Mellanox (NIC: NDR400)|7, 2|2|
|Mellanox (NIC: HDR100 VPI)|5, 7, 1, 2|4|
|Mellanox (NIC: HDR100 VPI)|6, 3|2|
|Mellanox (FH NIC: HDR VPI), 2P|5, 7, 1, 2|4|
|Mellanox (LP NIC: HDR VPI), 2P|6, 3|2|
|Mellanox (FH NIC: HDR VPI), 1P|5, 7, 1, 2|4|
|Mellanox (LP NIC: HDR VPI), 1P|6, 3|2|
|Mellanox (NIC: 100Gb)|5, 7, 1, 2|4|
|Mellanox (NIC: 100Gb)|6, 3|2|
|Broadcom (NIC: 100Gb)|5, 1|2|
|Broadcom (NIC: 100Gb)|6, 3|2|
|Intel (NIC: 100Gb)|5, 7, 1, 2|4|
|Intel (NIC: 100Gb)|6, 3|2|

**Table 161. Configuration 5-2: R1R+R2A+R3A+R4P (FL) (continued)**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Intel (NIC: 100Gb)|5, 7, 1, 2|4|
|Intel (NIC: 25Gb)|5, 7, 1, 2|4|
|Intel (NIC: 25Gb)|5, 7, 1, 2|4|
|Broadcom (NIC: 25Gb)|5, 7, 1, 2|4|
|Intel (NIC: 10Gb)|5, 7, 1, 2|4|
|Mellanox (NIC: 25Gb)|5, 7, 1, 2|4|
|Mellanox (NIC: 25Gb)|6, 3|2|
|Intel (NIC: 25Gb)|5, 7, 1, 2|4|
|Intel (NIC: 25Gb)|5, 7, 1, 2|4|
|Intel (NIC: 25Gb)|6, 3|2|
|Broadcom (Emulex) (HBA: FC64)|5, 7, 1, 2|4|
|Broadcom (Emulex) (HBA: FC64)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|5, 7, 2|3|
|Broadcom (Emulex) (HBA: FC32)|5, 7, 1, 2|4|
|Broadcom (Emulex) (HBA: FC32)|6, 3|2|
|Broadcom (Emulex) (HBA: FC32)|5, 7, 1, 2|4|
|Broadcom (Emulex) (HBA: FC32)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|5, 7, 2|4|
|Qlogic (Marvell) (HBA: FC32)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|5, 7, 1, 2|4|
|Qlogic (Marvell) (HBA: FC32)|6, 3|2|
|CORNELIS (NIC: 100Gb)|6, 3|2|
|Intel (NIC: (ACC100)|6, 3|2|
|Broadcom (NIC: 25Gb)|5, 7, 1, 2|4|
|Broadcom (NIC: 25Gb)|6, 3|2|
|Broadcom (NIC: 10Gb)|5, 7, 1, 2|4|
|Broadcom (NIC: 10Gb)|6, 3|2|
|Broadcom (NIC: 10Gb)|5, 7, 1, 6, 3, 2|6|
|Broadcom (NIC: 10Gb)|5, 7, 1, 2|4|
|Broadcom (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 10Gb)|5, 7, 1, 2|4|
|Intel (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 10Gb)|5, 7, 1, 2|4|
|Intel (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 1Gb)|5, 7, 1, 2|4|
|Intel (NIC: 1Gb)|6, 3|2|
|Intel (NIC: 1Gb)|5, 7, 1, 2|4|
|Intel (NIC: 1Gb)|6, 3|2|
|Broadcom (NIC: 1Gb)|5, 7, 1, 2|4|
|Broadcom (NIC: 1Gb)|6, 3|2|
|Foxconn (HBA355e DIB External<br>Adapter)|5, 7, 1, 6, 3, 2|6|
|Foxconn (H965e DIB External Adapter)|5, 7, 1, 6, 3, 2|1|

**Table 162. Configuration 6: R2A+R4Q**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Inventec (VGA)|8|1|
|Inventec (Serial)|8|1|
|Mellanox (NVIDIA) (Channel DPU: 25Gb)|7|1|
|Inventec (LOM Card)|Integrated slot|1|
|Broadcom (OCP: 100Gb)|Integrated slot|1|
|Mellanox (OCP: 100Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Mellanox (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Foxconn (BOSS)|Integrated slot|1|
|Foxconn (Front PERC12 HBA465i)|Integrated slot|1|
|Foxconn (Front PERC12 H965i)|Integrated slot|2|
|Foxconn (Front PERC11 H755)|Integrated slot|1|
|Foxconn (Front PERC11 H355)|Integrated slot|1|
|Foxconn (Front PERC11 HBA355i)|Integrated slot|1|
|Foxconn (PERC Adapter12 HBA465i)|3|1|
|Foxconn (PERC Adapter12 H965i)|3|1|
|Foxconn (PERC Adapter11 H755)|3|1|
|Foxconn (PERC Adapter11 HBA355i)|3|1|
|Foxconn (PERC Adapter11 H355)|3|1|
|Mellanox (LP NIC: NDR200)|6, 3|2|
|Mellanox (NIC: HDR100 VPI)|3, 6|2|
|Mellanox (LP NIC: HDR VPI), 2P|3, 6|2|
|Mellanox (LP NIC: HDR VPI), 1P|3, 6|2|
|Mellanox (NIC: 100Gb)|3, 6|2|
|Broadcom (NIC: 100Gb)|3, 6|2|
|Intel (NIC: 100Gb)|3, 6|2|
|Broadcom (NIC: 25Gb)|7|1|
|Mellanox (NIC: 25Gb)|7|1|
|Mellanox (NIC: 25Gb)|3, 6|2|
|Intel (NIC: 25Gb)|7|1|
|Intel (NIC: 25Gb)|7|1|
|Intel (NIC: 25Gb)|3, 6|2|
|Broadcom (Emulex) (HBA: FC64)|7|1|
|Broadcom (Emulex) (HBA: FC64)|3, 6|2|
|Broadcom (Emulex) (HBA: FC32)|7|1|
|Broadcom (Emulex) (HBA: FC32)|3, 6|2|
|Broadcom (Emulex) (HBA: FC32)|7|1|
|Broadcom (Emulex) (HBA: FC32)|3, 6|2|
|Qlogic (Marvell) (HBA: FC32)|7|1|
|Qlogic (Marvell) (HBA: FC32)|3, 6|2|
|Qlogic (Marvell) (HBA: FC32)|7|1|
|Qlogic (Marvell) (HBA: FC32)|3, 6|2|
|Intel (NIC: (ACC100)|3, 6|2|
|Broadcom (NIC: 25Gb)|7|1|
|Broadcom (NIC: 25Gb)|3, 6|2|
|Broadcom (NIC: 10Gb)|7|1|
|Broadcom (NIC: 10Gb)|3, 6|2|
|Broadcom (NIC: 10Gb)|7, 3, 6|3|
|Broadcom (NIC: 10Gb)|7|1|
|Broadcom (NIC: 10Gb)|3, 6|2|
|Intel (NIC: 10Gb)|7|1|
|Intel (NIC: 10Gb)|3, 6|2|
|Intel (NIC: 10Gb)|7|1|
|Intel (NIC: 10Gb)|3, 6|2|
|Intel (NIC: 1Gb)|7|1|

**Table 162. Configuration 6: R2A+R4Q (continued)**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Intel (NIC: 1Gb)|3, 6|2|
|Intel (NIC: 1Gb)|7|1|
|Intel (NIC: 1Gb)|3, 6|2|
|Broadcom (NIC: 1Gb)|7|1|
|Broadcom (NIC: 1Gb)|3, 6|2|
|Foxconn (HBA355e DIB External<br>Adapter)|7, 3, 6|3|
|Foxconn (H965e DIB External Adapter)|3, 6|1|
|Inventec (VGA)|8|1|
|Inventec (Serial)|8|1|
|Mellanox (NVIDIA) (Channel DPU: 25Gb)|7, 1, 2|3|
|Inventec (LOM Card)|Integrated slot|1|
|Broadcom (OCP: 100Gb)|Integrated slot|1|
|Mellanox (OCP: 100Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Mellanox (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Foxconn (BOSS)|Integrated slot|1|
|Foxconn (Front PERC12 HBA465i)|Integrated slot|1|
|Foxconn (Front PERC12 H965i)|Integrated slot|2|
|Foxconn (Front PERC11 H755)|Integrated slot|1|
|Foxconn (Front PERC11 H355)|Integrated slot|1|
|Foxconn (Front PERC11 HBA355i)|Integrated slot|1|
|Foxconn (PERC Adapter12 HBA465i)|3|1|
|Foxconn (PERC Adapter12 H965i)|3|1|

**Table 163. Configuration 7: R1Q+R2A+R4Q (continued)**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Foxconn (PERC Adapter11 H755)|3|1|
|Foxconn (PERC Adapter11 HBA355i)|3|1|
|Foxconn (PERC Adapter11 H355)|3|1|
|Mellanox (LP NIC: NDR200)|6, 3|2|
|Mellanox (NIC: HDR100 VPI)|6, 3|2|
|Mellanox (LP NIC: HDR VPI), 2P|6, 3|2|
|Mellanox (LP NIC: HDR VPI), 1P|6, 3|2|
|Mellanox (NIC: 100Gb)|6, 3|2|
|Broadcom (NIC: 100Gb)|6, 3|2|
|Intel (NIC: 100Gb)|6, 3|2|
|Broadcom (NIC: 25Gb)|7, 1, 2|3|
|Mellanox (NIC: 25Gb)|7, 1, 2|3|
|Mellanox (NIC: 25Gb)|6, 3|2|
|Intel (NIC: 25Gb)|7, 1, 2|3|
|Intel (NIC: 25Gb)|7, 1, 2|3|
|Intel (NIC: 25Gb)|6, 3|2|
|Broadcom (Emulex) (HBA: FC64)|7, 1, 2|3|
|Broadcom (Emulex) (HBA: FC64)|6, 3|2|
|Broadcom (Emulex) (HBA: FC32)|7, 1, 2|3|
|Broadcom (Emulex) (HBA: FC32)|6, 3|2|
|Broadcom (Emulex) (HBA: FC32)|7, 1, 2|3|
|Broadcom (Emulex) (HBA: FC32)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|7, 1, 2|3|
|Qlogic (Marvell) (HBA: FC32)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|7, 1, 2|3|
|Qlogic (Marvell) (HBA: FC32)|6, 3|2|
|Intel (NIC: ACC100)|6, 3|2|
|Broadcom (NIC: 25Gb)|7, 1, 2|3|
|Broadcom (NIC: 25Gb)|6, 3|2|
|Broadcom (NIC: 10Gb)|7, 1, 2|3|
|Broadcom (NIC: 10Gb)|6, 3|2|
|Broadcom (NIC: 10Gb)|7, 1, 2, 6, 3|5|
|Broadcom (NIC: 10Gb)|7, 1, 2|3|
|Broadcom (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 10Gb)|7, 1, 2|3|
|Intel (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 10Gb)|7, 1, 2|3|
|Intel (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 1Gb)|7, 1, 2|3|
|Intel (NIC: 1Gb)|6, 3|2|
|Intel (NIC: 1Gb)|7, 1, 2|3|
|Intel (NIC: 1Gb)|6, 3|2|
|Broadcom (NIC: 1Gb)|7, 1, 2|3|
|Broadcom (NIC: 1Gb)|6, 3|2|
|Foxconn (HBA355e DIB External<br>Adapter)|7, 1, 2, 6, 3|5|
|Foxconn (H965e DIB External Adapter)|6, 3|1|

**Table 164. Configuration 8: R1B+R2A**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Mellanox (NVIDIA) (Channel DPU: 25Gb)|1, 2|2|
|Inventec (LOM Card)|Integrated slot|1|
|Broadcom (OCP: 100Gb)|Integrated slot|1|
|Mellanox (OCP: 100Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Mellanox (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Foxconn (BOSS)|Integrated slot|1|
|Foxconn (PERC Adapter11 H755)|3|1|
|Foxconn (PERC Adapter11 HBA355i)|3|1|
|Foxconn (PERC Adapter11 H355)|3|1|
|Intel (GPU ATS-M)|1, 2|2|
|NVIDIA (GPU A2)|1, 2|2|
|Mellanox (LP NIC: NDR200)|3|1|
|Mellanox (NIC: HDR100 VPI)|3|1|
|Mellanox (LP NIC: HDR VPI), 2P|3|1|
|Mellanox (LP NIC: HDR VPI), 1P|3|1|
|Mellanox (NIC: 100Gb)|3|1|
|Broadcom (NIC: 100Gb)|3|1|
|Intel (NIC: 100Gb)|3|1|
|Broadcom (NIC: 25Gb)|1, 2|2|
|Mellanox (NIC: 25Gb)|1, 2|2|
|Mellanox (NIC: 25Gb)|3|1|
|Intel (NIC: 25Gb)|1, 2|2|
|Intel (NIC: 25Gb)|1, 2|2|
|Intel (NIC: 25Gb)|3|1|
|Broadcom (Emulex) (HBA: FC64)|1, 2|2|
|Broadcom (Emulex) (HBA: FC64)|3|1|
|Broadcom (Emulex) (HBA: FC32)|1, 2|2|
|Broadcom (Emulex) (HBA: FC32)|3|1|
|Broadcom (Emulex) (HBA: FC32)|1, 2|2|
|Broadcom (Emulex) (HBA: FC32)|3|1|
|Qlogic (Marvell) (HBA: FC32)|1, 2|2|
|Qlogic (Marvell) (HBA: FC32)|3|1|
|Qlogic (Marvell) (HBA: FC32)|1, 2|2|
|Qlogic (Marvell) (HBA: FC32)|3|1|
|Intel (NIC: ACC100)|3|1|
|Broadcom (NIC: 25Gb)|1, 2|2|
|Broadcom (NIC: 25Gb)|3|1|
|Broadcom (NIC: 10Gb)|1, 2|2|
|Broadcom (NIC: 10Gb)|3|1|
|Broadcom (NIC: 10Gb)|1, 3, 2|3|
|Broadcom (NIC: 10Gb)|1, 2|2|
|Broadcom (NIC: 10Gb)|3|1|
|Intel (NIC: 10Gb)|1, 2|2|
|Intel (NIC: 10Gb)|3|1|
|Intel (NIC: 10Gb)|1, 2|2|
|Intel (NIC: 10Gb)|3|1|
|Intel (NIC: 1Gb)|1, 2|2|
|Intel (NIC: 1Gb)|3|1|
|Intel (NIC: 1Gb)|1, 2|2|
|Intel (NIC: 1Gb)|3|1|

**Table 164. Configuration 8: R1B+R2A (continued)**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Broadcom (NIC: 1Gb)|1, 2|2|
|Broadcom (NIC: 1Gb)|3|1|
|Foxconn (HBA355e DIB External<br>Adapter)|1, 3, 2|3|
|Foxconn (H965e DIB External Adapter)|3|1|
|Inventec (VGA)|8|1|
|Inventec (Serial)|8|1|
|Mellanox (NVIDIA) (Channel DPU: 25Gb)|7, 1, 2|3|
|Inventec (LOM Card)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Mellanox (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Foxconn (BOSS)|Integrated slot|1|
|Foxconn (Front PERC12 H965i)|Integrated slot|2|
|Foxconn (Front PERC11 H755N)|Integrated slot|1|
|Foxconn (Front PERC11 H755)|Integrated slot|1|
|Foxconn (Front PERC11 H355)|Integrated slot|1|
|Foxconn (Front PERC11 HBA355i)|Integrated slot|1|
|Intel (GPU ATS-M)|7, 1, 2|3|
|NVIDIA (GPU A2)|7, 1, 2|3|
|Mellanox (LP NIC: NDR200)|3|1|
|Mellanox (NIC: HDR100 VPI)|3|1|
|Mellanox (LP NIC: HDR VPI), 2P|3|1|
|Mellanox (LP NIC: HDR VPI), 1P|3|1|
|Mellanox (NIC: 100Gb)|3|1|

**Table 165. Configuration 9: R1Q+R2A+R4R (continued)**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Broadcom (NIC: 100Gb)|3|1|
|Intel (NIC: 100Gb)|3|1|
|Broadcom (NIC: 25Gb)|7, 1, 2|3|
|Mellanox (NIC: 25Gb)|7, 1, 2|3|
|Mellanox (NIC: 25Gb)|3|1|
|Intel (NIC: 25Gb)|7, 1, 2|3|
|Intel (NIC: 25Gb)|7, 1, 2|3|
|Intel (NIC: 25Gb)|3|1|
|Broadcom (Emulex) (HBA: FC64)|7, 1, 2|3|
|Broadcom (Emulex) (HBA: FC64)|3|1|
|Broadcom (Emulex) (HBA: FC32)|7, 1, 2|3|
|Broadcom (Emulex) (HBA: FC32)|3|1|
|Broadcom (Emulex) (HBA: FC32)|7, 1, 2|3|
|Broadcom (Emulex) (HBA: FC32)|3|1|
|Qlogic (Marvell) (HBA: FC32)|7, 1, 2|3|
|Qlogic (Marvell) (HBA: FC32)|3|1|
|Qlogic (Marvell) (HBA: FC32)|7, 1, 2|3|
|Qlogic (Marvell) (HBA: FC32)|3|1|
|Intel (NIC: ACC100)|3|1|
|Broadcom (NIC: 25Gb)|7, 1, 2|3|
|Broadcom (NIC: 25Gb)|3|1|
|Broadcom (NIC: 10Gb)|7, 1, 2|3|
|Broadcom (NIC: 10Gb)|3|1|
|Broadcom (NIC: 10Gb)|7, 1, 3, 2|4|
|Broadcom (NIC: 10Gb)|7, 1, 2|3|
|Broadcom (NIC: 10Gb)|3|1|
|Intel (NIC: 10Gb)|7, 1, 2|3|
|Intel (NIC: 10Gb)|3|1|
|Intel (NIC: 10Gb)|7, 1, 2|3|
|Intel (NIC: 10Gb)|3|1|
|Intel (NIC: 1Gb)|7, 1, 2|3|
|Intel (NIC: 1Gb)|3|1|
|Intel (NIC: 1Gb)|7, 1, 2|3|
|Intel (NIC: 1Gb)|3|1|
|Broadcom (NIC: 1Gb)|7, 1, 2|3|
|Broadcom (NIC: 1Gb)|3|1|
|Foxconn (HBA355e DIB External<br>Adapter)|7, 1, 3, 2|4|
|Foxconn (H965e DIB External Adapter)|3|1|

**Table 166. Configuration 10-1: R1P+R2A+R4R (HL)**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Inventec (VGA)|8|1|
|Inventec (Serial)|8|1|
|Mellanox (NVIDIA) (Channel DPU: 25Gb)|7, 2|2|
|Inventec (LOM Card)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Mellanox (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Foxconn (BOSS)|Integrated slot|1|
|Foxconn (Front PERC12 H965i)|Integrated slot|2|
|Foxconn (Front PERC11 H755N)|Integrated slot|1|
|Foxconn (Front PERC11 H755)|Integrated slot|1|
|Foxconn (Front PERC11 H355)|Integrated slot|1|
|Foxconn (Front PERC11 HBA355i)|Integrated slot|1|
|NVIDIA (GPU L4)|2|1|
|Intel (GPU ATS-M)|7, 2|2|
|NVIDIA (GPU A2)|7, 2|2|
|Mellanox (FH NIC: NDR200)|2|1|
|Mellanox (LP NIC: NDR200)|3|1|
|Mellanox (NIC: NDR400)|2|1|
|Mellanox (NIC: HDR100 VPI)|2|1|
|Mellanox (NIC: HDR100 VPI)|3|1|
|Mellanox (FH NIC: HDR VPI), 2P|2|1|
|Mellanox (LP NIC: HDR VPI), 2P|3|1|
|Mellanox (FH NIC: HDR VPI), 1P|2|1|
|Mellanox (LP NIC: HDR VPI), 1P|3|1|
|Mellanox (NIC: 100Gb)|2|1|
|Mellanox (NIC: 100Gb)|3|1|
|Broadcom (NIC: 100Gb)|3|1|
|Intel (NIC: 100Gb)|2|1|
|Intel (NIC: 100Gb)|3|1|
|Intel (NIC: 100Gb)|2|1|
|Intel (NIC: 25Gb)|2|1|
|Intel (NIC: 25Gb)|2|1|
|Broadcom (NIC: 25Gb)|7, 2|2|
|Intel (NIC: 10Gb)|2|1|
|Mellanox (NIC: 25Gb)|7, 2|2|
|Mellanox (NIC: 25Gb)|3|1|
|Intel (NIC: 25Gb)|7, 2|2|
|Intel (NIC: 25Gb)|7, 2|2|
|Intel (NIC: 25Gb)|3|1|
|Broadcom (Emulex) (HBA: FC64)|7, 2|2|
|Broadcom (Emulex) (HBA: FC64)|3|1|
|Qlogic (Marvell) (HBA: FC32)|2|1|
|Broadcom (Emulex) (HBA: FC32)|7, 2|2|
|Broadcom (Emulex) (HBA: FC32)|3|1|
|Broadcom (Emulex) (HBA: FC32)|7, 2|2|
|Broadcom (Emulex) (HBA: FC32)|3|1|
|Qlogic (Marvell) (HBA: FC32)|7, 2|2|
|Qlogic (Marvell) (HBA: FC32)|3|1|
|Qlogic (Marvell) (HBA: FC32)|7, 2|2|
|Qlogic (Marvell) (HBA: FC32)|3|1|
|Intel (NIC: ACC100)|3|1|
|Broadcom (NIC: 25Gb)|7, 2|2|
|Broadcom (NIC: 25Gb)|3|1|
|Broadcom (NIC: 10Gb)|7, 2|2|
|Broadcom (NIC: 10Gb)|3|1|
|Broadcom (NIC: 10Gb)|7, 3, 2|3|
|Broadcom (NIC: 10Gb)|7, 2|2|
|Broadcom (NIC: 10Gb)|3|1|
|Intel (NIC: 10Gb)|7, 2|2|
|Intel (NIC: 10Gb)|3|1|

**Table 166. Configuration 10-1: R1P+R2A+R4R (HL) (continued)**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Intel (NIC: 10Gb)|7, 2|2|
|Intel (NIC: 10Gb)|3|1|
|Intel (NIC: 1Gb)|7, 2|2|
|Intel (NIC: 1Gb)|3|1|
|Intel (NIC: 1Gb)|7, 2|2|
|Intel (NIC: 1Gb)|3|1|
|Broadcom (NIC: 1Gb)|7, 2|2|
|Broadcom (NIC: 1Gb)|3|1|
|Foxconn (HBA355e DIB External<br>Adapter)|7, 3, 2|3|
|Foxconn (H965e DIB External Adapter)|3, 2|1|
|Inventec (VGA)|8|1|
|Inventec (Serial)|8|1|
|Mellanox (NVIDIA) (Channel DPU: 25Gb)|7, 2|2|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Mellanox (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Foxconn (BOSS)|Integrated slot|1|
|Foxconn (Front PERC12 H965i)|Integrated slot|2|
|Foxconn (Front PERC11 H755N)|Integrated slot|1|
|Foxconn (Front PERC11 H755)|Integrated slot|1|
|Foxconn (Front PERC11 H355)|Integrated slot|1|
|Foxconn (Front PERC11 HBA355i)|Integrated slot|1|
|Intel (GPU PVC)|2|1|
|NVIDIA (GPU H100)|2|1|

**Table 167. Configuration 10-2: R1P+R2A+R4R (FL) (continued)**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|NVIDIA (GPU L40S)|2|1|
|NVIDIA (GPU L40)|2|1|
|NVIDIA (GPU A40)|2|1|
|NVIDIA (GPU A800)|2|1|
|NVIDIA (GPU A100)|2|1|
|NVIDIA (GPU A30)|2|1|
|NVIDIA (GPU A16)|2|1|
|NVIDIA (GPU L4)|2|1|
|Intel (GPU ATS-M)|7, 2|2|
|NVIDIA (GPU A2)|7, 2|2|
|Mellanox (FH NIC: NDR200)|2|1|
|Mellanox (LP NIC: NDR200)|3|1|
|Mellanox (NIC: NDR400)|2|1|
|Mellanox (NIC: HDR100 VPI)|2|1|
|Mellanox (NIC: HDR100 VPI)|3|1|
|Mellanox (FH NIC: HDR VPI), 2P|2|1|
|Mellanox (LP NIC: HDR VPI), 2P|3|1|
|Mellanox (FH NIC: HDR VPI), 1P|2|1|
|Mellanox (LP NIC: HDR VPI), 1P|3|1|
|Mellanox (NIC: 100Gb)|2|1|
|Mellanox (NIC: 100Gb)|3|1|
|Broadcom (NIC: 100Gb)|3|1|
|Intel (NIC: 100Gb)|2|1|
|Intel (NIC: 100Gb)|3|1|
|Intel (NIC: 100Gb)|2|1|
|Intel (NIC: 25Gb)|2|1|
|Intel (NIC: 25Gb)|2|1|
|Broadcom (NIC: 25Gb)|7, 2|2|
|Intel (NIC: 10Gb)|2|1|
|Mellanox (NIC: 25Gb)|7, 2|2|
|Mellanox (NIC: 25Gb)|3|1|
|Intel (NIC: 25Gb)|7, 2|2|
|Intel (NIC: 25Gb)|7, 2|2|
|Intel (NIC: 25Gb)|3|1|
|Broadcom (Emulex) (HBA: FC64)|7, 2|2|
|Broadcom (Emulex) (HBA: FC64)|3|1|
|Qlogic (Marvell) (HBA: FC32)|2|1|
|Broadcom (Emulex) (HBA: FC32)|7, 2|2|
|Broadcom (Emulex) (HBA: FC32)|3|1|
|Broadcom (Emulex) (HBA: FC32)|7, 2|2|
|Broadcom (Emulex) (HBA: FC32)|3|1|
|Qlogic (Marvell) (HBA: FC32)|7, 2|2|
|Qlogic (Marvell) (HBA: FC32)|3|1|
|Qlogic (Marvell) (HBA: FC32)|7, 2|2|
|Qlogic (Marvell) (HBA: FC32)|3|1|
|Intel (NIC: ACC100)|3|1|
|Broadcom (NIC: 25Gb)|7, 2|2|
|Broadcom (NIC: 25Gb)|3|1|
|Broadcom (NIC: 10Gb)|7, 2|2|
|Broadcom (NIC: 10Gb)|3|1|
|Broadcom (NIC: 10Gb)|7, 3, 2|3|
|Broadcom (NIC: 10Gb)|7, 2|2|
|Broadcom (NIC: 10Gb)|3|1|
|Intel (NIC: 10Gb)|7, 2|2|
|Intel (NIC: 10Gb)|3|1|
|Intel (NIC: 10Gb)|7, 2|2|
|Intel (NIC: 10Gb)|3|1|
|Intel (NIC: 1Gb)|7, 2|2|
|Intel (NIC: 1Gb)|3|1|
|Intel (NIC: 1Gb)|7, 2|2|
|Intel (NIC: 1Gb)|3|1|
|Broadcom (NIC: 1Gb)|7, 2|2|
|Broadcom (NIC: 1Gb)|3|1|
|Foxconn (HBA355e DIB External<br>Adapter)|7, 3, 2|3|
|Foxconn (H965e DIB External Adapter)|3, 2|1|

**Table 168. Configuration 11: R1 paddle+R2A+R3B+R4 paddle**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Inventec (VGA)|8, 4|1|
|Inventec (Serial)|8, 4|1|
|Mellanox (NVIDIA) (Channel DPU: 25Gb)|5, 4|2|
|Inventec (LOM Card)|Integrated slot|1|
|Broadcom (OCP: 100Gb)|Integrated slot|1|
|Mellanox (OCP: 100Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Mellanox (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Foxconn (BOSS)|Integrated slot|1|
|Intel (GPU ATS-M)|4, 5|2|
|NVIDIA (GPU A2)|4, 5|2|
|Mellanox (LP NIC: NDR200)|6, 3|2|
|Mellanox (NIC: HDR100 VPI)|6, 3|2|
|Mellanox (LP NIC: HDR VPI), 2P|6, 3|2|
|Mellanox (LP NIC: HDR VPI), 1P|6, 3|2|
|Mellanox (NIC: 100Gb)|6, 3|2|
|Broadcom (NIC: 100Gb)|6, 3|2|
|Intel (NIC: 100Gb)|6, 3|2|
|Broadcom (NIC: 25Gb)|5, 4|2|
|Mellanox (NIC: 25Gb)|5, 4|2|
|Mellanox (NIC: 25Gb)|6, 3|2|
|Intel (NIC: 25Gb)|5, 4|2|
|Intel (NIC: 25Gb)|5, 4|2|
|Intel (NIC: 25Gb)|6, 3|2|
|Broadcom (Emulex) (HBA: FC64)|5, 4|2|
|Broadcom (Emulex) (HBA: FC64)|6, 3|2|
|Broadcom (Emulex) (HBA: FC32)|5, 4|2|
|Broadcom (Emulex) (HBA: FC32)|6, 3|2|
|Broadcom (Emulex) (HBA: FC32)|5, 4|2|
|Broadcom (Emulex) (HBA: FC32)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|5, 4|2|
|Qlogic (Marvell) (HBA: FC32)|6, 3|2|

**Table 168. Configuration 11: R1 paddle+R2A+R3B+R4 paddle (continued)**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Qlogic (Marvell) (HBA: FC32)|5, 4|2|
|Qlogic (Marvell) (HBA: FC32)|6, 3|2|
|Intel (NIC: ACC100)|6, 3|2|
|Broadcom (NIC: 25Gb)|5, 4|2|
|Broadcom (NIC: 25Gb)|6, 3|2|
|Broadcom (NIC: 10Gb)|5, 4|2|
|Broadcom (NIC: 10Gb)|6, 3|2|
|Broadcom (NIC: 10Gb)|5, 4, 6, 3|4|
|Broadcom (NIC: 10Gb)|5, 4|2|
|Broadcom (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 10Gb)|5, 4|2|
|Intel (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 10Gb)|5, 4|2|
|Intel (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 1Gb)|5, 4|2|
|Intel (NIC: 1Gb)|6, 3|2|
|Intel (NIC: 1Gb)|5, 4|2|
|Intel (NIC: 1Gb)|6, 3|2|
|Broadcom (NIC: 1Gb)|5, 4|2|
|Broadcom (NIC: 1Gb)|6, 3|2|
|Foxconn (HBA355e DIB External<br>Adapter)|5, 4, 6, 3|4|
|Foxconn (H965e DIB External Adapter)|6, 3|1|
|Inventec (VGA)|8|1|
|Inventec (Serial)|8|1|
|Mellanox (NVIDIA) (Channel DPU: 25Gb)|7, 1, 2|3|
|Inventec (LOM Card)|Integrated slot|1|
|Broadcom (OCP: 100Gb)|Integrated slot|1|
|Mellanox (OCP: 100Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Mellanox (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 25Gb)|Integrated slot|1|
|Broadcom (OCP: 25Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|

**Table 169. Configuration 12: R1Q+R2A+R4Q (continued)**

|Card type|Slot priority|Maximum number of cards|
|---|---|---|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Intel (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 10Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Broadcom (OCP: 1Gb)|Integrated slot|1|
|Intel (OCP: 1Gb)|Integrated slot|1|
|Foxconn (BOSS)|Integrated slot|1|
|Foxconn (Front PERC12 HBA465i)|Integrated slot|1|
|Foxconn (Front PERC12 H965i)|Integrated slot|1|
|Foxconn (Front PERC11 H755)|Integrated slot|1|
|Foxconn (Front PERC11 H355)|Integrated slot|1|
|Foxconn (Front PERC11 HBA355i)|Integrated slot|1|
|Foxconn (PERC Adapter11 H755)|3|1|
|Foxconn (PERC Adapter11 HBA355i)|3|1|
|Foxconn (PERC Adapter11 H355)|3|1|
|Mellanox (LP NIC: NDR200)|6, 3|2|
|Mellanox (NIC: HDR100 VPI)|6, 3|2|
|Mellanox (LP NIC: HDR VPI), 2P|6, 3|2|
|Mellanox (LP NIC: HDR VPI), 1P|6, 3|2|
|Mellanox (NIC: 100Gb)|6, 3|2|
|Broadcom (NIC: 100Gb)|6, 3|2|
|Broadcom (NIC: 25Gb)|7, 1, 2|3|
|Intel (NIC: 100Gb)|6, 3|2|
|Mellanox (NIC: 25Gb)|7, 1, 2|3|
|Mellanox (NIC: 25Gb)|6, 3|2|
|Intel (NIC: 25Gb)|7, 1, 2|3|
|Intel (NIC: 25Gb)|7, 1, 2|3|
|Intel (NIC: 25Gb)|6, 3|2|
|Broadcom (Emulex) (HBA: FC64)|7, 1, 2|3|
|Broadcom (Emulex) (HBA: FC64)|6, 3|2|
|Broadcom (Emulex) (HBA: FC32)|7, 1, 2|3|
|Broadcom (Emulex) (HBA: FC32)|6, 3|2|
|Broadcom (Emulex) (HBA: FC32)|7, 1, 2|3|
|Broadcom (Emulex) (HBA: FC32)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|7, 1, 2|3|
|Qlogic (Marvell) (HBA: FC32)|6, 3|2|
|Qlogic (Marvell) (HBA: FC32)|7, 1, 2|3|
|Qlogic (Marvell) (HBA: FC32)|6, 3|2|
|Intel (NIC: ACC100)|6, 3|2|
|Broadcom (NIC: 25Gb)|7, 1, 2|3|
|Broadcom (NIC: 25Gb)|6, 3|2|
|Broadcom (NIC: 10Gb)|7, 1, 2|3|
|Broadcom (NIC: 10Gb)|6, 3|2|
|Broadcom (NIC: 10Gb)|7, 1, 2, 6, 3|5|
|Broadcom (NIC: 10Gb)|7, 1, 2|3|
|Broadcom (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 10Gb)|7, 1, 2|3|
|Intel (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 10Gb)|7, 1, 2|3|
|Intel (NIC: 10Gb)|6, 3|2|
|Intel (NIC: 1Gb)|7, 1, 2|3|
|Intel (NIC: 1Gb)|6, 3|2|
|Intel (NIC: 1Gb)|7, 1, 2|3|
|Intel (NIC: 1Gb)|6, 3|2|
|Broadcom (NIC: 1Gb)|7, 1, 2|3|
|Broadcom (NIC: 1Gb)|6, 3|2|
|Foxconn (HBA355e DIB External<br>Adapter)|7, 1, 2, 6, 3|5|
|Foxconn (H965e DIB External Adapter)|6, 3|1|



### Removing the expansion card risers

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the air shroud or remove the GPU air shroud.
4.

**NOTE:** If BOSS-N1 module is installed, be sure to disconnect the BOSSN1 power cable and Signal cable before removing the Riser 1 cage.

5. If applicable, disconnect the cables from the expansion card or system board.
**Steps**

1. Loosen the captive screws on the riser and system.
2. Press the blue release tab or blue button on the riser and holding the edges lift the expansion card riser from the riser
connector on the system board.

**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.



### Figure 223. Removing the expansion card riser 1





### Figure 224. Removing the expansion card riser 3





### Figure 225. Removing the expansion card riser 2





### Figure 226. Removing the expansion card riser 4

3. If the risers are not going to be replaced, install riser blanks, and if required tighten the captive screws.
**NOTE:** You must install a filler bracket over an empty expansion card slot to maintain Federal Communications
Commission (FCC) certification of the system. The brackets also keep dust and dirt out of the system and aid in
proper cooling and airflow inside the system.

**Figure 227. Installing the Riser 1 blank**

**Figure 228. Installing the Riser 2 blank**

**Figure 229. Installing the Riser 3 blank**

**Figure 230. Installing the Riser 4 blank**

**Next steps**

1. Replace the expansion card riser.



### Installing the expansion card risers

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the air shroud or remove the GPU air shroud.
4. If removed, install the expansion cards into the expansion card risers.
**NOTE:** Install Riser 2 before installing Riser 1 and Riser 3. Install Riser 4 after installing Riser 3.

**CAUTION: Do not install GPUs, network cards, or other PCIe devices on your system that are not validated**
**and tested by Dell. Damage caused by unauthorized and invalidated hardware installation will null and void the**
**system warranty.**

**Steps**

1. If installed, remove the riser blanks and if required loosen the captive screws.
**NOTE:** Store the riser blanks for future use. Filler brackets must be installed in empty expansion card slots to maintain
Federal Communications Commission (FCC) certification of the system. The brackets also keep dust and dirt out of the
system and aid in proper cooling and airflow inside the system.

**Figure 231. Removing the Riser 2 blank**

**Figure 232. Removing the Riser 1 blank**

**Figure 233. Removing the Riser 3 blank**

**Figure 234. Removing the Riser 4 blank**

2. Holding the edges or the touch points, align the holes on the expansion card riser with the guides on the system board.
3. Lower the expansion card riser into place and press the touch points until the expansion card riser connector is fully seated
on the system board connector.

4. Tighten the captive screws on the risers and system if any.



### Figure 235. Installing the expansion card riser 2





### Figure 236. Installing the expansion card riser 1





### Figure 237. Installing the expansion card riser 3





### Figure 238. Installing the expansion card riser 4

**Next steps**

1. If required, reconnect the cables to the expansion card or system board.
2. Install the air shroud or install the GPU air shroud.
3. Follow the procedure listed in After working inside your system.
4. Install any device drivers required for the card as described in the documentation for the card.



### Removing expansion card from the expansion card riser

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. If applicable, disconnect the cables from the expansion card.
4. Remove the air shroud or remove the GPU air shroud.
5. Remove the expansion card riser.
**Steps**

1. Tilt the expansion card retention latch lock to open.
2. Pull the card holder before removing the card from the riser.
3. Hold the expansion card by the edges and pull the card from the riser.
**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.



### Figure 239. Removing expansion card from the expansion card riser

4. If the expansion card is not going to be replaced, install a filler bracket and close the card retention latch.
**NOTE:** You must install a filler bracket over an empty expansion card slot to maintain Federal Communications
Commission (FCC) certification of the system. The brackets also keep dust and dirt out of the system and aid in
proper cooling and airflow inside the system.

**Figure 240. Installing the filler bracket**

**Next steps**

1. If applicable, install an expansion card into the expansion card riser.



### Installing an expansion card into the expansion card riser

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the air shroud or remove the GPU air shroud.
4. Remove the expansion card riser.
5. If installing a new expansion card, unpack it and prepare the card for installation.
**NOTE:** For instructions, see the documentation accompanying the card.

**CAUTION: Do not install GPUs, network cards, or other PCIe devices on your system that are not validated**
**and tested by Dell. Damage caused by unauthorized and invalidated hardware installation will null and void the**
**system warranty.**

**Steps**

1. Tilt the expansion card retention latch lock to open.
2. If installed, remove the filler bracket.
**NOTE:** Store the filler bracket for future use. Filler brackets must be installed in empty expansion card slots to maintain
Federal Communications Commission (FCC) certification of the system. The brackets also keep dust and dirt out of the
system and aid in proper cooling and airflow inside the system.

**Figure 241. Removing the filler bracket**

3. Hold the card by edges, and align the card with the connector on the riser.
4. Insert the card firmly into the expansion card connector until seated.
5. Close the expansion card retention latch.
6. Push the card holder to hold the card in the riser.
**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.



### Figure 242. Installing an expansion card into the expansion card riser

**Next steps**

1. If applicable, connect the cables to the expansion card.
2. Install the expansion card risers.
3. Install the air shroud or install the GPU air shroud.
4. Follow the procedure listed in After working inside your system.
5. Install any device drivers required for the card as described in the documentation for the card.



### Removing the full length expansion card risers

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the GPU air shroud top cover.
4. If applicable, disconnect the cables from the expansion card or system board.

N1 power cable and signal cable before removing the Riser 1 cage.

**Steps**

1. To remove full length expansion card riser:

a. Loosen the captive screws on the riser.
b. Press the blue release tab and holding the edges, lift the expansion card riser from the riser connector on the system
board.
c. Disconnect the GPU power cable and signal cable from the system board.

**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.

**Figure 243. Removing the expansion card riser (Riser 1)**

**Figure 244. Removing the expansion card riser (Riser 4)**

2. If the risers are not going to be replaced, install riser blanks and tighten the captive screws.
**NOTE:** You must install a filler bracket over an empty expansion card slot to maintain Federal Communications
Commission (FCC) certification of the system. The brackets also keep dust and dirt out of the system and aid in
proper cooling and airflow inside the system.

**Figure 245. Installing the Riser 1 blank**

**Figure 246. Installing the Riser 4 blank**

**Next steps**

1. Replace the full length expansion card riser.



### Installing the full length expansion card risers

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. If installing full length expansion card riser for the first time, remove the air shroud and replace it with the GPU air shroud.
4. Remove the GPU air shroud top cover.
5. If installed, remove the GPU air shroud filler.
6. If removed, install the GPU into the expansion card risers.
**NOTE:** Full length risers are supported only in Riser 1 and Riser 4 slot. Install Riser 4 first and then Riser 1.

**Steps**

1. If installed, remove the riser blanks by loosening the captive screws.
**NOTE:** Store the Riser blanks for future use. Filler brackets must be installed in empty expansion card slots to maintain
Federal Communications Commission (FCC) certification of the system. The brackets also keep dust and dirt out of the
system and aid in proper cooling and airflow inside the system.

**Figure 247. Removing the Riser 1 blank**

**Figure 248. Removing the Riser 4 blank**

2. To install the full length expansion card risers:

a. For Riser 4 connect the GPU power cable to the connector PWR2_B and signal cable to the connector PSU2_SIG on the
system board.
b. For Riser 1 connect the GPU power cable to the connector PWR1_B and signal cable to the connector PSU1_SIG on the
system board.

**NOTE:** Temporarily unplug and plug the VGA cable for making space to connect Riser 1 GPU power cable to system
board.

c. Holding the edges or the touch points, align the holes on the expansion card riser with the guides on the system board
and GPU air shroud.
d. Lower the expansion card riser into place and press the touch points until the expansion card riser connector is fully
seated on the system board connector.
e. Tighten the captive screws on the risers and system if any.

**NOTE:** Route the cables properly through the riser clip.

**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.

**Figure 249. Installing the expansion card riser (Riser 4)**

**Figure 250. Installing the expansion card riser (Riser 1)**

**Next steps**

1. If applicable, connect the cables to the expansion card or system board.
2. Install the GPU air shroud top cover.
3. Follow the procedure listed in After working inside your system.
4. Install any device drivers required for the card as described in the documentation for the card.



### Removing a GPU

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. If applicable, disconnect the cables from the expansion card.
4. Remove the GPU air shroud top cover.
5. Remove the full length expansion card riser.
**Steps**

1. To remove the GPU from Riser 1:

a. Tilt the expansion card holder latch on the riser.
b. Press the tab, and pull the card holder from the riser.
c. Hold the GPU card by the edges and pull the card from the riser.
d. Disconnect the GPU power cable from the GPU card.

**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.

**Figure 251. Removing GPU from Riser 1**

2. To remove the GPU from Riser 4:

a. Slide the expansion card latch on the riser.
b. Press the tab, and pull the card holder from the riser.
c. Tilt the expansion card holder latch on the riser.
d. Hold the GPU card by the edges and pull the card from the riser.
e. Disconnect the GPU power cable from the GPU card.

**Figure 252. Removing GPU from Riser 4**

3. If removing the GPU permanently, install a filler bracket.
**NOTE:** You must install a filler bracket over an empty expansion card slot to maintain Federal Communications
Commission (FCC) certification of the system. The brackets also keep dust and dirt out of the system and aid in
proper cooling and airflow inside the system. The filler bracket is necessary to maintain proper thermal conditions.

4. Install a metal filler bracket over the empty expansion slot opening and close the expansion card latch.
**Figure 253. Installing the metal filler bracket**

**Next steps**

1. Replace the GPU.



### Installing a GPU

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. If installing a new expansion card, unpack it and prepare the card for installation.
**NOTE:** For instructions, see the documentation accompanying the card.

4. Remove the GPU air shroud top cover.
5. Remove the GPU air shroud filler.
6. Remove the full length expansion card riser.
**CAUTION: Do not install GPUs, network cards, or other PCIe devices on your system that are not validated**
**and tested by Dell. Damage caused by unauthorized and invalidated hardware installation will null and void the**
**system warranty.**

**WARNING: Consumer-Grade GPU should not be installed or used in the Enterprise Server products.**

**Steps**

1. If installed, remove the filler bracket.
**NOTE:** Store the filler bracket for future use. Filler brackets must be installed in empty expansion card slots to maintain
Federal Communications Commission (FCC) certification of the system. The brackets also keep dust and dirt out of the
system and aid in proper cooling and airflow inside the system.

**Figure 254. Removing the filler bracket**

2. To install the GPU on Riser 1:

a. Connect the GPU power cable to the GPU card.
b. Align the connector on the GPU with the connector on the riser.
c. Insert the GPU into the riser until firmly seated.
d. Tilt the expansion card holder latch.
e. Press the card holder latch to secure the GPU card to the riser.

**Figure 255. Installing GPU on Riser 1**

3. To install the GPU on Riser 4:

a. Connect the GPU power cable to the GPU card.
b. Align the connector on the GPU with the connector on the riser.
c. Insert the GPU into the riser until firmly seated.
d. Tilt the expansion card holder latch.
e. Press the card holder latch to secure the GPU card to the riser.
f. Slide the expansion card latch on the riser.

**Figure 256. Installing GPU on Riser 4**

**Next steps**

1. If removed, install the GPU air shroud.
**NOTE:** The GPU air shroud filler must be installed, if single-width GPU card or empty riser is used.

2. Install the full length expansion card riser.
3. Install the GPU air shroud top cover.
4. Follow the procedure listed in After working inside your system.
5. Install any device drivers required for the card as described in the documentation for the card.



### Removing R1 and R4 paddle cards

R1 and R4 paddle cards are supported on 24 x 2.5-inch NVMe Gen4 (passive) configuration only.

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the air shroud or remove the GPU air shroud.
4. Remove the cooling fan cage assembly.
**NOTE:** Observe the cable routing before disconnecting the cables.

5. Remove the paddle card cable from side wall bracket.
6. Disconnect the paddle card cables from the drive backplane.
**Steps**

Press the blue release tab on the paddle cards and holding the edges lift the paddle cards from the riser connector on the
system board.

**Figure 257. Removing the R1 paddle card**

**Figure 258. Removing the R4 paddle card**

**Next steps**

1. Replace the paddle cards.



### Installing R1 and R4 paddle cards

R1 and R4 paddle cards are supported on 24 x 2.5-inch NVMe Gen4 (passive) configuration only.

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the air shroud or remove the GPU air shroud.
4. Remove the cooling fan cage assembly.
**NOTE:** Observe the cable routing before disconnecting the cables.

5. Remove the paddle card cable from side wall bracket.
6. Disconnect the paddle card cables from the drive backplane.
**Steps**

1. Holding the edges, align the hole on the paddle cards with the guides on the system board.
2. Lower the paddle cards into place and press until the paddle card connector is fully seated on the system board connector.
**Figure 259. Installing the R1 paddle card**

**Figure 260. Installing the R4 paddle card**

**Next steps**

1. Connect the paddle card cables to the drive backplane.
2. Insert the paddle card cable into the side wall bracket.
3. Install the cooling fan cage assembly.
4. Install the air shroud or install the GPU air shroud.
5. Follow the procedure listed in After working inside your system.



## Data processing unit (DPU)





### Removing a DPU card from a full length riser

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. If applicable, disconnect the cables from the expansion card.
4. Remove the GPU air shroud top cover.
5. Remove the full length expansion card riser.

BOSS-N1 power cable and signal cable before removing the Riser 1 cage.

7. Disconnect the cables from the DPU card and the system board. See Cable routing topic, configuration 56.
**NOTE:** Dell DPUs only support vSphere 8.0+ operating system and Partner or Channel DPU only supports Linux-based
operating system.

**Steps**

1. To remove the DPU card from Riser 1:

a. Tilt the expansion card holder latch on the riser.
b. Press the tab, and pull the card holder from the riser.
c. Hold the DPU card by the edges and pull the card from the riser.

**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.

**Figure 261. Removing DPU card from Riser 1**

2. Using a Phillips #2 screwdriver loosen the two screws and remove the DPU bracket from the DPU card.
**Figure 262. Removing DPU bracket**

3. If removing the DPU permanently, install a filler bracket.
**NOTE:** You must install a filler bracket over an empty expansion card slot to maintain Federal Communications
Commission (FCC) certification of the system. The brackets also keep dust and dirt out of the system and aid in
proper cooling and airflow inside the system. The filler bracket is necessary to maintain proper thermal conditions.

4. Install a metal filler bracket over the empty expansion slot opening and close the expansion card latch.
**Figure 263. Installing the metal filler bracket**

**Next steps**

1. Replace the DPU card.



### Installing a DPU into a full length riser

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. If installing a new expansion card, unpack it and prepare the card for installation.
**NOTE:** For instructions, see the documentation accompanying the card.

4. Remove the GPU air shroud top cover.
5. If installed, remove the GPU air shroud filler.
6. Remove the full length expansion card riser.
7. Disconnect the cables from the DPU card and the system board. See Cable routing topic, configuration 56.
**NOTE:** Dell DPUs only support vSphere 8.0+ operating system and Partner or Channel DPU only supports Linux-based
operating system.

**CAUTION: Do not install GPUs, network cards, or other PCIe devices on your system that are not validated**
**and tested by Dell. Damage caused by unauthorized and invalidated hardware installation will null and void the**
**system warranty.**

**Steps**

1. If installed, remove the filler bracket.
**NOTE:** Store the filler bracket for future use. Filler brackets must be installed in empty expansion card slots to maintain
Federal Communications Commission (FCC) certification of the system. The brackets also keep dust and dirt out of the
system and aid in proper cooling and airflow inside the system.

**Figure 264. Removing the filler bracket**

2. Using a Phillips #2 screwdriver, tighten the two screws to secure the DPU bracket with the DPU card.
**Figure 265. Installing DPU bracket**

3. To install the DPU on Riser 1:

a. Align the connector on the DPU card with the connector on the riser.
b. Insert the DPU card into the riser until firmly seated.
c. Tilt the expansion card holder latch.
d. Press the card holder latch to secure the DPU card to the riser.

**Figure 266. Installing DPU card on Riser 1**

**Next steps**

1. Connect the cables to the DPU card and to the system board. See Cable routing topic, configuration 56.
2. If removed, install the GPU air shroud.
**NOTE:** The GPU air shroud filler must be installed, if a single-width GPU card or empty riser is used.

3. Install the full length expansion card riser.
4. Install the GPU air shroud top cover.
5. Follow the procedure listed in After working inside your system.
6. Install any device drivers required for the card as described in the documentation for the card.



## Optional serial COM port

This is a service technician replaceable part only.



### Removing the serial COM port

The procedure to remove serial COM port from Riser 3 or 4 is same.

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the air shroud or remove the GPU air shroud.
4. If required, remove PCIe card from slot 7.
**NOTE:** The serial COM port is supported only in slot 4 or slot 8 of the expansion card riser.

**Steps**

1. Loosen the captive screws on the system.
2. Press the blue release tab or blue button on the riser and holding the edges lift the expansion card riser from the riser
connector on the system board.

3. Disconnect the serial COM port cable from the rear I/O board.
**Figure 267. Disconnecting the serial COM port**

4. Open the latch on the expansion card riser and slide the serial COM port out of the expansion card riser.



### Figure 268. Removing the Serial COM port

5. Install the filler bracket if not replacing the serial COM port.
**Next steps**

1. Replace the serial COM port.



### Installing the serial COM port

The procedure to install serial COM port to Riser 3 or 4 is same.

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the air shroud or remove the GPU air shroud.
**NOTE:** The serial COM port is supported only in slot 4 or slot 8 of the expansion card riser.

4. Remove expansion card riser.
5. If required, remove PCIe card from slot 7.
**Steps**

1. Open the latch on the expansion card riser and remove the filler bracket from the expansion card riser.
2. Slide the serial COM port into the expansion card riser and close the latch.



### Figure 269. Installing the serial COM port

3. Connect the serial COM port cable to the rear I/O board.
4. Holding the edges or the touch points, align the holes on the expansion card riser with the guides on the system board.
5. Lower the expansion card riser into place and press the touch points until the expansion card riser connector is fully seated
on the system board connector.

6. Tighten the captive screws on the system.
**Figure 270. Connecting the serial COM port**

**Next steps**

1. Install the air shroud or install the GPU air shroud.
2. If required, install PCIe card to slot 7.
3. Follow the procedure listed in After working inside your system.



## Optional VGA port for Direct Liquid Cooling module





### Removing the VGA port

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the air shroud or remove the GPU air shroud.
**NOTE:** VGA port is supported only in slot 4 of the expansion card riser.

**Steps**

1. Loosen the captive screws on the system.
2. Press the blue release tab or blue button on the riser and holding the edges lift the expansion card riser from the riser
connector on the system board.

3. Disconnect the VGA port cable from the liquid cooling (LC) rear I/O board.
**Figure 271. Disconnecting the VGA port cable**

4. Open the latch on the expansion card riser and slide the VGA port out of the expansion card riser.
**Figure 272. Removing the VGA port**

5. Install the filler bracket if not replacing the VGA port.
**Next steps**

1. Replace the VGA port.



### Installing the VGA port

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the air shroud or remove the GPU air shroud.
4. Remove expansion card riser.
**NOTE:** VGA port is supported only in slot 4 of the expansion card riser.

**Steps**

1. Open the latch on the expansion card riser and remove the filler bracket from the expansion card riser.
2. Slide the VGA port into the expansion card riser.
**Figure 273. Installing the VGA port**

3. Connect the VGA port cable to the LC rear I/O board.
4. Holding the edges or the touch points, align the holes on the expansion card riser with the guides on the system board.
5. Lower the expansion card riser into place and press the touch points until the expansion card riser connector is fully seated
on the system board connector.

6. Tighten the captive screws on the system.
**Figure 274. Connecting the VGA port cable**

**Next steps**

1. Install the air shroud or install the GPU air shroud.
2. Follow the procedure listed in After working inside your system.



## M.2 SSD module





### Removing the M.2 NVMe SSD module

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
**Steps**

1. Pull and lift the BOSS-N1 card carrier retention latch lock to open.
2. Slide the BOSS-N1 card carrier out.
**Figure 275. Removing the BOSS-N1 card carrier**

3. Using the Phillips #1 screwdriver, remove the M3 x 0.5 x 4.5 mm screw securing the M.2 NVMe SSD module to the BOSS-N1
card.

4. Pull the M.2 NVMe SSD module to disconnect from the BOSS-N1 card connector.
**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.



### Figure 276. Removing the M.2 NVMe SSD module

**Next steps**

1. Replace the M.2 NVMe SSD module.



### Installing the M.2 NVMe SSD module

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
**Steps**

1. Align the M.2 NVMe SSD module at an angle with the BOSS-N1 card connector.
2. Insert the M.2 NVMe SSD module until it is firmly seated in the BOSS-N1 card connector.
3. Using the Phillips #1 screwdriver, secure the M.2 NVMe SSD module on the BOSS-N1 card with the M3 x 0.5 x 4.5 mm
screw.



### Figure 277. Installing the M.2 NVMe SSD module

4. Slide the BOSS-N1 card carrier into the BOSS-N1 module slot.
5. Close the BOSS-N1 card carrier release latch to lock the carrier in place.
**Figure 278. Installing the BOSS-N1 card carrier**

**Next steps**

1. Follow the procedure listed in the After working inside your system.



## Optional BOSS-N1 module





### Removing the BOSS-N1 module blank

The removal of the BOSS-N1 module blank from the Riser 1 and 4 x 2.5-inch rear drive module is similar.

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
**Steps**

Use a screwdriver to push out the blank from the BOSS-N1 module bay.



### Figure 279. Removing the BOSS-N1 module blank

**Next steps**

1. Replace the BOSS-N1 module blank or install BOSS-N1 module.



### Installing the BOSS-N1 module blank

The installation of the BOSS-N1 module blank to the Riser 1 and 4 x 2.5-inch rear drive module is similar.

**Prerequisites**

Follow the safety guidelines listed in the Safety instructions.

**Steps**

Align the blank with the BOSS-N1 module bay and push it into the bay until it clicks into place.



### Figure 280. Installing the BOSS-N1 module blank





### Removing the BOSS-N1 card carrier blank

The removal of the BOSS-N1 card carrier blank from the Riser 1 and 4 x 2.5-inch rear drive module is similar.

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
**Steps**

Press and pull the BOSS-N1 card carrier blank out from the BOSS-N1 module.



### Figure 281. Removing the BOSS-N1 card carrier blank

**Next steps**

1. Replace the BOSS-N1 card carrier blank or install BOSS-N1 card carrier.



### Installing the BOSS-N1 card carrier blank

The installation of the BOSS-N1 card carrier blank to the Riser 1 and 4 x 2.5-inch rear drive module is similar.

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
**Steps**

Align the blank with the BOSS-N1 module bay and push it into the bay until it clicks into place.



### Figure 282. Installing the BOSS-N1 card carrier blank





### Removing the BOSS-N1 module

The removal of the BOSS-N1 module from the Riser 1 and 4 x 2.5-inch rear drive module is similar.

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the M.2 NVMe SSD module.
**Steps**

1. Disconnect the BOSS-N1 power and signal cable from the system board.
2. Using the Phillips #1 screwdriver remove the M3 x 0.5 x 4.5 mm screw that secures the BOSS-N1 module on Riser 1.
3. Slide the BOSS-N1 module towards the front of the chassis and lift the module.



### Figure 283. Removing the BOSS-N1 module

4. Pull the blue tag to remove the BOSS-N1 signal cable from the BOSS-N1 module.
5. Remove the BOSS-N1 power cable from the BOSS-N1 module.
**Figure 284. Removing the BOSS-N1 power and signal cable from the BOSS-N1 module**

**Next steps**

1. Replace the BOSS-N1 module or Install the BOSS-N1 module blank.



### Installing the BOSS-N1 module

The installation of the BOSS-N1 module to the Riser 1 and 4 x 2.5-inch rear drive module is similar.

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. If installed, remove the BOSS module blank.
4. Remove the M.2 NVMe SSD module.
**Steps**

1. Connect the BOSS-N1 power and signal cables to the connectors on the BOSS-N1 module.
**Figure 285. Connecting the BOSS-N1 power and signal cables to the BOSS-N1 module**

2. Align the BOSS-N1 module at an angle with the controller card module slot.
3. Insert the BOSS-N1 module and push the module horizontally towards the rear of the system until firmly seated.
4. Using the Phillips #1 screwdriver, secure the BOSS-N1 module with the M3 x 0.5 x 4.5 mm screw.
5. Connect the BOSS-N1 power and signal cable to the connectors on the system board.



### Figure 286. Installing the BOSS-N1 module

**Next steps**

1. Install the M.2 NVMe SSD module.



## System battery

This is a service technician replaceable part only.



### Replacing the system battery

**Prerequisites**

**WARNING: There is a danger of a new battery exploding if it is incorrectly installed. Replace the battery only**
**with the same or equivalent type that is recommended by the manufacturer. Discard used batteries according to**
**the manufacturer's instructions. See the Safety instructions that came with your system for more information.**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. If applicable, disconnect the power or data cables from the expansion cards.
4. Remove the expansion card risers.
**Steps**

1. Press and hold the battery socket retention latch, for the battery to pop out.
**NOTE:** If the battery does not pop out, then lift it out of the socket.

**Figure 287. Removing the system battery**

2. To install a new system battery, hold the battery with the positive side facing up at an angle and slide it under the battery
holder socket latch.

3. Press the battery into the connector until it snaps into place.
**Figure 288. Installing the system battery**

**Next steps**

1. Install the expansion card risers.
2. If applicable, connect the cables to one or more expansion cards.
3. Follow the procedure listed in After working inside your system.
4. Confirm that the battery is operating properly, by performing the following steps:

a. Enter the System Setup, while booting, by pressing F2.

b. Enter the correct time and date in the System Setup **Time** and **Date** fields.
c. **Exit** the System Setup.
d. To test the newly installed battery, check the time and date at least an hour after installing the battery.
e. Enter the System Setup and if the time and date are still incorrect, see Getting help section.



## Optional internal USB card

**NOTE:** To locate the internal USB port on the system board, see the System board jumpers and connectors section.



### Removing the internal USB card

**Prerequisites**

**CAUTION: To avoid interference with other components in the server, the maximum permissible dimensions of**
**the USB memory key are 15.9 mm wide x 57.15 mm long x 7.9 mm high.**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the expansion card risers.
**Steps**

1. Holding the blue tag, lift the internal USB card to disconnect from the connector on the system board.
2. Remove the USB memory key from the internal USB card.



### Figure 289. Removing the internal USB card

**Next steps**

1. Replace the internal USB card.



### Installing the internal USB card

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the expansion card risers.
**Steps**

1. Connect the USB key to the internal USB card.
**NOTE:** For information about the exact location of USB on system board, see System board jumpers and connectors
section.

2. Align the internal USB card with the connector on the system board and press firmly until the internal USB card is seated.



### Figure 290. Installing the internal USB card

**Next steps**

1. Install the expansion card risers.
2. Follow the procedure listed in After working inside your system.
3. While booting, press F2 to enter **System Setup** and verify that the system detects the USB memory key.



## Intrusion switch

This is a service technician replaceable part only.



### Removing the intrusion switch module

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the expansion card riser.
**NOTE:** Ensure that you note the routing of the cable as you remove it from the system board. Route the cable properly
when you replace it to prevent the cable from being pinched or crimped.

**Steps**

1. Disconnect the intrusion switch cable from the connector on the rear I/O board.
2. Using a Phillips #1 screwdriver, loosen the screw on the intrusion switch module.
3. Slide the intrusion switch module out of the slot on the system.
**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.



### Figure 291. Removing the intrusion switch module

**Next steps**

1. Replace the intrusion switch module.



### Installing the intrusion switch module

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. Remove the expansion card riser.
**NOTE:** Ensure that you note the routing of the cable as you remove it from the system board. Route the cable properly
when you replace it to prevent the cable from being pinched or crimped.

**Steps**

1. Align and slide the intrusion switch module into the slot in the system until firmly seated.
2. Using a Phillips #1 screwdriver, tighten the screw on the intrusion switch module.
3. Connect the intrusion switch cable to the connector on the rear I/O board.



### Figure 292. Installing the intrusion switch module

**Next steps**

1. Install the expansion card riser.
2. Follow the procedure listed in After working inside your system.



## Optional OCP NIC card





### Removing the OCP NIC card

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the expansion card riser.
4. Disconnect OCP cable from the SL11_CPU1_PB7 to SL13_CPU1_PB7 connectors on the MAX system board, if 100 GbE OCP
card of PCIe width x16 is used.

**NOTE:** See cable routing section.

**Steps**

1. Open the blue latch to disengage the OCP NIC card.
2. Push the OCP NIC card towards the rear end of the system to disconnect from the connector on the system board.
3. Slide the OCP NIC card out of the slot on the system.



### Figure 293. Removing the OCP NIC card

4. If the OCP NIC card is not going to be replaced, install a filler bracket .
**NOTE:** You must install a filler bracket over an empty expansion card slot to maintain Federal Communications
Commission (FCC) certification of the system. The brackets also keep dust and dirt out of the system and aid in
proper cooling and airflow inside the system.

**Figure 294. Installation of filler bracket**

**Next steps**

1. Replace the OCP NIC card.



### Installing the OCP NIC card

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the expansion card riser.
4. Disconnect OCP NIC cable from the SL11_CPU1_PB7 to SL13_CPU1_PB7 connectors on the MAX system board, if 100 GbE
OCP NIC card of PCIe width x16 is used.

**NOTE:** See cable routing section.

**CAUTION: Do not install GPUs, network cards, or other PCIe devices on your system that are not validated**
**and tested by Dell. Damage caused by unauthorized and invalidated hardware installation will null and void the**
**system warranty.**

**Steps**

1. If installed, remove the filler bracket.
**NOTE:** Store the filler bracket for future use. Filler brackets must be installed in empty expansion card slots to maintain
Federal Communications Commission (FCC) certification of the system. The brackets also keep dust and dirt out of the
system and aid in proper cooling and airflow inside the system.

**Figure 295. Removal of filler bracket**

2. Open the blue latch on the system board.
3. Slide the OCP NIC card into the slot in the system.
4. Push until the OCP NIC card is connected to the connector on the system board.
5. Close the blue latch to lock the OCP NIC card to the system.
**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.



### Figure 296. Installing the OCP NIC card

**Next steps**

1. Connect OCP NIC cable from SL11_CPU1_PB7 to SL13_CPU1_PB7 connectors on the MAX system board, if 100 GbE OCP
NIC card of PCIe width x16 is used.

**NOTE:** See cable routing section.

2. Install the expansion card riser.
3. Follow the procedure listed in After working inside your system.



## Power supply unit

**NOTE:** While replacing the hot swappable PSU, after next server boot; the new PSU automatically updates to the same
firmware and configuration of the replaced one. For updating to the latest firmware and changing the configuration, see the
_Lifecycle Controller User's Guide_ [at iDRAC Manuals.](https://www.dell.com/idracmanuals)

**NOTE:** [For information about DC PSU cabling instructions, go to PowerEdge Manuals >](https://www.dell.com/poweredgemanuals) **Rack Servers**  - PowerEdge R760

  - **Select This Product**   - **Documentation**   - **Manuals and Documents**   - Cabling instructions for – 48 – 60 V DC power
supply.



### Hot spare feature

Your system supports the hot spare feature that significantly reduces the power overhead associated with the power supply
unit (PSU) redundancy.

When the hot spare feature is enabled, one of the redundant PSUs is switched to the sleep state. The active PSU supports 100
percent of the system load, thus operating at higher efficiency. The PSU in the sleep state monitors output voltage of the active
PSU. If the output voltage of the active PSU drops, the PSU in the sleep state returns to an active output state.

If having both PSUs active is more efficient than having one PSU in the sleep state, the active PSU can also activate the
sleeping PSU.

The default PSU settings are as follows:

- If the load on the active PSU is more than 50 percent of PSU rated power wattage, then the redundant PSU is switched to
the active state.

- If the load on the active PSU falls below 20 percent of PSU rated power wattage, then the redundant PSU is switched to
the sleep state.

You can configure the hot spare feature by using the iDRAC settings. For more information, see the _iDRAC User’s Guide_
[available at PowerEdge Manuals.](https://www.dell.com/poweredgemanuals)



### Removing a power supply unit blank

**Prerequisites**

Follow the safety guidelines listed in the Safety instructions.

**Steps**

Pull the blank out of the system.

**CAUTION: To ensure proper system cooling, the PSU blank must be installed in the second PSU bay in a**
**non-redundant configuration. Remove the PSU blank only if you are installing a second PSU.**



### Figure 297. Removing a power supply unit blank

**Next steps**

1. Replace the PSU blank or install the PSU.



### Installing a power supply unit blank

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
**NOTE:** Install the power supply unit (PSU) blank only in the second PSU bay.

2. If required, Remove the PSU.
**Steps**

Align the PSU blank with the PSU bay and push it into the PSU bay until it clicks into place.



### Figure 298. Installing a power supply unit blank





### Removing a power supply unit adapter

Remove the PSU adapter, when installing PSU with 86 mm wide form factor.

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
**Steps**

Using a Phillips #1 screwdriver, loosen the screw and remove the power supply unit adapter.



### Figure 299. Removing a power supply unit adapter

**Next steps**

1. Replace the PSU adapter or Install the PSU.



### Installing a power supply unit adapter

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. If required, Remove the PSU.
**NOTE:** Remove the PSU adapter, when installing PSU with 86 mm wide form factor.

**Steps**

1. Align and insert the power supply unit adapter.
2. Using a phillips #1 screwdriver, tighten the screw.



### Figure 300. Installing a power supply unit adapter

**Next steps**

1. Follow the procedure listed in After working inside your system.



### Removing a power supply unit

**Prerequisites**

**CAUTION: The system requires one power supply unit (PSU) for normal operation. On power-redundant**
**systems, remove and replace only one PSU at a time in a system that is powered on.**

1. Follow the safety guidelines listed in the Safety instructions.
2. Disconnect the power cable from the power outlet and from the PSU that you intend to remove.
3. Remove the cable from the strap on the PSU handle.
4. Unlatch and lift or remove the optional cable management accessory if it interferes with the PSU removal.
**NOTE:** For information about the cable management when the PSU is removed or installed while the system is in a rack,
[see the system’s cable management arm documentation at PowerEdge Manuals.](https://www.dell.com/poweredgemanuals)

**Steps**

Press the release latch and holding the PSU handle, slide the PSU out of the bay.



### Figure 301. Removing a power supply unit

**Next steps**

1. Replace the PSU or install the PSU blank.



### Installing a power supply unit

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. For systems that support redundant PSU, ensure that both the PSUs are of the same type and have the same maximum
output power.

**NOTE:** The maximum output power (shown in watts) is listed on the PSU label.

3. If required, Remove the PSU blank.
**Steps**

Slide the PSU into the PSU bay until the release latch snaps into place.



### Figure 302. Installing a power supply unit

**Next steps**

1. If you have unlatched or removed the cable management accessory, reinstall or relatch it. For information about the cable
management when the PSU is removed or installed while the system is in the rack, see the system’s cable management
[accessory documentation at PowerEdge Manuals.](https://www.dell.com/poweredgemanuals)
2. Connect the power cable to the PSU, and plug the cable into a power outlet.
**CAUTION: When connecting the power cable to the PSU, secure the cable to the PSU with the strap.**

**NOTE:** When installing hot swapping, or hot adding a new PSU, wait for 15 seconds for the system to recognize the
PSU and determine its status. The PSU redundancy may not occur until discovery is complete. The PSU status indicator
turns green to indicate that the PSU is functioning properly.



## Trusted Platform Module

This is a service technician replaceable part only.



### Upgrading the Trusted Platform Module





#### Removing the TPM

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
**NOTE:**

  - Ensure that the operating system is compatible with the TPM version you are installing.
  - Ensure that you download and install the latest BIOS firmware on your system.
  - Ensure that the BIOS is configured to enable UEFI boot mode.
**CAUTION: The TPM plug-in module is cryptographically bound to that particular system board after it is**
**installed. When the system is powered on, any attempt to remove an installed TPM plug-in module breaks the**
**cryptographic binding, and the removed TPM cannot be installed on another system board. Ensure any keys that**
**you have stored on the TPM have been securely transferred.**

**Steps**

1. Locate the TPM connector on the system board. For more information, see system board connectors .
2. Press to hold the module down and remove the screw using the security Torx 8-bit shipped with the TPM module.
3. Slide the TPM module out from its connector.
4. Push the plastic rivet away from the TPM connector and rotate it 90° counterclockwise to release it from the system board.
5. Pull the plastic rivet out of its slot on the system board.



#### Installing the TPM

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
**Steps**

1. To install the TPM, align the edge connectors on the TPM with the slot on the TPM connector.
2. Insert the TPM into the TPM connector such that the plastic rivet aligns with the slot on the system board.
3. Press the plastic rivet until the rivet snaps into place.
4. Replace the screw that secures the TPM to the system board.
**Figure 303. Installing the TPM**



### Initializing TPM for users

**Steps**

1. Initialize the TPM.

For more information, see Initialize the TPM 2.0 for users.

2. The **TPM Status** changes to **Enabled, Activated** .



### Initializing the TPM 2.0 for users

**Steps**

1. While booting your system, press F2 to enter System Setup.
2. On the **System Setup Main Menu** screen, click **System BIOS** - **System Security Settings** .
3. From the **TPM Security** option, select **On** .
4. Save the settings.
5. Restart your system.



## System board

This is a service technician replaceable part only.



### Removing the system board

**Prerequisites**

**CAUTION: If you are using the Trusted Platform Module (TPM) with an encryption key, you may be prompted**
**to create a recovery key during program or System Setup. Be sure to create and safely store this recovery key.**
**If you replace this system board, you must supply the recovery key when you restart your system or program**
**before you can access the encrypted data on your drives.**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the following components:

a. Air shroud
b. Cooling fan cage assembly
c. Side wall bracket
d. Memory modules
e. Serial COM port (if installed)
f. VGA port (if installed)
g. Expansion card risers
h. Rear drive module
i. Processor and heat sink module or Direct Liquid Cooling module
j. R1 and R4 paddle cards (if installed)
k. BOSS-N1 module
l. GPU air shroud (if installed)
m. Internal USB card (if installed)
n. OCP card (if installed)
o. Power supply units (PSU)
p. Disconnect all the cables from the system board and make note of all the cable connections.

**CAUTION: Take care not to damage the system identification button while removing the system board**
**from the system.**

**CAUTION: Do not lift the system board by holding a memory module, processor, or other components.**

**Steps**

1. Using the system board holder and plunger, slide the system board towards the front of the system.
2. At a tilted angle, lift the system board out of the chassis.



### Figure 304. Removing the system board

**Next steps**

1. Install the system board.



### Installing the system board

**Prerequisites**

**NOTE:** Before replacing the system board, replace the old iDRAC MAC address label on the Express Service Tag with the
iDRAC MAC address label of the replacement system board.

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in Before working inside your system.
3. If you are replacing the system board, remove all the components that are listed in the removing the system board section.
**Steps**

1. Unpack the new system board assembly.
**CAUTION: Do not lift the system board by holding a memory module, processor, or other components.**

**CAUTION: Take care not to damage the system identification button while placing the system board into the**
**chassis.**

2. Holding the system board holder and plunger, lower the system board at a tilted angle into the system.
3. Slide the system board towards the rear of the chassis until the connectors are firmly seated in the slots.



### Figure 305. Installing the system board

**Next steps**

1. Replace the following components:
a. Trusted Platform Module (TPM)

**NOTE:** The TPM Module must be replaced only while installing a new system board.

b. Internal USB card (if removed)
c. OCP card (if removed)
d. Processor and heat sink module or Direct Liquid Cooling module
e. Memory modules
f. R1 and R4 paddle cards (if removed)
g. GPU air shroud (if removed)
h. Expansion card risers
i. Rear drive module
j. VGA port (if removed)
k. Serial COM port (if removed)
l. BOSS-N1 module
m. Side wall bracket
n. Cooling fan cage assembly
o. Air shroud
p. Power supply units (PSU)
2. Reconnect all cables to the system board.
**NOTE:** Ensure that the cables inside the system are routed along the chassis wall and secured using the cable securing
bracket.

3. Ensure that you perform the following steps:

a. Use the Easy Restore feature to restore the Service Tag. See the Restoring the system by using the Easy Restore
feature section.
b. If the service tag is not backed up in the backup flash device, enter the system service tag manually. See the Manually

update the Service Tag by using System Setup section.
c. Install BIOS and iDRAC version updates, Diagnostics, and OS Driver Pack and OS Collector.
d. Re-enable the Trusted Platform Module (TPM). See the Upgrading the Trusted Platform Module section.
4. Follow the procedure listed in After working inside your system.



### Restoring the system using Easy Restore

The Easy Restore feature enables you to restore your service tag, license, UEFI configuration, and the system configuration data
after replacing the system board. All data is backed up in a backup flash device automatically. If BIOS detects a new system
board, and the service tag in the backup flash device, BIOS prompts the user to restore the backup information.

**About this task**

Below is a list of options/steps available:

**Steps**

1. Restore the service tag, license, and diagnostics information, press **Y**
2. Navigate to the Lifecycle Controller based restore options, press **N**
3. Restore data from a previously created **Hardware Server Profile**, press **F10**
**NOTE:** When the restore process is complete, BIOS prompts to restore the system configuration data.

4. Restore data from a previously created **Hardware Server Profile**, press **F10**
5. To restore the system configuration data, press **Y**
6. To use the default configuration settings, press **N**
**NOTE:** After the restore process is complete, system reboots.



### Manually update the Service Tag

After replacing a system board, if Easy Restore fails, follow this process to manually enter the Service Tag, using **System**
**Setup** .

**About this task**

If you know the system service tag, use the **System Setup** menu to enter the service tag.

**Steps**

1. Power on the system.
2. To enter the **System Setup**, press **F2** .
3. Click **Service Tag Settings** .
4. Enter the service tag.
**NOTE:** You can enter the service tag only when the **Service Tag** field is empty. Ensure that you enter the correct
service tag. Once the service tag is entered, it cannot be updated or changed. Incorrectly entered service tag will lead to
system board replacement.

5. Click **OK** .



## LOM card, MIC card and rear I/O board





### Removing the LOM card, MIC card and rear I/O board

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the system board.
**NOTE:** The procedure to remove the liquid cooling rear I/O board and rear I/O board is the same.

**Steps**

1. Using a Phillips # 2 screwdriver, remove the screws that secure the LAN on Motherboard (LOM) card, Management
Interface card (MIC), and rear I/O board to the system board.

2. Holding the edges, pull the LOM card, MIC card, or rear I/O board to disconnect from the connector on the system board.
**Figure 306. Removing the LOM card and rear I/O board**

**Figure 307. Removing the LOM card and liquid cooling rear I/O board**

**Figure 308. Removing the MIC card**

**NOTE:** MIC is card is only available in the system that supports Dell DPU cards.

**Next steps**

1. Replace the LOM card, MIC card and rear I/O board.



### Installing the LOM card, MIC card and rear I/O board

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the system board.
**NOTE:** The procedure to install the liquid cooling rear I/O board and rear I/O board is the same.

**Steps**

1. Align the connectors and slots on the LAN on Motherboard (LOM) card, Management Interface card (MIC), or rear I/O
board with the connector and standoffs on the system board.

2. Press the LOM card, MIC card, or rear I/O board until firmly seated on the system board connector.
3. Using a Phillips #2 screwdriver, secure the LOM card, MIC card or rear I/O board to the system board with screws.
**Figure 309. Installing the LOM card and rear I/O board**

**Figure 310. Installing the LOM card and Liquid cooling rear I/O board**

**Figure 311. Installing the MIC card**

**NOTE:** MIC is card is only available in the system that supports Dell DPU cards.

**Next steps**

1. Install the system board.
2. Follow the procedure listed in After working inside your system.



## Control panel

This is a service technician replaceable part only.



### Removing the right control panel

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the drive backplane cover.
4. If installed, remove the air shroud or remove the GPU air shroud.
5. Remove the cooling fan cage assembly.
6. Remove the side wall bracket.
**Steps**

1. Using the Phillips #1 screwdriver, remove the screws that secure the right control panel and cable cover to the system.
2. Remove the cable cover away from the system.
3. Disconnect the right control panel cable and the VGA cable from the connectors on the system board.
4. Holding the right control panel and VGA cable assembly, slide the right control panel out of the system.
**NOTE:** Observe the routing of the cable assembly as you remove the right control panel from the system.

**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.



### Figure 312. Removing the right control panel

**Next steps**

1. Replace the right control panel.



### Installing the right control panel

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the drive backplane cover.
4. If installed, remove the air shroud or remove the GPU air shroud.
5. Remove the cooling fan cage assembly.
6. Remove the side wall bracket.
**Steps**

1. Align and slide the right control panel into the slot on the system.
2. Route the right control panel cable through the side wall of the system.
3. Align and slide the right control panel cable cover in the slot on the system.
**NOTE:** Route the cable properly to prevent the cable from being pinched or crimped.

4. Connect the right control panel cable and VGA cable to the connectors on the system board.
5. Using the Phillips #1 screwdriver, tighten the screws that secure the right control panel and the cable cover to the system.
**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.



### Figure 313. Installing the right control panel

**Next steps**

1. Install the side wall bracket.
2. Install the cooling fan cage assembly.
3. Install the drive backplane cover.
4. If removed, install the air shroud or install the GPU air shroud
5. Follow the procedure listed in After working inside your system.



### Removing the left control panel

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the drive backplane cover.
4. If installed, remove the air shroud or remove the GPU air shroud.
5. Remove the cooling fan cage assembly.
6. Remove the side wall bracket.
**Steps**

1. Using the Phillips #1 screwdriver, remove the screws that secure the left control panel and the cable cover to the system.
2. Remove the cable cover away from the system.
3. Disconnect the control panel cable from the connector on the system board.
4. Holding the cable, slide the left control panel out of the system.
**NOTE:** Observe the routing of the cable as you remove the left control panel from the system.

**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.



### Figure 314. Removing the left control panel

**Next steps**

1. Replace the left control panel.



### Installing the left control panel

**Prerequisites**

1. Follow the safety guidelines listed in the Safety instructions.
2. Follow the procedure listed in the Before working inside your system.
3. Remove the drive backplane cover.
4. If installed, remove the air shroud or remove the GPU air shroud.
5. Remove the cooling fan cage assembly.
6. Remove the side wall bracket.
**Steps**

1. Align and slide the left control panel in the slot on the system.
2. Route the left control panel cable through the side wall of the system.
3. Align and slide the left control panel cable cover in the slot on the system.
**NOTE:** Route the cable properly to prevent the cable from being pinched or crimped.

4. Connect the left control panel cable to the connector on the system board .
5. Using the Phillips #1 screwdriver, tighten the screws to secure the left control panel and the cable cover to the system.
**NOTE:** The numbers on the image do not depict the exact steps. The numbers are for representation of sequence.



### Figure 315. Installing the left control panel

**Next steps**

1. Install the side wall bracket.
2. Install the cooling fan cage assembly.
3. Install the drive backplane cover.
4. If removed, install the air shroud or install the GPU air shroud
5. Follow the procedure listed in After working inside your system.



# 8 Upgrade Kits

The table lists the available After Point Of Sale [APOS] kits.

**Table 170. Upgrade kits**

|Kits|Related links to service instructions|
|---|---|
|Bezel|SeeInstalling the front bezel|
|M.2 NVMe SSD|SeeInstalling the M.2 NVMe SSD module|
|BOSS-N1|SeeInstalling the BOSS-N1 controller card module|
|GPU|SeeGPU kit|
|Drives|SeeInstalling the drive|
|Memory|SeeInstalling a memory module|
|PERC|SeeInstalling the rear mounting front PERC module and also see the document<br>included with the kit.|
|Network cards (Standard PCIe adapter<br>LP/FH)|SeeInstalling the LOM card and rear I/O board|
|Network cards (OCP)|SeeInstalling the OCP card|
|PCIe SSD card|SeeInstalling the drive|
|Power cords|Offered, but without special service instructions|
|Power supplies|SeeInstalling a power supply unit|
|Quick sync|Offered, but without special service instructions|
|TPM|SeeUpgrading the Trusted Platform Module|
|Processor enablement thermal kits|SeeInstalling the processor|
|Internal USB 3.0 card|SeeInternal USB card kit|
|Serial COM port|SeeInstalling the serial COM port|
|Cables|Offered, but without special service instructions|
|Fans|SeeInstalling a fan|
|Heat sink|SeeInstalling a processor heat sink module orDirect Liquid Cooling Module|
|Risers|Not available as APOS kits, upgrades offered only with the "Basic Deployment<br>Upgrade of Dell Server" service|
|Rail|Offered, but without special service instructions|
|Cable Management Arm (CMA)|Offered, but without special service instructions<br>**NOTE:** CMA is not supported in Direct Liquid Cooling configuration.|

**Topics:**

- BOSS-N1 module kit
- GPU kit
- Internal USB card kit
- Serial COM port kit



## BOSS-N1 module kit

The BOSS-N1 module supports up to two M.2 NVMe SSDs.

Before you begin the installation or removal process, follow the safety guidelines and before working inside the system
instructions.

**Table 171. BOSS-N1 module kit components**

|Components in kit|R760 (quantity)|
|---|---|
|BOSS-N1 controller card module|1|
|BOSS-N1 card carrier|1 or 2*|
|M.2 NVMe SSD|1 or 2*|
|M.2 NVMe SSD capacity label|1 or 2†|
|BOSS-N1 card carrier blank|1|
|M3 x 0.5 x 4.5 mm screws|1|
|BOSS-N1 power cable for Riser 1 (220 mm)|1|
|BOSS-N1 signal cable for Riser 1 (170 mm)|1|
|BOSS-N1 power cable for x4 rear drive module (260 mm)|1|
|BOSS-N1 signal cable for x4 rear drive module (240 mm)|1|

**NOTE:** *The quantity depends on the purchase order.

**NOTE:** [†] The quantity depends on the BOSS-N1 card carrier

To remove the BOSS blank :

1. Power off the system and remove the system cover.
2. Use a screwdriver to push out the blank from the BOSS-N1 module bay.



### Figure 316. Removing the BOSS-N1 module blank





### Figure 317. Installing the BOSS-N1 module blank

To install the BOSS-N1 module:

1. To install the BOSS-N1 module, see installing the BOSS-N1 module steps 1 to 5.
2. To install the M.2 NVMe SSD and BOSS-N1 card carrier, see installing the BOSS-N1 module steps 6 to 10.
**NOTE:** Install a BOSS-N1 card carrier blank if you are only using one BOSS-N1 card carrier.

**NOTE:** Refer to cable routing section, configuration 49 and 50 for more information about connecting the BOSS cables to
system board connectors.

**NOTE:** The installation of the BOSS-N1 module to the Riser 1 and 4 x 2.5-inch rear drive module is similar.

**NOTE:** Installing the BOSS-N1 card carrier does not require the system to be powered off. System shutdown is only
required when installing the BOSS-N1 controller card module.



## GPU kit

The GPU FL kit is available for the Customer. Depending on the kit ordered, the respective components are available.

**CAUTION: Do not install GPUs, network cards, or other PCIe devices on your system that are not validated**
**and tested by Dell. Damage caused by unauthorized and invalidated hardware installation will null and void the**
**system warranty.**

**CAUTION: Double Width (DW) GPU only supported on Riser Configurations 3-2, 5-2, or 10-2. APOS Riser**
**upgrades to Riser Configurations 3, 5, and 10 are not supported.**

**WARNING: Consumer-Grade GPU should not be installed or used in the Enterprise Server products.**

**Table 172. Components in the full length (FL) GPU kit**

|Components|GPU FL kit|Col3|
|---|---|---|
|**Components**|**Details**|**Quantity**|
|Risers|Riser configuration (RC)<br>3-2, 5-2*, or, 10-2*|RC 3-2: R1P^ (FL) + R2A (HL) + R3B (HL) + R4P^ (FL)<br>RC 5-2: R1R (FL) + R2A (HL) + R3A (FL) + R4P^ (FL)<br>RC 10-2: R1P^ (FL) + R2A (HL) + R4R (FL)|
|Shroud|GPU shroud|1|
|Fans|HPR GOLD fan|6|
|Heat sinks|L-type heat sink<br>for processor 1 and<br>processor 2|RC 3-2, 5-2: 2<br>RC 10-2: 1|
|Cables|Power cable|2 x 4 (8-position) or 2 x 6 + 1 x 4 (12-position + 4-sideband)|
|FL - Full Length, HL - Half Length, HPR - High Performance, RC - Riser configuration|FL - Full Length, HL - Half Length, HPR - High Performance, RC - Riser configuration|FL - Full Length, HL - Half Length, HPR - High Performance, RC - Riser configuration|

**NOTE:** Adding GPUs to a system may increase the power envelope of that configuration beyond the current power supplies
rated capability. Confirm power increase and upgrade PSU as needed to avoid negative impact on performance.

**NOTE:** The power cable is imprinted with a silkscreen indicating which connector on the system board and to which GPU it
should be connected. Refer to the GPU power cable matrix table below for the power cable required for your GPU.

**NOTE:** Populate only processor 1 slot in RC10-2, as it only supports a single processor configuration.

**NOTE:** See expansion card installation guidelines for more information about riser configuration supported for the system.

**NOTE:** ^ R1P and R4P supports Double Width (DW) GPU.

**NOTE:** *The configuration 5-2 supports DW GPU only on slot 7 and configuration 10-2 supports DW GPU only on slot 2.

**NOTE:** All GPU cards require an L-type heat sink and GPU shroud, irrespective of the length. However, the cooling fan and
foam are dependent on different configurations which is shown in the below table.

Before you begin, follow the safety guidelines and before working inside the system instructions.

1. Remove the standard or High Performance Silver (HPR) cooling fans and install the High performance Gold (VHP) cooling
fans.

**NOTE:** See the cooling fan and foam requirement matrix for the cooling fan and foam requirement for different
configurations.

**Table 173. Cooling fan and foam requirement matrix**

|System configuration|Cooling fan|Foam requirement|
|---|---|---|
|**System configuration**|**w/GPU**|**w/GPU**|
|8 x 2.5-inch NVMe|HPR GOLD|No|
|8 x 2.5-inch NVMe + 8 x 2.5-inch SAS/SATA|HPR GOLD|Yes|
|16 x 2.5-inch SAS/SATA|HPR GOLD|Yes|
|16 x 2.5-inch NVMe|HPR GOLD|No|
|24 x 2.5-inch SAS/SATA|HPR GOLD|Yes|
|16 x 2.5-inch SAS/SATA + 8 x 2.5-inch<br>NVMe|HPR GOLD|Yes|
|24 x 2.5-inch NVMe|HPR GOLD|No|
|16 X EDSFF E3.S NVMe|HPR GOLD|No|
|**NOTE:** HPR GOLD is also known as High performance Gold (HPR GOLD) fans.|**NOTE:** HPR GOLD is also known as High performance Gold (HPR GOLD) fans.|**NOTE:** HPR GOLD is also known as High performance Gold (HPR GOLD) fans.|

**NOTE:** Foam is required for all the configurations with rotational drives having:

   - 3.5-inch drives with HPR Silver or HPR Gold fan
   - 2.5-inch drives with HPR Gold fan
**NOTE:** For more information about supported cooling fans matrix, see the thermal restriction matrix section.

**NOTE:** 12 x 3.5-inch and rear drive configuration systems do not support a GPU card.

2. Remove the air shroud.
3. Remove the processor and heat sink module and also remove the processor.
**NOTE:** All GPU cards require an L-type heat sink and GPU shroud that is installed, irrespective of the length.

4. Install the processor on L-type heat sink and install the processor and heat sink module.
5. Install the GPU air shroud.
6. Remove the GPU air shroud top cover.
**NOTE:** The GPU air shroud top cover is part of the GPU air shroud.

7. Remove the GPU air shroud filler.
8. Install the GPU into full length risers.
**NOTE:** See installing full length expansion card riser into the system.

**NOTE:** For information about riser slot location on the system board, see the System board jumpers and connectors
section.

9. If applicable, connect the power cables to the GPU. To know the connectors for the GPU on the system board, see the

System board jumpers and connectors section.

See the GPU power cable matrix to know about the requirement of cables for the GPU.

**Table 174. GPU power cable matrix**

|Catego<br>ry|Supported GPUs|Type|Vendor|Cable|Cable quantity|
|---|---|---|---|---|---|
|GPU|NVIDIA A2, L4|HL (FH and LP<br>brackets)|NVIDIA|Not required|Not required|
|GPU|NVIDIA A30, A40, A16,<br>A100, and A800|FH and FL|NVIDIA|CPU type 2 x 4 (8-position)|1 each for Riser 1<br>and Riser 4|
|GPU|NVIDIA H100, L40, and<br>L40S|FH and FL|NVIDIA|12VH type 2 x 6 + 1 x 4 (12-<br>position + 4-sideband)|1 each for Riser 1<br>and Riser 4|
|HL - Half Length, FH - Full Height, FL - Full Length|HL - Half Length, FH - Full Height, FL - Full Length|HL - Half Length, FH - Full Height, FL - Full Length|HL - Half Length, FH - Full Height, FL - Full Length|HL - Half Length, FH - Full Height, FL - Full Length|HL - Half Length, FH - Full Height, FL - Full Length|

**NOTE:** A maximum of two DW GPUs with power cables or a maximum of six SW GPUs are supported in a system based
on riser configuration.

10. Install the full length expansion card riser or half height expansion card riser. See GPU kit components table for supported
GPU risers.
11. If installing SW GPU Install the GPU air shroud filler.
12. Install the GPU air shroud top cover.
13. Install the foam on the system cover. To install the foam,

a. Place the system cover with the System Information Label (SIL) side facing up.
b. Clean the surface of the system cover and make sure it is clean before attaching the foam.
c. For easier handling, peel off a small section of the adhesive cover and align the foam with the system cover.
d. Remove rest of the adhesive cover, and install foam on the system cover.
e. Press along the length of the foam to ensure that it is firmly affixed to the system cover.

**Figure 318. Installing foam on the system cover**

After installing, follow the After working inside the system instructions.



## Internal USB card kit

The internal USB card kit contains one internal USB card. For installation of internal USB card, see installing the internal USB
card section.

**NOTE:** Ensure to install the internal USB card in the USB card port and not in the J_R3_PCIE_PWR connector port.

**Figure 319. Internal USB card port information**



## Serial COM port kit

The serial COM port kit contains the components that are listed in the table.

**Table 175. Serial COM port kit**

|Components|Quantity|
|---|---|
|Serial COM port card|1|
|Cable|1|

For installation procedure of the serial COM port, see serial COM port section.



# 9 Jumpers and connectors

This topic provides some basic and specific information about jumpers and switches. It also describes the connectors on the
various boards in the system. Jumpers on the system board help to disable the system and reset the passwords. To install
components and cables correctly, you must know the connectors on the system board.

**Topics:**

- System board jumpers and connectors
- System board jumper settings
- Disabling a forgotten password



## System board jumpers and connectors





## Figure 320. System board jumpers and connectors

**Table 176. System board jumpers and connectors**

|Item|Connector|Description|
|---|---|---|
|1.|Rear_I/O_connector|Rear I/O connector|
|2.|J_R3_PCIE_PWR|Riser 3 power connector|
|3.|IO_RISER3 (CPU2)|Riser 3|
|4|B9, B1, B15, B7, B11, B3, B13, B5|DIMM for CPU 2 channels A, B, C, D|
|5.|SL10_PCH_SA11|SATA connector 10|
|6.|IO_RISER2_A (CPU1) and IO_RISER2_B (CPU2)|Riser 2|
|7.|TPM|TPM connector|
|8.|OCP|OCP NIC 3.0 connector|
|9.|SL13_CPU1_PB7|PCIe connector 13|
|10.|BATTERY|Coin cell battery|
|11.|LOM_Connector|LOM connector|
|12.|Internal USB|Internal USB connector|
|13.|SL11_CPU1_PB7|PCIe connector 11|
|14.|IO_RISER1 (CPU1)|Riser 1|
|15.|SIG_PWR_0|Power connector 0 - use for BP only|
|16.|BOSS_PWR|BOSS card power|
|17.|PSU1_SIG|PUCK sideband signal for Riser 1 GPU|
|18.|SL12_PCH_PA6|PCIe connector12|
|19.|FRONT_VIDEO|Front VGA|
|20.|PWR1_A|For power cable|
|21.|PWR1_B|For Riser 1 GPU power|
|22.|CPU 1|Processor 1|
|23.|A9, A1, A15, A7, A11, A3, A13, A5|DIMM for CPU 1 channels A, B, C, D|
|24.|SL8_CPU1_PA4|PCIe connector 8|
|25.|RGT_CP|Right control panel connector|
|26.|FAN_2U6|Fan 6 connector|
|27.|SIG_PWR_2|Power connector 2 - use for BP only|
|28.|SL7_CPU1_PB4|PCIe connector 7|
|29.|FAN_2U5|Fan 5 connector|
|30.|SL4_CPU1_PB2|PCIe connector 4|
|31.|FAN_2U4|Fan 4 connector|
|32.|SL3_CPU1_PA2|PCIe connector 3|
|33.|SIG_PWR_1|Power connector 2 - use for BP only|
|34.|SL6_CPU2_PA3|PCIe connector 6|
|35.|FAN_2U3|Fan 3 connector|
|36.|SL5_CPU2_PB3|PCIe connector 5|
|37.|FAN_2U2|Fan 2 connector|
|38.|SL2_CPU2_PB1|PCIe connector 2|
|39.|FAN_2U1|Fan 1 connector|

**Table 176. System board jumpers and connectors (continued)**

|Item|Connector|Description|
|---|---|---|
|40.|SL1_CPU2_PA1|PCIe connector 1|
|41.|PWRD_EN and NVRAM_CLR|Jumper|
|42.|LFT_CP|Left control panel connector|
|43.|A8, A16, A2, A10, A6, A14, A4, A12|DIMMs for CPU 1 channels H, G, F, E|
|44.|CPU 2|Processor 2|
|45.|B8, B16, B2, B10, B6, B14, B4, B12|DIMMs for CPU 2 channels H, G, F, E|
|46.|PWR2_B|For Riser 4 GPU power|
|47.|PWR2_A|For power cable|
|48.|PSU2_SIG|PUCK sideband signal for Riser 4 GPU|
|49.|IO_RISER4 (CPU2)|Riser 4|
|50.|SL9_CPU2_PA51|PCIe connector 9|
|51.|BAT_SIG|Battery signal connector|

**NOTE:** The platform supports Maximum (MAX) and Mainstream (MS) system boards.

  - 1 SL9_CPU2_PA5 and SL10_PCH_SA1 connectors are available only on MAX system board.
  - MS system board supports CPU TDP < 250 W
  - MAX system board supports CPU TDP => 250 W



## System board jumper settings

For information about resetting the password jumper to disable a password, see the Disabling a forgotten password section.

**Table 177. System board jumper settings**

|Jumper|Setting|Description|
|---|---|---|
|PWRD_EN||The BIOS password feature is enabled.|
|PWRD_EN||The BIOS password feature is disabled. The BIOS password is<br>now disabled and you are not allowed to set a new password.|
|NVRAM_CLR||The BIOS configuration settings are retained at system boot.|
|NVRAM_CLR||The BIOS configuration settings are cleared at system boot.|

**CAUTION: You should be cautious when changing the BIOS settings. The BIOS interface is designed for**
**advanced users. Any changes in the setting might prevent your system from starting correctly and may even**
**result in data loss.**



## Disabling a forgotten password

The software security features of the system include a system password and a setup password. The password jumper enables or
disables password features and clears any password(s) currently in use.

**Prerequisites**

**CAUTION: Many repairs may only be done by a certified service technician. You should only perform**
**troubleshooting and simple repairs as authorized in your product documentation, or as directed by the online or**
**telephone service and support team. Damage due to servicing that is not authorized by Dell is not covered by**
**your warranty. Read and follow the safety instructions that are shipped with your product.**

**Steps**

1. Power off the system and all attached peripherals. Disconnect the system from the electrical outlet, and disconnect the
peripherals.

2. Remove the system cover.
3. Move the jumper on the system board from pins 2 and 4 to pins 4 and 6.
4. Replace the system cover.
**NOTE:** The existing passwords are not disabled (erased) until the system boots with the jumper on pins 4 and 6.
However, before you assign a new system and/or setup password, you must move the jumper back to pins 2 and 4.

**NOTE:** If you assign a new system and/or setup password with the jumper on pins 4 and 6, the system disables the new
password(s) the next time it boots.

5. Reconnect the peripherals and connect the system to the electrical outlet, and then power on the system.
6. Power off the system.
7. Remove the system cover.
8. Move the jumper on the system board from pins 4 and 6 to pins 2 and 4.
9. Replace the system cover.
10. Reconnect the peripherals and connect the system to the electrical outlet, and then power on the system.
11. Assign a new system and/or setup password.



# 10 System diagnostics and indicator codes

The diagnostic indicators on the system front panel display system status during system startup.

**Topics:**

- Status LED indicators
- System health and system ID indicator codes
- iDRAC Quick Sync 2 indicator codes
- iDRAC Direct LED indicator codes
- LCD panel
- NIC indicator codes
- Power supply unit indicator codes
- Drive indicator codes
- EDSFF E3.S drive led codes
- Using system diagnostics



## Status LED indicators

**NOTE:** The indicators display solid amber if any error occurs.

**Figure 321. Status LED indicators**

**Table 178. Status LED indicators and descriptions**

|Icon|Description|Condition|Corrective action|
|---|---|---|---|
||Drive indicator|The indicator turns solid amber if<br>there is a drive error.|- Check the System Event Log to determine if the<br>drive has an error.<br>- Run the appropriate Online Diagnostics test.<br>Restart the system and run embedded<br>diagnostics (ePSA).|
||||- If the drives are configured in a RAID array,<br>restart the system, and enter the host adapter<br>configuration utility program.|
||Temperature<br>indicator|The indicator turns solid amber if<br>the system experiences a thermal<br>error (for example, the ambient<br>temperature is out of range or<br>there is a fan failure).|Ensure that none of the following conditions exist:<br>- A cooling fan has been removed or has failed.<br>- System cover, air shrouds, or back filler bracket<br>has been removed.<br>- Ambient temperature is too high.<br>- External airflow is obstructed.<br>If the problem persists, see theGetting help section.|
||Electrical<br>indicator|The indicator turns solid amber<br>if the system experiences an<br>electrical error (for example,<br>voltage out of range, or a failed<br>power supply unit (PSU) or<br>voltage regulator).|Check the System Event Log or system messages<br>for the specific issue. If it is due to a problem with<br>the PSU, check the LED on the PSU. Reseat the<br>PSU.<br>If the problem persists, see theGetting help section.|
||Memory indicator|The indicator turns solid amber if a<br>memory error occurs.|Check the System Event Log or system messages<br>for the location of the failed memory. Reseat the<br>memory module.<br>If the problem persists, see theGetting help section.|
||PCIe indicator|The indicator turns solid amber if a<br>PCIe card experiences an error.|Restart the system. Update any required drivers for<br>the PCIe card. Reinstall the card.<br>If the problem persists, see theGetting help section.<br>**NOTE:** For more information about the<br>supported PCIe cards, see theExpansion cards<br>and expansion card risers > Expansion card<br>installation guidelines section.|



## System health and system ID indicator codes

The system health and system ID indicator is located on the left control panel of the system.



## Figure 322. System health and system ID indicator

**Table 179. System health and system ID indicator codes**

**System health and system**



## ID indicator code

**Condition**

Solid blue Indicates that the system is powered on, is healthy, and system ID mode is not active. Press
the system health and system ID button to switch to system ID mode.

Blinking blue Indicates that the system ID mode is active. Press the system health and system ID button to
switch to system health mode.

Solid amber Indicates that the system is in fail-safe mode. If the problem persists, see the Getting help
section.

Blinking amber [EEMI guide.](https://dell.com/ErrorCodes)



## iDRAC Quick Sync 2 indicator codes

iDRAC Quick Sync 2 module (optional) is located on the left control panel front IO panel of the system.

**Table 180. iDRAC Quick Sync 2 indicators and descriptions**



## iDRAC Quick Sync 2 indicator

**code**

**Condition** **Corrective action**

Off (default state) Indicates that the iDRAC Quick Sync 2
feature is powered off. Press the iDRAC
Quick Sync 2 button to power on the
iDRAC Quick Sync 2 feature.

Solid white Indicates that iDRAC Quick Sync 2 is
ready to communicate. Press the iDRAC
Quick Sync 2 button to power off.

If the LED fails to power on, reseat the left
control panel flex cable and check. If the problem
persists, see the Getting help section.

If the LED fails to power off, restart the system.
If the problem persists, see the Getting help
section.

Blinks white rapidly Indicates data transfer activity. If the indicator continues to blink indefinitely, see
the Getting help section.

Blinks white slowly Indicates that firmware update is in
progress.

Blinks white five times rapidly
and then powers off

Indicates that the iDRAC Quick Sync 2
feature is disabled.

If the indicator continues to blink indefinitely, see
the Getting help section.

Check if iDRAC Quick Sync 2 feature is
configured to be disabled by iDRAC. If the
problem persists, see the Getting help section.
[PowerEdge Manuals or](https://www.dell.com/poweredgemanuals) _Dell OpenManage Server_
_Administrator User’s Guide_ at OpenManage
Manuals.

Restart the system. If the problem persists, see
the Getting help section.

Restart the system. If the problem persists, see
the Getting help section.

Solid amber Indicates that the system is in fail-safe
mode.

Blinking amber Indicates that the iDRAC Quick Sync 2
hardware is not responding properly.



## iDRAC Direct LED indicator codes

The iDRAC Direct LED indicator lights up to indicate that the port is connected and is being used as a part of the iDRAC
subsystem.

You can configure iDRAC Direct by using a USB to micro USB (type AB) cable, which you can connect to your laptop or
tablet. Cable length should not exceed 3 feet (0.91 meters). Performance could be affected by cable quality. The following table
describes iDRAC Direct activity when the iDRAC Direct port is active:

**Table 181. iDRAC Direct LED indicator codes**

**iDRAC Direct LED**



## indicator code

Solid green for two
seconds

Blinking green (on for
two seconds and off for
two seconds)

**Condition**

Indicates that the laptop or tablet is connected.

Indicates that the laptop or tablet that is connected is recognized.

LED Indicator off Indicates that the laptop or tablet is unplugged.



## LCD panel

The LCD panel provides system information, status, and error messages to indicate if the system is functioning correctly or
requires attention. The LCD panel is used to configure or view the iDRAC IP address of the system. For more information about
the event and error messages that are generated by the system firmware and agents that monitor system components, go to
[EEMI guide.](https://dell.com/ErrorCodes)

The LCD panel is available only on the optional front bezel. The optional front bezel is hot pluggable.

The status and conditions of the LCD panel are outlined here:

- The LCD backlight is white during normal operating conditions.
- If there is an issue, the LCD backlight turns amber and displays an error code followed by descriptive text.
**NOTE:** If the system is connected to a power source and an error is detected, the LCD turns amber regardless of
whether the system is powered on or off.

- When the system powers off and there are no errors, the LCD enters the standby mode after five minutes of inactivity.
Press any button on the LCD to power it on.

- If the LCD panel stops responding, remove the bezel and reinstall it.

If the problem persists, see Getting help.

- The LCD backlight remains off if LCD messaging is powered off using the iDRAC utility, the LCD panel, or other tools.
**Figure 323. LCD panel features**

**Table 182. LCD panel features**

|Item|Button or<br>display|Description|
|---|---|---|
|1|Left|Moves the cursor back in one-step increments.|
|2|Select|Selects the menu item that is highlighted by the cursor.|
|3|Right|Moves the cursor forward in one-step increments.<br>During message scrolling:<br>- Press and hold the right button to increase scrolling speed.<br>- Release the button to stop.<br>**NOTE:** The display stops scrolling when the button is released. After 45 seconds of inactivity,<br>the display starts scrolling.|
|4|LCD display|Displays the system information, status, and error messages or iDRAC IP address.|



### Viewing Home screen

The **Home** screen displays user-configurable information about the system. This screen is displayed during normal system
operation when there are no status messages or errors. When the system turns off and there are no errors, the LCD enters the
standby mode after five minutes of inactivity. Press any button on the LCD to turn it on.

**Steps**

1. To view the **Home** screen, press one of the three navigation buttons (Select, Left, or Right).
2. To navigate to the **Home** screen from another menu, complete the following steps:

a. Press and hold the navigation button till the up arrow is displayed.

b. Navigate to the **Home** icon using the up arrow .
c. Select the **Home** icon.
d. On the **Home** screen, press the **Select** button to enter the main menu.



### Setup menu

**NOTE:** When you select an option in the Setup menu, you must confirm the option before proceeding to the next action.

**Table 183. Setup menu**

|Option|Description|
|---|---|
|iDRAC|Select**DHCP** or**Static IP** to configure the network mode. If**Static IP** is selected, the available fields<br>are**IP**, **Subnet (Sub)**, and**Gateway (Gtw)**. Select**Setup DNS** to enable DNS and to view domain<br>addresses. Two separate DNS entries are available.|
|Set error|Select**SEL**to view LCD error messages in a format that matches the IPMI description in the SEL. This<br>enables you to match an LCD message with an SEL entry. Select**Simple** to view LCD error messages in a<br>simplified user-friendly description. For information about the event and error messages generated by the<br>system firmware and agents that monitor system components go toEEMI guide.|
|Set home|Select the default information to be displayed on the**Home** screen. SeeView Home menu section for the<br>options and option items that can be set as the default on the**Home** screen.|



### View menu

**NOTE:** When you select an option in the View menu, you must confirm the option before proceeding to the next action.

|Table 184. View menu|Col2|
|---|---|
|**Option**|**Description**|
|**iDRAC IP**|Displays the**IPv4** or**IPv6** addresses for iDRAC9. Addresses include**DNS**(**Primary** and<br>**Secondary**),**Gateway**, **IP**, and**Subnet** (IPv6 does not have Subnet).|
|**MAC**|Displays the MAC addresses for**iDRAC**, **iSCSI**, or**Network** devices.|
|**Name**|Displays the name of the**Host**, **Model**, or**User String** for the system.|
|**Number**|Displays the**Asset tag** or the**Service tag** for the system.|
|**Power**|Displays the power output of the system in BTU/hr or Watts. The display format can be<br>configured in the**Set home** submenu of the**Setup** menu.|
|**Temperature**|Displays the temperature of the system in Celsius or Fahrenheit. The display format can<br>be configured in the**Set home** submenu of the**Setup** menu.|



## NIC indicator codes

Each NIC on the back of the system has indicators that provide information about the activity and link status. The activity LED
indicator indicates if data is flowing through the NIC, and the link LED indicator indicates the speed of the connected network.

**Figure 324. NIC indicator codes**

1. Link LED indicator
2. Activity LED indicator
**Table 185. NIC indicator codes**

**NIC indicator codes** **Condition**

Link and activity indicators are off. Indicates that the NIC is not connected to the network.

Link indicator is green, and the activity indicator is
blinking green.

Indicates that the NIC is connected to a valid network at its maximum
port speed, and data is being sent or received.

**Table 185. NIC indicator codes (continued)**

**NIC indicator codes** **Condition**

Link indicator is amber, and the activity indicator is
blinking green.

Indicates that the NIC is connected to a valid network at less than its
maximum port speed, and data is being sent or received.

Link indicator is green, and the activity indicator is off. Indicates that the NIC is connected to a valid network at its maximum
port speed, and data is not being sent or received.

The link indicator is amber, and the activity indicator is
off.

Indicates that the NIC is connected to a valid network at less than its
maximum port speed, and data is not being sent or received.

The link indicator is blinking green, and activity is off. Indicates that the NIC identity is enabled through the NIC
configuration utility.



## Power supply unit indicator codes

AC and DC power supply units (PSUs) have an illuminated translucent handle that serves as an indicator. The indicator shows if
power is present or if a power fault has occurred.

**Figure 325. AC PSU status indicator**

1. AC PSU handle
2. Socket
3. Release latch
**Table 186. AC and DC PSU status indicator codes**

|Power indicator codes|Condition|
|---|---|
|Green|Indicates that a valid power source is connected to the PSU<br>and the PSU is operational.|
|Blinking amber|Indicates an issue with the PSU.|
|Not powered on|Indicates that the power is not connected to the PSU.|
|Blinking green|Indicates that the firmware of the PSU is being updated.<br>**CAUTION: Do not disconnect the power cord or**<br>**unplug the PSU when updating firmware. If firmware**<br>**update is interrupted, the PSUs will not function.**|
|Blinking greens and powers off|When hot-plugging a PSU, it blinks green five times at a rate<br>of 4 Hz and powers off. This indicates a PSU mismatch due to<br>efficiency, feature set, health status, or supported voltage.<br>**CAUTION: If two PSUs are installed, both the PSUs**<br>**must have the same type of label; for example,**<br>**Extended Power Performance (EPP) label. Mixing**<br>**PSUs from previous generations of PowerEdge**<br>**servers is not supported, even if the PSUs have the**|
||**same power rating. This results in a PSU mismatch**<br>**condition or failure to power on the system.**<br>**CAUTION: If two PSUs are used, they must be of**<br>**the same type and have the same maximum output**<br>**power.**<br>**CAUTION: When correcting a PSU mismatch, replace**<br>**the PSU with the blinking indicator. Swapping the**<br>**PSU to make a matched pair can result in an error**<br>**condition and an unexpected system shutdown. To**<br>**change from a high output configuration to a low**<br>**output configuration or vice versa, you must power**<br>**off the system.**<br>**CAUTION: AC PSUs support both 240 V and 120 V**<br>**input voltages with the exception of Titanium PSUs,**<br>**which support only 240 V. When two identical PSUs**<br>**receive different input voltages, they can output**<br>**different wattages, and trigger a mismatch.**|



## Drive indicator codes

The LEDs on the drive carrier indicate the state of each drive. Each drive carrier has two LEDs: an activity LED (green) and a
status LED (bicolor, green/amber). The activity LED blinks whenever the drive is accessed.

**Figure 326. Drive indicators**

1. Drive activity LED indicator
2. Drive status LED indicator
3. Drive capacity label
**NOTE:** If the drive is in the Advanced Host Controller Interface (AHCI) mode, the status LED indicator does not power on.

**NOTE:** Drive status indicator behavior is managed by Storage Spaces Direct. Not all drive status indicators may be used.

**Table 187. Drive indicator codes**

|Drive status indicator code|Condition|
|---|---|
|Blinks green twice per second|Indicates that the drive is being identified or preparing for removal.|
|Table 187. Drive indicator codes (continued)|Col2|
|**Drive status indicator code**|**Condition**|
|Not powered on|Indicates that the drive is ready for removal.<br>**NOTE:** The drive status indicator remains off until all drives<br>are initialized after the system is powered on. Drives are not<br>ready for removal during this time.|
|Blinks green, amber, and then powers off|Indicates that there is an unexpected drive failure.|
|Blinks amber four times per second|Indicates that the drive has failed.|
|Blinks green slowly|Indicates that the drive is rebuilding.|
|Solid green|Indicates that the drive is online.|
|Blinks green for three seconds, amber for three seconds,<br>and then powers off after six seconds|Indicates that the rebuild has stopped.|



## EDSFF E3.S drive led codes

The LEDs on the drive carrier indicate the state of each drive. The LEDs on the EDSFF E3.S drive have two LEDs: an activity
LED (green) and a locate/fault LED (blue/amber). The activity LED blinks whenever the drive is accessed.

**Figure 327. EDSFF E3.S drive indicators**

1. Drive activity LED indicator
2. Drive status LED indicator
3. Drive capacity label

## EDSFF E3.S drive led codes

E3.S hard drives have Green LED and Blue/Amber LED.

- Green LED shows : Drive power status, Activity
- Blue/Amber LED shows: Drive Fault, Locate

EDSFF indicator behavior

**Table 188. EDSFF indicator behavior**

|Pattern Name|Description|Blue Element|Amber Element|
|---|---|---|---|
|Locate|This device is being identified.|ON (1 sec ON 1 sec OFF)|OFF|
|Fault|The device is in a fault<br>condition.|OFF|ON (2 sec ON 1 sec OFF)|
|N/A|This device does not have<br>fault or locate device.|OFF|OFF|

**NOTE:** Locate behavior overrides Fault state.

Green LED

The green LED is driven and controlled by the device. The two functions for this LED are defined as follows:

- Power: This function indicates that the device has power and has no issues with its power regulation. Once the green LED
is ON, it shall either remain ON or blink at the activity frequency unless the device determines power is no longer within its
operating range.

- Activity: This function indicates if the device is being used.
**Table 189. LED and device state per function for Green LED**

|Function/Device state|LED state|
|---|---|
|Power ON/Device is powered, no activity occurring.|ON|
|Activity/Device is powered, host initiated I/O activity<br>occurring.|4 Hz nominal blink rate|
|Power OFF/Device is not powered.|OFF|



## Using system diagnostics

If you experience an issue with the system, run the system diagnostics before contacting Dell for technical support. The purpose
of running system diagnostics is to test the system hardware without using additional equipment or risking data loss. If you are
unable to fix the issue yourself, service and support personnel can use the diagnostics results to help you solve the issue.



### Dell Embedded System Diagnostics

**NOTE:** The Dell Embedded System Diagnostics is also known as Enhanced Pre-boot System Assessment (ePSA)
diagnostics.

The Embedded System Diagnostics provide a set of options for particular device groups or devices allowing you to:

- Run tests automatically or in an interactive mode
- Repeat tests
- Display or save test results
- Run thorough tests to introduce additional test options to provide extra information about the failed devices
- View status messages that inform you if tests are completed successfully
- View error messages that inform you of issues encountered during testing



#### Running the Embedded System Diagnostics from Boot Manager

Run the Embedded System Diagnostics (ePSA) if your system does not boot.

**Steps**

1. When the system is booting, press F11.
2. Use the up arrow and down arrow keys to select **System Utilities** - **Launch Diagnostics** .
3. Alternatively, when the system is booting, press F10, select **Hardware Diagnostics** - **Run Hardware Diagnostics** .

The **ePSA Pre-boot System Assessment** window is displayed, listing all devices detected in the system. The diagnostics
starts executing the tests on all the detected devices.



#### Running the Embedded System Diagnostics from the Dell Lifecycle

**Controller**

**Steps**

1. When the system is booting, press F10.
2. Select **Hardware Diagnostics** → **Run Hardware Diagnostics** .

The **ePSA Pre-boot System Assessment** window is displayed, listing all devices detected in the system. The diagnostics
start executing the tests on all the detected devices.



#### System diagnostic controls

|Table 190. System diagnostic controls|Col2|
|---|---|
|**Menu**|**Description**|
|**Configuration**|Displays the configuration and status information of all<br>detected devices.|
|**Results**|Displays the results of all tests that are run.|
|**System health**|Provides the current overview of the system performance.|
|**Event log**|Displays a timestamped log of the results of all tests run on<br>the system. This is displayed if at least one event description<br>is recorded.|



# 11 Getting help

**Topics:**

- Recycling or End-of-Life service information
- Contacting Dell Technologies
- Accessing system information by using QR code
- Receiving automated support with Secure Connect Gateway (SCG)



## Recycling or End-of-Life service information

Take back and recycling services are offered for this product in certain countries. If you want to dispose of system components,
[visit How to Recycle and select the relevant country.](https://www.dell.com/recyclingworldwide)



## Contacting Dell Technologies

Dell provides online and telephone based support and service options. If you do not have an active internet connection, you can
find Dell contact information on your purchase invoice, packing slip, bill or Dell product catalog. The availability of services varies
depending on the country and product, and some services may not be available in your area. To contact Dell for sales, technical
assistance, or customer service issues follow these steps:

**Steps**

1. [Go to Dell Support.](https://www.dell.com/support/home)
2. Select your country from the drop-down menu on the lower right corner of the page.
3. For customized support:

a. Enter the system Service Tag in the **Enter a Service Tag, Serial Number, Service Request, Model, or Keyword**
field.
b. Click **Search** .

The support page that lists the various support categories is displayed.

4. For general support:

a. Select your product category.
b. Select your product segment.
c. Select your product.
The support page that lists the various support categories is displayed.

5. For contact details of Dell Global Technical Support:

a. [Click Contact Technical Support.](https://www.dell.com/support/incidents-online/contactus/Dynamic)
b. The **Contact Technical Support** page is displayed with details to call, chat, or e-mail the Dell Global Technical Support
team.



## Accessing system information by using QR code

You can use the QR code located on the Express service tag in the front of the R760 system, to access information about
PowerEdge R760. There is also another QR code for accessing product information on the back of the system cover.

**Prerequisites**

Ensure that your smart phone or tablet has a QR code scanner installed.

The QR code includes the following information about your system:

- How-to videos
- Reference materials, including the Installation and Service Manual, LCD diagnostics, and mechanical overview
- The system service tag to quickly access the specific hardware configuration and warranty information
- A direct link to Dell to contact technical support and sales teams
**Steps**

1. [Go to PowerEdge Manuals, and navigate to your specific product or](https://www.dell.com/poweredgemanuals)
2. Use your smart phone or tablet to scan the model-specific QR code on your system.



### QR code for PowerEdge R760 system resources

**Figure 328. QR code for PowerEdge R760 system**



## Receiving automated support with Secure Connect Gateway (SCG)

Dell Secure Connect Gateway (SCG) is an optional Dell Services offering that automates technical support for your Dell
server, storage, and networking devices. By installing and setting up a Secure Connect Gateway (SCG) application in your IT
environment, you can receive the following benefits:

- Automated issue detection — Secure Connect Gateway (SCG) monitors your Dell devices and automatically detects
hardware issues, both proactively and predictively.

- Automated case creation — When an issue is detected, Secure Connect Gateway (SCG) automatically opens a support case
with Dell Technical Support.

- Automated diagnostic collection — Secure Connect Gateway (SCG) automatically collects system state information from
your devices and uploads it securely to Dell. This information is used by Dell Technical Support to troubleshoot the issue.

- Proactive contact — A Dell Technical Support agent contacts you about the support case and helps you resolve the issue.

The available benefits vary depending on the Dell Service entitlement purchased for your device. For more information about
[Secure Connect Gateway (SCG), go to secureconnectgateway.](https://www.dell.com/support/home/en-us/product-support/product/secure-connect-gateway/overview)



# 12 Documentation resources

This section provides information about the documentation resources for your system.

To view the document that is listed in the documentation resources table:

- From the Dell support site:
1. Click the documentation link that is provided in the Location column in the table.
2. Click the required product or product version.
**NOTE:** To locate the model number, see the front of your system.

3. On the Product Support page, click **Documentation** .
- Using search engines:
  - Type the name and version of the document in the search box.
**Table 191. Additional documentation resources for your system**

|Task|Document|Location|
|---|---|---|
|Setting up your system|For more information about installing and<br>securing the system into a rack, see the<br>Rail Installation Guide included with your rail<br>solution.<br>For information about setting up your system,<br>see the_Getting Started Guide_<br>document that is shipped with your system.|PowerEdge Manuals|
|Configuring your system|For information about the iDRAC features,<br>configuring and logging in to iDRAC, and<br>managing your system remotely, see the<br>Integrated Dell Remote Access Controller<br>User's Guide.<br>For information about understanding Remote<br>Access Controller Admin (RACADM)<br>subcommands and supported RACADM<br>interfaces, see the RACADM CLI Guide for<br>iDRAC.<br>For information about Redfish and its protocol,<br>supported schema, and Redfish<br>Eventing implemented in iDRAC, see the<br>Redfish API Guide.<br>For information about iDRAC property<br>database group and object descriptions, see<br>the Attribute Registry Guide.<br>For information about Intel QuickAssist<br>Technology, see the Integrated Dell Remote<br>Access Controller User's Guide.|PowerEdge Manuals|
|Configuring your system|For information about earlier versions of, the<br>iDRAC documents.<br>To identify the version of iDRAC available on<br>your system, on the iDRAC web interface,<br>click**?** >**About**.|iDRAC Manuals|
||For information about installing the<br>operating system, see the operating system<br>documentation.|Operating System Manuals|
||For information about updating drivers and<br>firmware, see the Methods to download<br>firmware and drivers section in this document.|Drivers|
|Managing your system|For information about systems management<br>software offered by Dell, see the Dell<br>OpenManage Systems Management Overview<br>Guide.|PowerEdge Manuals|
|Managing your system|For information about setting up, using,<br>and troubleshooting OpenManage, see the<br>Dell OpenManage Server Administrator User’s<br>Guide.|OpenManage Manuals|
|Managing your system|For information about installing and using Dell<br>Secure Connect Gateway, see the Dell Secure<br>Connect Gateway Enterprise User’s Guide.|serviceability tools|
|Managing your system|For information about partner programs<br>enterprise systems management, see the<br>OpenManage Connections Enterprise Systems<br>Management documents.|OpenManage Manuals|
|Working with the Dell<br>PowerEdge RAID controllers<br>(if applicable)|For information about understanding the<br>features of the Dell PowerEdge RAID<br>controllers (PERC), Software RAID controllers,<br>or BOSS card and deploying the cards, see the<br>Storage controller documentation.|Storage Controller Manuals|
|Understanding event and<br>error messages|For information about the event and error<br>messages that are generated by the system<br>firmware and agents that monitor system<br>components, see the EEMI guide.|EEMI guide|
|Troubleshooting your<br>system|For information about identifying and<br>troubleshooting the PowerEdge server issues,<br>see the Server Troubleshooting Guide.|PowerEdge Manuals|